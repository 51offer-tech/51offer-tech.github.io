{"pages":[{"title":"关于我","text":"我是一个大龄低学历程序猿，虽然不是非常热爱代码，但是还是喜欢折腾.不管你是想学编程还是刚工作的小白，可以加我QQ:188781475与我们闲聊。","link":"/about/index.html"},{"title":"文章分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"DataX将DB2数据同步到GBase中","text":"DataX 是阿里开源的异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。阿里云开源离线同步工具DataX3.0介绍 在2015年的时候，项目有用到DataX1.0来开发提数功能，使用很方便，我们自己也扩展一些插件,如DB2Writer、DB2Reader、GBaseWriter、GBaseReader等，不得不说，DataX的插件功能真的很强大很灵活。再到后来，2018年的时候，涉及到跨库数据定时同步。因为当时所涉及的数据量不大，也就几万，最多十几万的数据处理和同步。所以选用了Kettle,这款工具上手也比较快，图形化操作，我们在GUI中设计好我们的ktr和kjb，在定时通过命令的方式来执行kjb的方式实现定时ETL数据处理、同步功能。但是,Kettle已经满足不了我们了，因为我们要把上亿的指标数据从DB2同步到GBase中，当然比较好的方式多是从DB2导出文件然后load到GBase中，但是目前权限不够，也因为只有一台主机，没办法做相应的操作。那么就又得请出我们的DataX杀手锏。 手动编译DataX工具: Maven Git Idea Jdk1.8当然你也可以不用Git和Idea直接下载下来执行Maven命令即可.使用Idea导入DataX:点击clone后，就会下载代码，下载完成后就会下载依赖包。等待依赖包安装完成,执行打包命令:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107mvn -U clean package assembly:assembly -Dmaven.test.skip=true[INFO] Reading assembly descriptor: package.xml[INFO] datax/lib\\commons-io-2.4.jar already added, skipping[INFO] datax/lib\\commons-lang3-3.3.2.jar already added, skipping[INFO] datax/lib\\commons-math3-3.1.1.jar already added, skipping[INFO] datax/lib\\datax-common-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\datax-transformer-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\fastjson-1.1.46.sec01.jar already added, skipping[INFO] datax/lib\\hamcrest-core-1.3.jar already added, skipping[INFO] datax/lib\\logback-classic-1.0.13.jar already added, skipping[INFO] datax/lib\\logback-core-1.0.13.jar already added, skipping[INFO] datax/lib\\slf4j-api-1.7.10.jar already added, skipping[INFO] Building tar : F:\\WorkAbout\\work\\DataX\\target\\datax.tar.gz[INFO] datax/lib\\commons-io-2.4.jar already added, skipping[INFO] datax/lib\\commons-lang3-3.3.2.jar already added, skipping[INFO] datax/lib\\commons-math3-3.1.1.jar already added, skipping[INFO] datax/lib\\datax-common-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\datax-transformer-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\fastjson-1.1.46.sec01.jar already added, skipping[INFO] datax/lib\\hamcrest-core-1.3.jar already added, skipping[INFO] datax/lib\\logback-classic-1.0.13.jar already added, skipping[INFO] datax/lib\\logback-core-1.0.13.jar already added, skipping[INFO] datax/lib\\slf4j-api-1.7.10.jar already added, skipping[INFO] datax/lib\\commons-io-2.4.jar already added, skipping[INFO] datax/lib\\commons-lang3-3.3.2.jar already added, skipping[INFO] datax/lib\\commons-math3-3.1.1.jar already added, skipping[INFO] datax/lib\\datax-common-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\datax-transformer-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\fastjson-1.1.46.sec01.jar already added, skipping[INFO] datax/lib\\hamcrest-core-1.3.jar already added, skipping[INFO] datax/lib\\logback-classic-1.0.13.jar already added, skipping[INFO] datax/lib\\logback-core-1.0.13.jar already added, skipping[INFO] datax/lib\\slf4j-api-1.7.10.jar already added, skipping[INFO] Copying files to F:\\WorkAbout\\work\\DataX\\target\\datax[INFO] datax/lib\\commons-io-2.4.jar already added, skipping[INFO] datax/lib\\commons-lang3-3.3.2.jar already added, skipping[INFO] datax/lib\\commons-math3-3.1.1.jar already added, skipping[INFO] datax/lib\\datax-common-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\datax-transformer-0.0.1-SNAPSHOT.jar already added, skipping[INFO] datax/lib\\fastjson-1.1.46.sec01.jar already added, skipping[INFO] datax/lib\\hamcrest-core-1.3.jar already added, skipping[INFO] datax/lib\\logback-classic-1.0.13.jar already added, skipping[INFO] datax/lib\\logback-core-1.0.13.jar already added, skipping[INFO] datax/lib\\slf4j-api-1.7.10.jar already added, skipping[WARNING] Assembly file: F:\\WorkAbout\\work\\DataX\\target\\datax is not a regular file (it may be a directory). It cannot be attached to the project build for installation or deployment.[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO][INFO] datax-all 0.0.1-SNAPSHOT ........................... SUCCESS [14:23 min][INFO] datax-common ....................................... SUCCESS [ 3.743 s][INFO] datax-transformer .................................. SUCCESS [ 4.746 s][INFO] datax-core ......................................... SUCCESS [ 35.608 s][INFO] plugin-rdbms-util .................................. SUCCESS [ 2.233 s][INFO] mysqlreader ........................................ SUCCESS [ 3.712 s][INFO] drdsreader ......................................... SUCCESS [ 2.849 s][INFO] sqlserverreader .................................... SUCCESS [ 2.668 s][INFO] postgresqlreader ................................... SUCCESS [ 3.018 s][INFO] oraclereader ....................................... SUCCESS [ 2.720 s][INFO] odpsreader ......................................... SUCCESS [ 4.960 s][INFO] otsreader .......................................... SUCCESS [ 5.210 s][INFO] otsstreamreader .................................... SUCCESS [ 4.729 s][INFO] plugin-unstructured-storage-util ................... SUCCESS [ 1.542 s][INFO] txtfilereader ...................................... SUCCESS [ 11.342 s][INFO] hdfsreader ......................................... SUCCESS [ 36.736 s][INFO] streamreader ....................................... SUCCESS [ 2.925 s][INFO] ossreader .......................................... SUCCESS [ 12.321 s][INFO] ftpreader .......................................... SUCCESS [ 12.288 s][INFO] mongodbreader ...................................... SUCCESS [ 11.568 s][INFO] rdbmsreader ........................................ SUCCESS [ 3.784 s][INFO] hbase11xreader ..................................... SUCCESS [ 15.342 s][INFO] hbase094xreader .................................... SUCCESS [ 13.149 s][INFO] tsdbreader ......................................... SUCCESS [ 3.722 s][INFO] opentsdbreader ..................................... SUCCESS [ 6.592 s][INFO] cassandrareader .................................... SUCCESS [ 11.834 s][INFO] mysqlwriter ........................................ SUCCESS [ 3.282 s][INFO] drdswriter ......................................... SUCCESS [ 9.150 s][INFO] odpswriter ......................................... SUCCESS [ 13.498 s][INFO] txtfilewriter ...................................... SUCCESS [ 21.791 s][INFO] ftpwriter .......................................... SUCCESS [ 19.378 s][INFO] hdfswriter ......................................... SUCCESS [ 32.956 s][INFO] streamwriter ....................................... SUCCESS [ 5.611 s][INFO] otswriter .......................................... SUCCESS [ 5.545 s][INFO] oraclewriter ....................................... SUCCESS [ 4.152 s][INFO] sqlserverwriter .................................... SUCCESS [ 2.689 s][INFO] postgresqlwriter ................................... SUCCESS [ 2.502 s][INFO] osswriter .......................................... SUCCESS [ 12.467 s][INFO] mongodbwriter ...................................... SUCCESS [ 12.070 s][INFO] adswriter .......................................... SUCCESS [ 9.578 s][INFO] ocswriter .......................................... SUCCESS [ 5.485 s][INFO] rdbmswriter ........................................ SUCCESS [ 2.927 s][INFO] hbase11xwriter ..................................... SUCCESS [ 15.926 s][INFO] hbase094xwriter .................................... SUCCESS [ 15.623 s][INFO] hbase11xsqlwriter .................................. SUCCESS [ 27.912 s][INFO] hbase11xsqlreader .................................. SUCCESS [ 35.410 s][INFO] elasticsearchwriter ................................ SUCCESS [ 7.341 s][INFO] tsdbwriter ......................................... SUCCESS [ 3.051 s][INFO] adbpgwriter ........................................ SUCCESS [ 5.857 s][INFO] gdbwriter .......................................... SUCCESS [ 11.935 s][INFO] cassandrawriter .................................... SUCCESS [ 6.054 s][INFO] hbase20xsqlreader .................................. SUCCESS [ 3.264 s][INFO] hbase20xsqlwriter 0.0.1-SNAPSHOT ................... SUCCESS [ 3.577 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 23:03 min[INFO] Finished at: 2019-12-18T11:59:17+08:00[INFO] ------------------------------------------------------------------------ 打包成功后，会在对应目录下生成target文件夹.我这儿打包好的文件有1.12G，因为里面包含了很多插件以及插件所需要的依赖包。我只留下了mysql和rdbms两个插件。 因为我是按照的Python3.6.5如果直接使用官网的命令的话，会报错: 12345F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin&gt;python datax.py -r mysqlreader -w mysqlwriter File &quot;datax.py&quot;, line 114 print readerRef ^SyntaxError: Missing parentheses in call to &apos;print&apos;. Did you mean print(readerRef)? 所以，咱们得转换一下啦，Python3.x已经为我们提供了2to3的工具,在安装目录下面的Tools\\scripts中,来体验一下它的强大: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687D:\\Programs\\Python\\Python36-32\\Tools\\scripts&gt;2to3.py --output-dir=D:/datax -W -n F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin\\datax.pyWARNING: --write-unchanged-files/-W implies -w.lib2to3.main: Output in &apos;D:/datax&apos; will mirror the input directory &apos;F:\\\\WorkAbout\\\\work\\\\DataX\\\\target\\\\datax\\\\datax\\\\bin&apos; layout.RefactoringTool: Skipping optional fixer: bufferRefactoringTool: Skipping optional fixer: idiomsRefactoringTool: Skipping optional fixer: set_literalRefactoringTool: Skipping optional fixer: ws_commaRefactoringTool: Refactored F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin\\datax.py--- F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin\\datax.py (original)+++ F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin\\datax.py (refactored)@@ -52,13 +52,13 @@ def suicide(signum, e): global child_process- print &gt;&gt; sys.stderr, &quot;[Error] DataX receive unexpected signal %d, starts to suicide.&quot; % (signum)+ print(&quot;[Error] DataX receive unexpected signal %d, starts to suicide.&quot; % (signum), file=sys.stderr) if child_process: child_process.send_signal(signal.SIGQUIT) time.sleep(1) child_process.kill()- print &gt;&gt; sys.stderr, &quot;DataX Process was killed ! you did ?&quot;+ print(&quot;DataX Process was killed ! you did ?&quot;, file=sys.stderr) sys.exit(RET_STATE[&quot;KILL&quot;])@@ -111,10 +111,10 @@ def generateJobConfigTemplate(reader, writer): readerRef = &quot;Please refer to the %s document:\\n https://github.com/alibaba/DataX/blob/master/%s/doc/%s.md \\n&quot; % (reader,reader,reader) writerRef = &quot;Please refer to the %s document:\\n https://github.com/alibaba/DataX/blob/master/%s/doc/%s.md \\n &quot; % (writer,writer,writer)- print readerRef- print writerRef+ print(readerRef)+ print(writerRef) jobGuid = &apos;Please save the following configuration as a json file and use\\n python {DATAX_HOME}/bin/datax.py {JSON_FILE_NAME}.json \\nto run the job.\\n&apos;- print jobGuid+ print(jobGuid) jobTemplate={ &quot;job&quot;: { &quot;setting&quot;: {@@ -134,15 +134,15 @@ writerTemplatePath = &quot;%s/plugin/writer/%s/plugin_job_template.json&quot; % (DATAX_HOME,writer) try: readerPar = readPluginTemplate(readerTemplatePath);- except Exception, e:- print &quot;Read reader[%s] template error: can\\&apos;t find file %s&quot; % (reader,readerTemplatePath)+ except Exception as e:+ print(&quot;Read reader[%s] template error: can\\&apos;t find file %s&quot; % (reader,readerTemplatePath)) try: writerPar = readPluginTemplate(writerTemplatePath);- except Exception, e:- print &quot;Read writer[%s] template error: : can\\&apos;t find file %s&quot; % (writer,writerTemplatePath)+ except Exception as e:+ print(&quot;Read writer[%s] template error: : can\\&apos;t find file %s&quot; % (writer,writerTemplatePath)) jobTemplate[&apos;job&apos;][&apos;content&apos;][0][&apos;reader&apos;] = readerPar; jobTemplate[&apos;job&apos;][&apos;content&apos;][0][&apos;writer&apos;] = writerPar;- print json.dumps(jobTemplate, indent=4, sort_keys=True)+ print(json.dumps(jobTemplate, indent=4, sort_keys=True)) def readPluginTemplate(plugin): with open(plugin, &apos;r&apos;) as f:@@ -168,7 +168,7 @@ if options.remoteDebug: tempJVMCommand = tempJVMCommand + &quot; &quot; + REMOTE_DEBUG_CONFIG- print &apos;local ip: &apos;, getLocalIp()+ print(&apos;local ip: &apos;, getLocalIp()) if options.loglevel: tempJVMCommand = tempJVMCommand + &quot; &quot; + (&quot;-Dloglevel=%s&quot; % (options.loglevel))@@ -198,11 +198,11 @@ def printCopyright():- print &apos;&apos;&apos;+ print(&apos;&apos;&apos; DataX (%s), From Alibaba ! Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.-&apos;&apos;&apos; % DATAX_VERSION+&apos;&apos;&apos; % DATAX_VERSION) sys.stdout.flush()RefactoringTool: Writing converted F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin\\datax.py to D:/datax\\datax.py.RefactoringTool: Files that were modified:RefactoringTool: F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin\\datax.py 我将转到好的py文件更名为datax3.py,复制到bin目录下面,重新执行命令: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\bin&gt;python datax3.py -r mysqlreader -w mysqlwriterDataX (DATAX-OPENSOURCE-3.0), From Alibaba !Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.Please refer to the mysqlreader document: https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.mdPlease refer to the mysqlwriter document: https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.mdPlease save the following configuration as a json file and use python {DATAX_HOME}/bin/datax.py {JSON_FILE_NAME}.jsonto run the job.{ &quot;job&quot;: { &quot;content&quot;: [ { &quot;reader&quot;: { &quot;name&quot;: &quot;mysqlreader&quot;, &quot;parameter&quot;: { &quot;column&quot;: [], &quot;connection&quot;: [ { &quot;jdbcUrl&quot;: [], &quot;table&quot;: [] } ], &quot;password&quot;: &quot;&quot;, &quot;username&quot;: &quot;&quot;, &quot;where&quot;: &quot;&quot; } }, &quot;writer&quot;: { &quot;name&quot;: &quot;mysqlwriter&quot;, &quot;parameter&quot;: { &quot;column&quot;: [], &quot;connection&quot;: [ { &quot;jdbcUrl&quot;: &quot;&quot;, &quot;table&quot;: [] } ], &quot;password&quot;: &quot;&quot;, &quot;preSql&quot;: [], &quot;session&quot;: [], &quot;username&quot;: &quot;&quot;, &quot;writeMode&quot;: &quot;&quot; } } } ], &quot;setting&quot;: { &quot;speed&quot;: { &quot;channel&quot;: &quot;&quot; } } }} 我们可以把命令执行结果保存在文件中: 1python datax3.py -r mysqlreader -w mysqlwriter &gt; mysql2gbase.json 会在当前目录生成mysql2gbase.json文件。做相应的配置即可: 123456789101112131415161718192021222324252627282930313233343536373839{ &quot;job&quot;: { &quot;content&quot;: [{ &quot;reader&quot;: { &quot;name&quot;: &quot;mysqlreader&quot;, &quot;parameter&quot;: { &quot;column&quot;: [&quot;SCENE_ID&quot;, &quot;OP_TIME&quot;, &quot;OP_JOB_NO&quot;, &quot;REMARK&quot;], &quot;connection&quot;: [{ &quot;jdbcUrl&quot;: [&quot;jdbc:mysql://10.101.42.91:1299/zhcj_test&quot;], &quot;table&quot;: [&quot;op_log&quot;] }], &quot;password&quot;: &quot;bingosoft!&quot;, &quot;username&quot;: &quot;root&quot;, &quot;where&quot;: &quot;&quot; } }, &quot;writer&quot;: { &quot;name&quot;: &quot;mysqlwriter&quot;, &quot;parameter&quot;: { &quot;column&quot;: [&quot;SCENE_ID&quot;, &quot;OP_TIME&quot;, &quot;OP_JOB_NO&quot;, &quot;REMARK&quot;], &quot;connection&quot;: [{ &quot;jdbcUrl&quot;: &quot;jdbc:mysql://10.101.42.91:1299/zhcs_test&quot;, &quot;table&quot;: [&quot;datax_op_log&quot;] }], &quot;password&quot;: &quot;bingosoft!&quot;, &quot;preSql&quot;: [], &quot;session&quot;: [], &quot;username&quot;: &quot;root&quot;, &quot;writeMode&quot;: &quot;insert&quot; } } }], &quot;setting&quot;: { &quot;speed&quot;: { &quot;channel&quot;: &quot;4&quot; } } }} 执行同步命令python datax3.py mysql2gbase.json 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148DataX (DATAX-OPENSOURCE-3.0), From Alibaba !Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.2019-12-18 18:38:21.572 [main] INFO VMInfo - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl2019-12-18 18:38:21.578 [main] INFO Engine - the machine info =&gt; osInfo: Oracle Corporation 1.8 25.231-b11 jvmInfo: Windows 10 amd64 10.0 cpu num: 8 totalPhysicalMemory: -0.00G freePhysicalMemory: -0.00G maxFileDescriptorCount: -1 currentOpenFileDescriptorCount: -1 GC Names [PS MarkSweep, PS Scavenge] MEMORY_NAME | allocation_size | init_size PS Eden Space | 256.00MB | 256.00MB Code Cache | 240.00MB | 2.44MB Compressed Class Space | 1,024.00MB | 0.00MB PS Survivor Space | 42.50MB | 42.50MB PS Old Gen | 683.00MB | 683.00MB Metaspace | -0.00MB | 0.00MB 2019-12-18 18:38:21.594 [main] INFO Engine - { &quot;content&quot;:[ { &quot;reader&quot;:{ &quot;name&quot;:&quot;mysqlreader&quot;, &quot;parameter&quot;:{ &quot;column&quot;:[ &quot;SCENE_ID&quot;, &quot;OP_TIME&quot;, &quot;OP_JOB_NO&quot;, &quot;REMARK&quot; ], &quot;connection&quot;:[ { &quot;jdbcUrl&quot;:[ &quot;jdbc:mysql://10.101.42.91:1299/zhcj_test&quot; ], &quot;table&quot;:[ &quot;op_log&quot; ] } ], &quot;password&quot;:&quot;**********&quot;, &quot;username&quot;:&quot;root&quot;, &quot;where&quot;:&quot;&quot; } }, &quot;writer&quot;:{ &quot;name&quot;:&quot;mysqlwriter&quot;, &quot;parameter&quot;:{ &quot;column&quot;:[ &quot;SCENE_ID&quot;, &quot;OP_TIME&quot;, &quot;OP_JOB_NO&quot;, &quot;REMARK&quot; ], &quot;connection&quot;:[ { &quot;jdbcUrl&quot;:&quot;jdbc:mysql://10.101.42.91:1299/zhcs_test&quot;, &quot;table&quot;:[ &quot;datax_op_log&quot; ] } ], &quot;password&quot;:&quot;**********&quot;, &quot;preSql&quot;:[], &quot;session&quot;:[], &quot;username&quot;:&quot;root&quot;, &quot;writeMode&quot;:&quot;insert&quot; } } } ], &quot;setting&quot;:{ &quot;speed&quot;:{ &quot;channel&quot;:&quot;4&quot; } }}2019-12-18 18:38:21.606 [main] WARN Engine - prioriy set to 0, because NumberFormatException, the value is: null2019-12-18 18:38:21.608 [main] INFO PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=02019-12-18 18:38:21.608 [main] INFO JobContainer - DataX jobContainer starts job.2019-12-18 18:38:21.609 [main] INFO JobContainer - Set jobId = 02019-12-18 18:38:21.906 [job-0] INFO OriginalConfPretreatmentUtil - Available jdbcUrl:jdbc:mysql://10.101.42.91:1299/zhcj_test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true.2019-12-18 18:38:21.962 [job-0] INFO OriginalConfPretreatmentUtil - table:[op_log] has columns:[SCENE_ID,OP_TIME,OP_JOB_NO,REMARK].2019-12-18 18:38:22.184 [job-0] INFO OriginalConfPretreatmentUtil - table:[datax_op_log] all columns:[SCENE_ID,OP_TIME,OP_JOB_NO,REMARK].2019-12-18 18:38:22.242 [job-0] INFO OriginalConfPretreatmentUtil - Write data [insert INTO %s (SCENE_ID,OP_TIME,OP_JOB_NO,REMARK) VALUES(?,?,?,?)], which jdbcUrl like:[jdbc:mysql://10.101.42.91:1299/zhcs_test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true]2019-12-18 18:38:22.243 [job-0] INFO JobContainer - jobContainer starts to do prepare ...2019-12-18 18:38:22.243 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do prepare work .2019-12-18 18:38:22.243 [job-0] INFO JobContainer - DataX Writer.Job [mysqlwriter] do prepare work .2019-12-18 18:38:22.244 [job-0] INFO JobContainer - jobContainer starts to do split ...2019-12-18 18:38:22.244 [job-0] INFO JobContainer - Job set Channel-Number to 4 channels.2019-12-18 18:38:22.247 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] splits to [1] tasks.2019-12-18 18:38:22.248 [job-0] INFO JobContainer - DataX Writer.Job [mysqlwriter] splits to [1] tasks.2019-12-18 18:38:22.260 [job-0] INFO JobContainer - jobContainer starts to do schedule ...2019-12-18 18:38:22.262 [job-0] INFO JobContainer - Scheduler starts [1] taskGroups.2019-12-18 18:38:22.264 [job-0] INFO JobContainer - Running by standalone Mode.2019-12-18 18:38:22.272 [taskGroup-0] INFO TaskGroupContainer - taskGroupId=[0] start [1] channels for [1] tasks.2019-12-18 18:38:22.275 [taskGroup-0] INFO Channel - Channel set byte_speed_limit to -1, No bps activated.2019-12-18 18:38:22.275 [taskGroup-0] INFO Channel - Channel set record_speed_limit to -1, No tps activated.2019-12-18 18:38:22.282 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started2019-12-18 18:38:22.284 [0-0-0-reader] INFO CommonRdbmsReader$Task - Begin to read record by Sql: [select SCENE_ID,OP_TIME,OP_JOB_NO,REMARK from op_log ] jdbcUrl:[jdbc:mysql://10.101.42.91:1299/zhcj_test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true].2019-12-18 18:38:23.295 [0-0-0-reader] INFO CommonRdbmsReader$Task - Finished read record by Sql: [select SCENE_ID,OP_TIME,OP_JOB_NO,REMARK from op_log ] jdbcUrl:[jdbc:mysql://10.101.42.91:1299/zhcj_test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true].2019-12-18 18:38:23.592 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[1311]ms2019-12-18 18:38:23.592 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] completed it&apos;s tasks.2019-12-18 18:38:32.281 [job-0] INFO StandAloneJobContainerCommunicator - Total 15167 records, 849566 bytes | Speed 82.96KB/s, 1516 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.832s | All Task WaitReaderTime 0.058s | Percentage 100.00%2019-12-18 18:38:32.281 [job-0] INFO AbstractScheduler - Scheduler accomplished all tasks.2019-12-18 18:38:32.281 [job-0] INFO JobContainer - DataX Writer.Job [mysqlwriter] do post work.2019-12-18 18:38:32.281 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do post work.2019-12-18 18:38:32.281 [job-0] INFO JobContainer - DataX jobId [0] completed successfully.2019-12-18 18:38:32.282 [job-0] INFO HookInvoker - No hook invoked, because base dir not exists or is a file: F:\\WorkAbout\\work\\DataX\\target\\datax\\datax\\hook2019-12-18 18:38:32.283 [job-0] INFO JobContainer - [total cpu info] =&gt; averageCpu | maxDeltaCpu | minDeltaCpu -1.00% | -1.00% | -1.00% [total gc info] =&gt; NAME | totalGCCount | maxDeltaGCCount | minDeltaGCCount | totalGCTime | maxDeltaGCTime | minDeltaGCTime PS MarkSweep | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s PS Scavenge | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s 2019-12-18 18:38:32.283 [job-0] INFO JobContainer - PerfTrace not enable!2019-12-18 18:38:32.284 [job-0] INFO StandAloneJobContainerCommunicator - Total 15167 records, 849566 bytes | Speed 82.96KB/s, 1516 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.832s | All Task WaitReaderTime 0.058s | Percentage 100.00%2019-12-18 18:38:32.285 [job-0] INFO JobContainer - 任务启动时刻 : 2019-12-18 18:38:21任务结束时刻 : 2019-12-18 18:38:32任务总计耗时 : 10s任务平均流量 : 82.96KB/s记录写入速度 : 1516rec/s读出记录总数 : 15167读写失败总数 : 0 上面只是一个简单的示例，接下来。我们将进入正式实践环节。首先需要开发一个GbaseWriter,因为没有涉及到太多细节的处理，直接按照DataX插件开发宝典开发一个,打包插件mvn -U clean package assembly:assembly -pl gbasewriter -am -Dmaven.test.skip=true,然后执行:python datax.py -r mysqlreader -w gbasewriter,按照实际情况配置即可，运行效果如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157DataX (DATAX-OPENSOURCE-3.0), From Alibaba !Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.2019-12-20 15:07:22.954 [main] INFO VMInfo - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl2019-12-20 15:07:22.960 [main] INFO Engine - the machine info =&gt; osInfo: Oracle Corporation 1.8 25.231-b11 jvmInfo: Windows 10 amd64 10.0 cpu num: 8 totalPhysicalMemory: -0.00G freePhysicalMemory: -0.00G maxFileDescriptorCount: -1 currentOpenFileDescriptorCount: -1 GC Names [PS MarkSweep, PS Scavenge] MEMORY_NAME | allocation_size | init_size PS Eden Space | 256.00MB | 256.00MB Code Cache | 240.00MB | 2.44MB Compressed Class Space | 1,024.00MB | 0.00MB PS Survivor Space | 42.50MB | 42.50MB PS Old Gen | 683.00MB | 683.00MB Metaspace | -0.00MB | 0.00MB 2019-12-20 15:07:22.996 [main] INFO Engine - { &quot;content&quot;:[ { &quot;reader&quot;:{ &quot;name&quot;:&quot;mysqlreader&quot;, &quot;parameter&quot;:{ &quot;column&quot;:[ &quot;SCENE_ID&quot;, &quot;OP_TIME&quot;, &quot;OP_JOB_NO&quot;, &quot;REMARK&quot; ], &quot;connection&quot;:[ { &quot;jdbcUrl&quot;:[ &quot;jdbc:mysql://10.101.42.91:1299/zhcj_test&quot; ], &quot;table&quot;:[ &quot;op_log&quot; ] } ], &quot;password&quot;:&quot;**********&quot;, &quot;username&quot;:&quot;root&quot;, &quot;where&quot;:&quot;&quot; } }, &quot;writer&quot;:{ &quot;name&quot;:&quot;gbasewriter&quot;, &quot;parameter&quot;:{ &quot;column&quot;:[ &quot;SCENE_ID&quot;, &quot;OP_TIME&quot;, &quot;OP_JOB_NO&quot;, &quot;REMARK&quot; ], &quot;connection&quot;:[ { &quot;jdbcUrl&quot;:&quot;jdbc:gbase://10.101.42.91:1238/gdb_cd&quot;, &quot;table&quot;:[ &quot;datax_op_log&quot; ] } ], &quot;password&quot;:&quot;********&quot;, &quot;preSql&quot;:[], &quot;session&quot;:[], &quot;username&quot;:&quot;cd_system&quot;, &quot;writeMode&quot;:&quot;insert&quot; } } } ], &quot;setting&quot;:{ &quot;speed&quot;:{ &quot;channel&quot;:&quot;2&quot; } }}2019-12-20 15:07:23.009 [main] WARN Engine - prioriy set to 0, because NumberFormatException, the value is: null2019-12-20 15:07:23.011 [main] INFO PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=02019-12-20 15:07:23.011 [main] INFO JobContainer - DataX jobContainer starts job.2019-12-20 15:07:23.013 [main] INFO JobContainer - Set jobId = 02019-12-20 15:07:23.344 [job-0] INFO OriginalConfPretreatmentUtil - Available jdbcUrl:jdbc:mysql://10.101.42.91:1299/zhcj_test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true.2019-12-20 15:07:23.404 [job-0] INFO OriginalConfPretreatmentUtil - table:[op_log] has columns:[SCENE_ID,OP_TIME,OP_JOB_NO,REMARK].2019-12-20 15:07:23.761 [job-0] INFO OriginalConfPretreatmentUtil - table:[datax_op_log] all columns:[SCENE_ID,OP_TIME,OP_JOB_NO,REMARK].2019-12-20 15:07:23.962 [job-0] INFO OriginalConfPretreatmentUtil - Write data [insert INTO %s (SCENE_ID,OP_TIME,OP_JOB_NO,REMARK) VALUES(?,?,?,?)], which jdbcUrl like:[jdbc:gbase://10.101.42.91:1238/gdb_cd]2019-12-20 15:07:23.962 [job-0] INFO JobContainer - jobContainer starts to do prepare ...2019-12-20 15:07:23.962 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do prepare work .2019-12-20 15:07:23.963 [job-0] INFO JobContainer - DataX Writer.Job [gbasewriter] do prepare work .2019-12-20 15:07:23.963 [job-0] INFO JobContainer - jobContainer starts to do split ...2019-12-20 15:07:23.963 [job-0] INFO JobContainer - Job set Channel-Number to 2 channels.2019-12-20 15:07:24.060 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] splits to [1] tasks.2019-12-20 15:07:24.060 [job-0] INFO JobContainer - DataX Writer.Job [gbasewriter] splits to [1] tasks.2019-12-20 15:07:24.173 [job-0] INFO JobContainer - jobContainer starts to do schedule ...2019-12-20 15:07:24.199 [job-0] INFO JobContainer - Scheduler starts [1] taskGroups.2019-12-20 15:07:24.201 [job-0] INFO JobContainer - Running by standalone Mode.2019-12-20 15:07:24.245 [taskGroup-0] INFO TaskGroupContainer - taskGroupId=[0] start [1] channels for [1] tasks.2019-12-20 15:07:24.313 [taskGroup-0] INFO Channel - Channel set byte_speed_limit to -1, No bps activated.2019-12-20 15:07:24.313 [taskGroup-0] INFO Channel - Channel set record_speed_limit to -1, No tps activated.2019-12-20 15:07:24.434 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started2019-12-20 15:07:24.441 [0-0-0-reader] INFO CommonRdbmsReader$Task - Begin to read record by Sql: [select SCENE_ID,OP_TIME,OP_JOB_NO,REMARK from op_log ] jdbcUrl:[jdbc:mysql://10.101.42.91:1299/zhcj_test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true].2019-12-20 15:07:34.475 [job-0] INFO StandAloneJobContainerCommunicator - Total 0 records, 0 bytes | Speed 0B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.000s | All Task WaitReaderTime 0.000s | Percentage 0.00%2019-12-20 15:07:44.502 [job-0] INFO StandAloneJobContainerCommunicator - Total 2560 records, 149496 bytes | Speed 14.60KB/s, 256 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.142s | All Task WaitReaderTime 0.035s | Percentage 0.00%2019-12-20 15:07:54.502 [job-0] INFO StandAloneJobContainerCommunicator - Total 2560 records, 149496 bytes | Speed 0B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.142s | All Task WaitReaderTime 0.035s | Percentage 0.00%2019-12-20 15:08:04.503 [job-0] INFO StandAloneJobContainerCommunicator - Total 4608 records, 269164 bytes | Speed 11.69KB/s, 204 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 24.193s | All Task WaitReaderTime 0.044s | Percentage 0.00%2019-12-20 15:08:14.504 [job-0] INFO StandAloneJobContainerCommunicator - Total 4608 records, 269164 bytes | Speed 0B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 24.193s | All Task WaitReaderTime 0.044s | Percentage 0.00%2019-12-20 15:08:24.505 [job-0] INFO StandAloneJobContainerCommunicator - Total 6656 records, 390129 bytes | Speed 11.81KB/s, 204 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 43.026s | All Task WaitReaderTime 0.050s | Percentage 0.00%2019-12-20 15:08:44.505 [job-0] INFO StandAloneJobContainerCommunicator - Total 8704 records, 510488 bytes | Speed 5.88KB/s, 102 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 60.643s | All Task WaitReaderTime 0.053s | Percentage 0.00%2019-12-20 15:08:54.506 [job-0] INFO StandAloneJobContainerCommunicator - Total 10752 records, 612379 bytes | Speed 9.95KB/s, 204 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 78.139s | All Task WaitReaderTime 0.055s | Percentage 0.00%2019-12-20 15:09:14.506 [job-0] INFO StandAloneJobContainerCommunicator - Total 12800 records, 731913 bytes | Speed 5.84KB/s, 102 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 94.918s | All Task WaitReaderTime 0.058s | Percentage 0.00%2019-12-20 15:09:24.507 [job-0] INFO StandAloneJobContainerCommunicator - Total 12800 records, 731913 bytes | Speed 0B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 94.918s | All Task WaitReaderTime 0.058s | Percentage 0.00%2019-12-20 15:09:34.467 [0-0-0-reader] INFO CommonRdbmsReader$Task - Finished read record by Sql: [select SCENE_ID,OP_TIME,OP_JOB_NO,REMARK from op_log ] jdbcUrl:[jdbc:mysql://10.101.42.91:1299/zhcj_test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true].2019-12-20 15:09:34.508 [job-0] INFO StandAloneJobContainerCommunicator - Total 14848 records, 840903 bytes | Speed 10.64KB/s, 204 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 112.371s | All Task WaitReaderTime 0.060s | Percentage 0.00%2019-12-20 15:09:42.112 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[137708]ms2019-12-20 15:09:42.112 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] completed it&apos;s tasks.2019-12-20 15:09:44.508 [job-0] INFO AbstractScheduler - Scheduler accomplished all tasks.2019-12-20 15:09:44.508 [job-0] INFO JobContainer - DataX Writer.Job [gbasewriter] do post work.2019-12-20 15:09:44.509 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do post work.2019-12-20 15:09:44.509 [job-0] INFO JobContainer - DataX jobId [0] completed successfully.2019-12-20 15:09:44.510 [job-0] INFO HookInvoker - No hook invoked, because base dir not exists or is a file: F:\\WorkAbout\\datax\\datax\\hook2019-12-20 15:09:44.511 [job-0] INFO JobContainer - [total cpu info] =&gt; averageCpu | maxDeltaCpu | minDeltaCpu -1.00% | -1.00% | -1.00% [total gc info] =&gt; NAME | totalGCCount | maxDeltaGCCount | minDeltaGCCount | totalGCTime | maxDeltaGCTime | minDeltaGCTime PS MarkSweep | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s PS Scavenge | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s 2019-12-20 15:09:44.511 [job-0] INFO JobContainer - PerfTrace not enable!2019-12-20 15:09:44.511 [job-0] INFO StandAloneJobContainerCommunicator - Total 15167 records, 849566 bytes | Speed 5.93KB/s, 108 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 129.781s | All Task WaitReaderTime 0.061s | Percentage 100.00%2019-12-20 15:09:44.512 [job-0] INFO JobContainer - 任务启动时刻 : 2019-12-20 15:07:23任务结束时刻 : 2019-12-20 15:09:44任务总计耗时 : 141s任务平均流量 : 5.93KB/s记录写入速度 : 108rec/s读出记录总数 : 15167读写失败总数 : 0 没办法，机器太low。这并不是Datax真实的效果。现在就可以把Datax放在服务器上去同步数据了。","link":"/posts/615128810.html"},{"title":"AngularJS v1.x使用总结","text":"最近在改一个前后端分离的项目,前端用的Angular 1.4.5,后端用的java,由于AngularJs在4年前简单的使用过一次，大多都忘记了，这次拿到这个项目(前端被压缩过),第一眼就瞬间懵逼了,但是领导已经下了死命令了，也只得硬着头皮干，索性就把这次遇到的一些知识点记录一下。 控制器&amp;作用域ng-controller 指令指定使用的控制器，ng-app 指令初始化一个 AngularJS 应用程序,$scope作用域作为当前controller范围内的数据传输纽带，在controller中的$scope上添加相应的对象或者属性，在html即可使用 12345678910&lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt;&lt;/div&gt;&lt;script&gt;var app = angular.module(&apos;myApp&apos;, []); //定义一个模块app.controller(&apos;myCtrl&apos;, function($scope) { });&lt;/script&gt; 表达式 ,AngularJS 表达式可包含文字、运算符、变量、对象，将表达式中的内容绑定到HTML中 12345678910111213div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt;&lt;h1&gt;{{carname}}&lt;/h1&gt;&lt;/div&gt;&lt;script&gt;var app = angular.module(&apos;myApp&apos;, []);app.controller(&apos;myCtrl&apos;, function($scope) { $scope.carname = &quot;Volvo&quot;;});&lt;/script&gt; ng-model 指令数据双向绑定,在界面或者js中改变绑定变量的值，另一边也会随之影响 123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;script src=&quot;https://cdn.staticfile.org/angular.js/1.4.6/angular.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt;&lt;body&gt;&lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt;名字: &lt;input ng-model=&quot;name&quot;&gt;&lt;h1&gt;你输入了: {{name}}&lt;/h1&gt;&lt;/div&gt;&lt;script&gt;var app = angular.module(&apos;myApp&apos;, []);app.controller(&apos;myCtrl&apos;, function($scope) { $scope.name = &quot;John Doe&quot;;});&lt;/script&gt;&lt;p&gt;修改输入框的值，标题的名字也会相应修改。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 自定义指令Angular可以使用directive 来自定义指令 12345678910111213angular.module(&quot;app&quot;).directive(&quot;pivottableViewPanel&quot;, function() { return { restrict: &quot;AE&quot;, scope: { authType: &quot;@&quot;, authId: &quot;=&quot;, appChange(): &quot;&amp;&quot; }, templateUrl: &quot;js/modules/apps/pivottable/tpls/pivottableViewPanel.html&quot;, link: function($scope, $element, $attr) {} } }) restrict 值可以是以下几种: E 作为元素名使用 A 作为属性使用 C 作为类名使用 M 作为注释使用templateUrl：指定组件引用的html，可以是html代码也可以是html路径；scope：定义组件属性:@:单项绑定的前缀标识符,例如:;=:双向数据绑定前缀标识符,例如:;&lt;:单项绑定的前缀标识符，和=使用类似,例如:,但是不会影响父组件对象的值;&amp;:绑定函数方法的前缀标识符，例如：我们定义好的组件有一下几种方式可以调用: 元素名:&lt;pivottable-view-panel&gt;&lt;/pivottable-view-panel&gt;; 属性：&lt;div pivottable-view-panel&gt;&lt;/div&gt;; 类名：&lt;div class=&quot;pivottable-view-panel&quot;&gt;&lt;/div&gt;; 注解：&lt;!--directive: pivottable-view-panel--&gt;;这里需要注意的是，组件用驼峰命名法，Angular会自动将大写字母转为小写且在字母前以-隔开,在定义的scope中，如果对象属性只有值，默认组件属性是属性名转换后的名称,如果属性有值则使用属性值转换后的属性123456789101112scope: { authType: &quot;@&quot;, authId: &quot;=&quot;, appChange(): &quot;&amp;&quot; } 用的时候则是&lt;xxx auth-id&gt;&lt;/xxx&gt;scope: { authType: &quot;@&quot;, authId: &quot;=curAuthId&quot;, appChange(): &quot;&amp;&quot; } 用的时候则是&lt;xxx cur-auth-id&gt;&lt;/xxx&gt; 自定义服务Service是一个函数或对象，AngularJs自带了30多个服务，例如$http(向服务器请求数据)、$timeout(setTimeout),也可以自定义 123angular.module(&quot;demo&quot;).service(&quot;dataSetService&quot;,function(){}); 使用的话需要在控制器中注入这个Service 123angular.module(&quot;demo&quot;).controller(&apos;myCtrl&apos;, function($scope, dataSetService) { //使用dataSetService}); 依赖注入Angular提供了依赖注入功能，在使用的时候，加入事先定义好的service/factory/provider即可在自动注入 [\"$q\", \"$timeout\", \"DataAjaxService\"123function ($q, $timeout, dataAjaxService) { //此处即可使用$q}]); 未完待续","link":"/posts/3445328094.html"},{"title":"Docker常用命令整理","text":"Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。 Docker基础命令docker images 查看当前下载镜像列表 12345[root@VM_175_142_centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhwdsl2/ipsec-vpn-server latest 62e5a169190a 7 months ago 206MBpostgres latest 30bf4f039abe 8 months ago 312MBredis latest 0f88f9be5839 8 months ago 95MB docker search {镜像名} (镜像仓库中的镜像)docker pull {镜像名}:{版本号} (拉取指定版本镜像,如果不指定版本， 将默认使用 latest 镜像)docker run -t -i {镜像名} (启动镜像,如果主机不存在，会自动下载镜像) 123456789101112131415161718192021222324252627282930313233343536373839404142-d, --detach=false， 设置容器上前台运行还是后台运行，默认为false后台运行-i, --interactive=false， 打开STDIN，用于控制台交互-t, --tty=false， 分配tty设备，可以支持终端登录，默认为false-u, --user=&quot;&quot;， 设置容器的用户-a, --attach=[] 登录容器（必须是以docker run -d启动的容器） -w, --workdir=&quot;&quot; 指定容器的工作目录 -c, --cpu-shares=0 设置容器CPU权重，在CPU共享场景使用 -e, --env=[] 指定环境变量，容器中可以使用该环境变量 -m, --memory=&quot;&quot; 指定容器的内存上限 -P, --publish-all=false， 指定容器暴露的端口-p, --publish=[]， 指定容器暴露的端口-h, --hostname=&quot;&quot;， 指定容器的主机名-v, --volume=[]， 给容器挂载存储卷，挂载到容器的某个目录--volumes-from=[]， 给容器挂载其他容器上的卷，挂载到容器的某个目录--cap-add=[]， 添加权限，权限清单详见：http://linux.die.net/man/7/capabilities--cap-drop=[]， 删除权限，权限清单详见：http://linux.die.net/man/7/capabilities--cidfile=&quot;&quot;， 运行容器后，在指定文件中写入容器PID值，一种典型的监控系统用法--cpuset=&quot;&quot;， 设置容器可以使用哪些CPU，此参数可以用来容器独占CPU--device=[]， 添加主机设备给容器，相当于设备直通--dns=[]， 指定容器的dns服务器--dns-search=[]， 指定容器的dns搜索域名，写入到容器的/etc/resolv.conf文件--entrypoint=&quot;&quot;， 覆盖image的入口点--env-file=[]， 指定环境变量文件，文件格式为每行一个环境变量--expose=[]， 指定容器暴露的端口，即修改镜像的暴露端口--link=[]， 指定容器间的关联，使用其他容器的IP、env等信息--lxc-conf=[]， 指定容器的配置文件，只有在指定--exec-driver=lxc时使用--name=&quot;&quot;， 指定容器名字，后续可以通过名字进行容器管理，links特性需要使用名字--net=&quot;bridge&quot; 容器网络设置: bridge 使用docker daemon指定的网桥 host //容器使用主机的网络 container:NAME_or_ID &gt;//使用其他容器的网路，共享IP和PORT等网络资源 none 容器使用自己的网络（类似--net=bridge),但是不进行配置--privileged=false 指定容器是否为特权容器，特权容器拥有所有的capabilities --restart=&quot;no&quot; 指定容器停止后的重启策略: no：容器退出时不重启 on-failure：容器故障退出（返回值非零）时重启 always：容器退出时总是重启 --rm=false 指定容器停止后自动删除容器(不支持以docker run -d启动的容器) --sig-proxy=true 设置由代理接受并处理信号，但是SIGCHLD、SIGSTOP和SIGKILL不能被代理 docker run -d --name=nginx nginx:latest -p 宿主机端口:容器端口 -v 宿主机目录:容器目录 docker ps #查看正在运行的容器docker ps -l #查看最后退出的容器的IDdocker ps -a #查看所有的容器，包括退出的。docker logs {容器ID|容器名称} #查询某个容器的所有操作记录。docker logs -f {容器ID|容器名称} #实时查看容易的操作记录。 1234docker rm$(docker ps -a -q) #删除所有容器docker rm {容器ID|容器名} #删除单个容器docker rmi {镜像ID} #删除单个镜像docker rmi$(docker images | grep none | awk &apos;{print $3}&apos; | sort -r) #删除所有镜像 docker stop {容器ID|容器名} #停止某个容器docker start {容器ID|容器名} #启动某个容器docker kill {容器ID|容器名} #杀掉某个容器 docker export {容器ID|容器名} -o /root/文件名.tar(或者docker export {容器ID|容器名} &gt; /root/文件名.tar) #导出docker import {容器文件} {镜像名}:{tag} #导入后生成的是镜像不是容器，docker load也可以导入，其中两者人区别如下： 12docker load 保留了容器的完整记录docker import 仅保存容器当时的快照状态，在导入的时候自己定义标签、名称等元数据 docker container inspect {容器ID|容器名} #返回容器的ID、创建时间、路径、状态、镜像等信息 docker container stats {容器ID|容器名} #查看容器的CPU、内存、存储、网络等资源的使用情况可以使用 docker cp {宿主机目录} {容器ID}:{容器目录} #将宿主机内的指定目录文件传输至容器内部的指定目录 docker cp {容器ID}:{容器目录} {宿主机目录} #将容器内部的指定目录文件复制到宿主机指定目录 docker commit {容器ID} {镜像名}:{tag} #将容器重新打包成镜像 1234567-a :提交的镜像作者；-c :使用Dockerfile指令来创建镜像；-m :提交时的说明文字；-p :在commit时，将容器暂停。 docker push {镜像名|镜像ID} #推送在镜像仓库 Dockerfile文件参数 FROM #指定基础镜像，必须为第一个命令 MAINTAINER #作者信息 RUN #构建镜像时执行的命令 ADD #复制文件到容器中 COPY #复制文本到容器中COPY &lt;源路径&gt;… &lt;目标路径&gt; 1ADD和COPY的差别：ADD命令tar类型文件会自动解压(网络压缩资源不会被解压)，可以访问网络资源，COPY不会自动解压文件，也不能访问网络资源 CMD #容器启动时执行的命令。shell 格式： CMD &lt;命令&gt; exec 格式： CMD [“可执行文件”, “参数1”, “参数2”…] ENTRYPOINT #配置容器 LABEL #为镜像添加元数据 ENV #设置环境变量 EXPOSE #指定外界交互的容器端口EXPOSE &lt;端口1&gt; [&lt;端口2&gt;…] VOLUME #指定持久化目录VOLUME [“&lt;路径1&gt;”, “&lt;路径2&gt;”…] WORKDIR #工作目录，自动cd到执行目录 示例： 123456789101112 # FROM代表此次构建的镜像的基础镜像基础,可在镜像名后带版本号，不带版本号默认latestFROM python# COPY是拷贝宿主机文件到镜像中COPY ./spider /work# RUN则是在镜像中执行命令，有时候可能需要安装依赖环境，也可以在run中执行RUN ls /work# 切换工作目录，相当于cd workWORKDIR /work#EXPOSE 外界交互的端口EXPOSE 8080# CMD是镜像启动后默认执行，CMD加中括号等同于exec执行命令，不加中括号等同于 sh -c 执行命令CMD [&quot;python&quot;,&quot;spider.py&quot;] docker build ocnfig #用于检查dockerfile文件是否有误 docker build . -t spider:v1.0 #构建镜像，构建好之后可以使用docker images命令进行查询 Docker-compose.yml文件配置build：定义镜像生成，可以指定Dockerfile构建，在up 启动之时执行构建任务 image：指定镜像启动容器，如果镜像不存在会自动拉去最新镜像 environment：环境变量和配置 ports：端口映射，将容器端口映射在宿主机 depends_on：指定依赖关系。适用于需要按顺序启动的服务，会先启动所依赖的镜像 volumes：挂载宿主机目录或者数据容器卷 volumes_from: 从容器挂载 context：指定Dockerfile文件或者是远程网络文件 args：构建参数，这些参数只能在构建过程中访问 container_name：指定容器名称 links: 链接其他容器 command: 启动执行命令 示例： 123456789101112131415161718192021version: &quot;3&quot;services: pgsql: image: postgres ports: - &quot;15432:5432&quot; environment: POSTGRES_PASSWORD: scyd! volumes: - ./data/postgresql_data1:/var/lib/postgresql/data restart: always redis: image: redis command: redis-server --requirepass scyd! ports: - &quot;16379:6379&quot; volumes: - ./data/redis_data1:/data restart: always 执行docker-compose ps： 12345[root@VM_175_142_centos ~]# docker-compose ps Name Command State Ports -------------------------------------------------------------------------------root_pgsql_1 docker-entrypoint.sh postgres Up 0.0.0.0:15432-&gt;5432/tcproot_redis_1 docker-entrypoint.sh redis ... Up 0.0.0.0:16379-&gt;6379/tcp 容器名：{目录名}{服务名}{容器序号} 从1开始 docker-compose up -d #构建并启动容器,首次运行会执行docker-compose build 1234-d：后台进程--scale：指定服务运行的容器个数（如果服务有对外的端口就不能指定多个容器，因为端口已经被占用） Eg：docker-compose up -d --scale web=1 --scale redis=2 docker-compose exec {服务名称} bash #登录到某个容器中 docker-compose down #删除所有容器,镜像 docker-compose ps #显示所有容器 docker-compose restart {服务名称} #重新启动容器 docker-compose build {服务名称} #构建镜像 。 docker-compose build –no-cache {服务名称} #不带缓存的构建。 docker-compose logs {服务名称} #查看容器的日志 docker-compose logs -f {服务名称} #查看容器的实时日志 docker-compose rm {服务名称} #删除compose服务 docker-compose kill {服务名称} #kill compose服务 docker-compose stop {服务名称} #重启compose服务 docker-compose start {服务名称} #启动容器 docker-compose config -q #验证yml文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息 docker-compose run {服务名} {cmd} #在某个服务上运行shell命令","link":"/posts/2574598278.html"},{"title":"ES6常用关键字总结","text":"ES6(ECMAScript 6.0)是JavaScript 语言的新一代标准，2015年6月正式发布。其提供了一些新特性使代码更加简洁。由于目前有些版本浏览兼容性不足，所以需要实用babel这类工具讲我们的ES6转换为ES5. let、const在ES6之前，是使用var来定义变量， 1var demo=&apos;demo&apos;; ES6之前，只有全局作用域和函数作用域，没有块级作用域,这种情况下会出现变量提升的现象导致变量被覆盖之类的问题，在ES6中，使用let就不存在这种问题，它只会在定义之后且在该块级作用域内可使用 1let demo=&apos;demo&apos;; const声明一个只读的常量，在声明时必须赋初始值，并且赋值后不可改变 1const API_URL=&quot;http://xxx.xxx.xxx&quot;; 模板字符串在ES6之前，需要拼接字符串，一般是通过”+”或者数组join的方式 123var a1=&apos;a1&apos;;var a2=a1+&apos;aa&apos;;var strs=[&apos;a1&apos;,&apos;a2&apos;].join(); 现在我们可以通过模版字符串来拼接 12let a1=&apos;a1&apos;;let a2=`aa${a1}`; 变量解构赋值E6之前只能一个个变量赋值 123let a = 1;let b = 2;let c = 3; ES6以后可以使用数组批量按照对应位置赋值 12let [a, b, c] = [1, 2, 3]; //a=1,b=2,c=3;let [a1,b1,c1]=[1,2]; //a1=1,a2=2,a3=undefined 也可以解析对象，解析对象的时候变量名必须和属性名一致，否则会解构失败， 1let { a, b } = { a: &apos;aaa&apos;, b: &apos;bbb&apos; }; //a=&apos;aaa&apos;,b=&apos;bbb&apos; 解构赋值对提取 JSON 对象中的数据 123456789let jsonData = { id: 42, status: &quot;OK&quot;, data: [867, 5309]};let { id, status, data: number } = jsonData;console.log(id, status, number); 箭头函数ES6 允许使用”箭头”（=&gt;）定义函数。函数体内的this对象，就是定义时所在的对象，而不是使用时所在的对象 123456var f = v =&gt; v;// 等同于var f = function (v) { return v;}; 如果箭头函数不需要参数或需要多个参数，就使用一个圆括号代表参数部分。 123456789var f = () =&gt; 5;// 等同于var f = function () { return 5 };var sum = (num1, num2) =&gt; num1 + num2;// 等同于var sum = function(num1, num2) { return num1 + num2;}; …操作符…用于获取函数的多余参数，这样就不需要使用arguments对象了。rest 参数搭配的变量是一个数组，该变量将多余的参数放入数组中。 1234567891011function add(...values) { let sum = 0; for (var val of values) { sum += val; } return sum;}add(2, 5, 3) // 10 注意，rest 参数之后不能再有其他参数（即只能是最后一个参数），否则会报错。 Class类ES6中提供了class关键字来定义类 12345678910111213141516171819class Point { constructor(x, y) { this.x = x; this.y = y; } toString() { return &apos;(&apos; + this.x + &apos;, &apos; + this.y + &apos;)&apos;; }}//等价于function Point(x, y) { this.x = x; this.y = y;}Point.prototype.toString = function () { return &apos;(&apos; + this.x + &apos;, &apos; + this.y + &apos;)&apos;;}; constructor方法是类的默认方法，通过new命令生成对象实例时，自动调用该方法。一个类必须有constructor方法，如果没有显式定义，一个空的constructor方法会被默认添加。 1234567class Point {}// 等同于class Point { constructor() {}} 这里只记录了几个常用的 新特性关键字，系统完整的学习可以去阮一峰大神的博客学习","link":"/posts/1332675393.html"},{"title":"借助alibaba Driud SQL Parser组件处理sql语句","text":"Druid是Java语言中最好的数据库连接池。Druid能够提供强大的监控和扩展功能。 先来看看我们要达到的效果:这个项目的目的是让业务用户通过手动拖拽自己想要的字段即可展示、分析、下载数据，之前的流程是先添加数据源-&gt;在系统中导入表-&gt;配置分析模型-选择分析模型展示数据。这里的分析模型其实就是通过拖拽表类配置表与表之间是关联关系。然后拖拽字段后，前端就发送选择的字段表达式和过滤条件表达式到后端，通过配置是分析模型关系，解析出相应数据库的SQL语句发送到数据库执行。现在有一种场景，就是用户能只需要查询单张表，如果这样也需要去配置分析模型，那么就增加了复杂度。所以，需要给用户提供一种直接通过数据源直接选择未建立关联关系的原表来查询、分析、下载数据。由于项目是没有源代码，而且之前的解析那一块都是基于分析模型，内部逻辑比较复杂。所以我选择了一种相对来说要方便一点的方式就是通过Driud来解析生成SQL语句。我们都知道Druid是一个JDBC组件库，包括数据库连接池、SQL Parser等组件,我们一般都是拿来做JDBC连接池和SQL语句监控使用。这里，我只是把这次SQL Parser组件的使用过程记录一下。感兴趣的朋友可以去看看Druid 使用手册 Druid_SQL_AST学习什么是ASTAST是abstract syntax tree的缩写，也就是抽象语法树。和所有的Parser一样，Druid Parser会生成一个抽象语法树。 在Druid SQL Parser中有哪些AST节点类型在Druid中，AST节点类型主要包括SQLObject、SQLExpr、SQLStatement三种抽象类型。 123456789package com.alibaba.druid.sql.ast; interface SQLObject {}interface SQLExpr extends SQLObject {}interface SQLStatement extends SQLObject {} interface SQLTableSource extends SQLObject {}class SQLSelect extends SQLObject {}class SQLSelectQueryBlock extends SQLObject {} 常用的SQLExpr有哪些123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.alibaba.druid.sql.ast.expr; // SQLName是一种的SQLExpr的Expr，包括SQLIdentifierExpr、SQLPropertyExpr等public interface SQLName extends SQLExpr {} // 例如 ID = 3 这里的ID是一个SQLIdentifierExprclass SQLIdentifierExpr implements SQLExpr, SQLName { String name;} // 例如 A.ID = 3 这里的A.ID是一个SQLPropertyExprclass SQLPropertyExpr implements SQLExpr, SQLName { SQLExpr owner; String name;} // 例如 ID = 3 这是一个SQLBinaryOpExpr// left是ID (SQLIdentifierExpr)// right是3 (SQLIntegerExpr)class SQLBinaryOpExpr implements SQLExpr { SQLExpr left; SQLExpr right; SQLBinaryOperator operator;} // 例如 select * from where id = ?，这里的?是一个SQLVariantRefExpr，name是&apos;?&apos;class SQLVariantRefExpr extends SQLExprImpl { String name;} // 例如 ID = 3 这里的3是一个SQLIntegerExprpublic class SQLIntegerExpr extends SQLNumericLiteralExpr implements SQLValuableExpr { Number number; // 所有实现了SQLValuableExpr接口的SQLExpr都可以直接调用这个方法求值 @Override public Object getValue() { return this.number; }} // 例如 NAME = &apos;jobs&apos; 这里的&apos;jobs&apos;是一个SQLCharExprpublic class SQLCharExpr extends SQLTextLiteralExpr implements SQLValuableExpr{ String text;} 常用的SQLStatemment最常用的Statement当然是SELECT/UPDATE/DELETE/INSERT，他们分别是 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package com.alibaba.druid.sql.ast.statement; class SQLSelectStatement implements SQLStatement { SQLSelect select;}class SQLUpdateStatement implements SQLStatement { SQLExprTableSource tableSource; List&lt;SQLUpdateSetItem&gt; items; SQLExpr where;}class SQLDeleteStatement implements SQLStatement { SQLTableSource tableSource; SQLExpr where;}class SQLInsertStatement implements SQLStatement { SQLExprTableSource tableSource; List&lt;SQLExpr&gt; columns; SQLSelect query;}2.3. SQLTableSource常见的SQLTableSource包括SQLExprTableSource、SQLJoinTableSource、SQLSubqueryTableSource、SQLWithSubqueryClause.Entryclass SQLTableSourceImpl extends SQLObjectImpl implements SQLTableSource { String alias;} // 例如 select * from emp where i = 3，这里的from emp是一个SQLExprTableSource// 其中expr是一个name=emp的SQLIdentifierExprclass SQLExprTableSource extends SQLTableSourceImpl { SQLExpr expr;} // 例如 select * from emp e inner join org o on e.org_id = o.id// 其中left &apos;emp e&apos; 是一个SQLExprTableSource，right &apos;org o&apos;也是一个SQLExprTableSource// condition &apos;e.org_id = o.id&apos;是一个SQLBinaryOpExprclass SQLJoinTableSource extends SQLTableSourceImpl { SQLTableSource left; SQLTableSource right; JoinType joinType; // INNER_JOIN/CROSS_JOIN/LEFT_OUTER_JOIN/RIGHT_OUTER_JOIN/... SQLExpr condition;} // 例如 select * from (select * from temp) a，这里第一层from(...)是一个SQLSubqueryTableSourceSQLSubqueryTableSource extends SQLTableSourceImpl { SQLSelect select;} /* 例如WITH RECURSIVE ancestors AS ( SELECT * FROM org UNION SELECT f.* FROM org f, ancestors a WHERE f.id = a.parent_id)SELECT *FROM ancestors; 这里的ancestors AS (...) 是一个SQLWithSubqueryClause.Entry*/class SQLWithSubqueryClause { static class Entry extends SQLTableSourceImpl { SQLSelect subQuery; }}2.4. SQLSelect &amp; SQLSelectQuerySQLSelectStatement包含一个SQLSelect，SQLSelect包含一个SQLSelectQuery，都是组成的关系。SQLSelectQuery有主要的两个派生类，分别是SQLSelectQueryBlock和SQLUnionQuery。class SQLSelect extends SQLObjectImpl { SQLWithSubqueryClause withSubQuery; SQLSelectQuery query;} interface SQLSelectQuery extends SQLObject {} class SQLSelectQueryBlock implements SQLSelectQuery { List&lt;SQLSelectItem&gt; selectList; SQLTableSource from; SQLExprTableSource into; SQLExpr where; SQLSelectGroupByClause groupBy; SQLOrderBy orderBy; SQLLimit limit;} class SQLUnionQuery implements SQLSelectQuery { SQLSelectQuery left; SQLSelectQuery right; SQLUnionOperator operator; // UNION/UNION_ALL/MINUS/INTERSECT} SQLCreateTableStatement建表语句包含了一系列方法，用于方便各种操作 1234567891011121314public class SQLCreateTableStatement extends SQLStatementImpl implements SQLDDLStatement, SQLCreateStatement { SQLExprTableSource tableSource; List&lt;SQLTableElement&gt; tableElementList; Select select; // 忽略大小写的查找SQLCreateTableStatement中的SQLColumnDefinition public SQLColumnDefinition findColumn(String columName) {} // 忽略大小写的查找SQLCreateTableStatement中的column关联的索引 public SQLTableElement findIndex(String columnName) {} // 是否外键依赖另外一个表 public boolean isReferenced(String tableName) {}} 怎样产生AST通过SQLUtils产生List1234import com.alibaba.druid.util.JdbcConstants; String dbType = JdbcConstants.MYSQL;List&lt;SQLStatement&gt; statementList = SQLUtils.parseStatements(sql, dbType); 通过SQLUtils产生SQLExpr12String dbType = JdbcConstants.MYSQL;SQLExpr expr = SQLUtils.toSQLExpr(&quot;id=3&quot;, dbType); 怎样打印AST节点通过SQLUtils工具类打印节点123456789package com.alibaba.druid.sql; public class SQLUtils { // 可以将SQLExpr/SQLStatement打印为String类型 static String toSQLString(SQLObject sqlObj, String dbType); // 可以将一个&amp;lt;SQLStatement&amp;gt;打印为String类型 static String toSQLString(List&lt;SQLStatement&gt; statementList, String dbType);} 在Select语句中添加Group by这个先简单的做一个场景，依然拿本次这个项目来说，在前端发送SUM、COUNT等聚合表达式的SQL时，我们需要在语句后面添加Group by。 123456789101112131415161718192021222324252627282930313233343536373839404142public static String getSql(String sql, int offset, int count) { List&lt;SQLStatement&gt; stmtList = SQLUtils.parseStatements(sql, JdbcConstants.MYSQL); SQLSelectStatement sqlSelectStatement = (SQLSelectStatement) stmtList.get(0); SQLSelectQuery sqlSelectQuery = sqlSelectStatement.getSelect().getQuery(); MySqlSchemaStatVisitor visitor = new MySqlSchemaStatVisitor(); sqlSelectStatement.accept(visitor); if (sqlSelectQuery instanceof SQLSelectQueryBlock) { SQLSelectQueryBlock sqlSelectQueryBlock = (SQLSelectQueryBlock) sqlSelectQuery; List&lt;TableStat.Column&gt; cols = new ArrayList&lt;&gt;(visitor.getColumns()); SQLExpr where = sqlSelectQueryBlock.getWhere(); // 获取字段列表 List&lt;SQLSelectItem&gt; selectItems = sqlSelectQueryBlock.getSelectList(); int idx = 0; boolean hasAggregate = false; SQLSelectGroupByClause sqlSelectGroupByClause = new SQLSelectGroupByClause(); //判断是否有聚合函数 for (int selectIdx = 0; selectIdx &lt;= selectItems.size() - 1; selectIdx++) { if (selectItems.get(selectIdx).getExpr() instanceof SQLAggregateExpr) { hasAggregate = true; break; } } if (hasAggregate) { //添加SQLSelectGroupByClause字段 for (int colIndex = 0; colIndex &lt;= cols.size() - 1; colIndex++) { if (cols.get(colIndex).isSelect()) { sqlSelectGroupByClause.addItem(SQLUtils.toSQLExpr(cols.get(colIndex).toString(), JdbcConstants.MYSQL)); } } } sqlSelectQueryBlock.setGroupBy(sqlSelectGroupByClause); if (count &gt;= 1) { return PagerUtils.limit(sqlSelectStatement.toString(), JdbcConstants.MYSQL, offset, count); } return sqlSelectStatement.toString(); } return &quot;&quot;; } 输入sql语句： 1SELECT SUM(`address`) `address`,SUM(`address1`) `address1` FROM `demo` WHERE 1 = 1 最终结果: 1234SELECT SUM(`address`) AS `address`, SUM(`address1`) AS `address1`FROM `demo`WHERE 1 = 1GROUP BY address, address1 生成分页 12345SELECT SUM(`address`) AS `address`, SUM(`address1`) AS `address1`FROM `demo`WHERE 1 = 1GROUP BY address, address1LIMIT 1, 100 也可以创建一个SQLSelect对象来生成sql语句: 1234567SQLSelect sqlSelect = new SQLSelect();MySqlSelectQueryBlock sqlSelectQueryBlock = new MySqlSelectQueryBlock();sqlSelectQueryBlock.addSelectItem(new SQLSelectItem(new SQLIdentifierExpr(&quot;col1&quot;))); //查询列名sqlSelectQueryBlock.addSelectItem(new SQLSelectItem(new SQLIdentifierExpr(&quot;col2&quot;))); //查询列名sqlSelectQueryBlock.setFrom(new SQLExprTableSource(new SQLIdentifierExpr(&quot;demo&quot;))); //表名sqlSelect.setQuery(sqlSelectQueryBlock);String sql = SQLUtils.toSQLString(sqlSelect); Druid支持多种数据库的sql解析,在JdbcConstants中可以看到 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public interface JdbcConstants { String JTDS = &quot;jtds&quot;; String MOCK = &quot;mock&quot;; String HSQL = &quot;hsql&quot;; String DB2 = &quot;db2&quot;; String DB2_DRIVER = &quot;com.ibm.db2.jcc.DB2Driver&quot;; String POSTGRESQL = &quot;postgresql&quot;; String POSTGRESQL_DRIVER = &quot;org.postgresql.Driver&quot;; String SYBASE = &quot;sybase&quot;; String SQL_SERVER = &quot;sqlserver&quot;; String SQL_SERVER_DRIVER = &quot;com.microsoft.jdbc.sqlserver.SQLServerDriver&quot;; String SQL_SERVER_DRIVER_SQLJDBC4 = &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;; String SQL_SERVER_DRIVER_JTDS = &quot;net.sourceforge.jtds.jdbc.Driver&quot;; String ORACLE = &quot;oracle&quot;; String ORACLE_DRIVER = &quot;oracle.jdbc.OracleDriver&quot;; String ORACLE_DRIVER2 = &quot;oracle.jdbc.driver.OracleDriver&quot;; String ALI_ORACLE = &quot;AliOracle&quot;; String ALI_ORACLE_DRIVER = &quot;com.alibaba.jdbc.AlibabaDriver&quot;; String MYSQL = &quot;mysql&quot;; String MYSQL_DRIVER = &quot;com.mysql.jdbc.Driver&quot;; String MYSQL_DRIVER_6 = &quot;com.mysql.cj.jdbc.Driver&quot;; String MYSQL_DRIVER_REPLICATE = &quot;com.mysql.jdbc.&quot;; String MARIADB = &quot;mariadb&quot;; String MARIADB_DRIVER = &quot;org.mariadb.jdbc.Driver&quot;; String DERBY = &quot;derby&quot;; String HBASE = &quot;hbase&quot;; String HIVE = &quot;hive&quot;; String HIVE_DRIVER = &quot;org.apache.hive.jdbc.HiveDriver&quot;; String H2 = &quot;h2&quot;; String H2_DRIVER = &quot;org.h2.Driver&quot;; String DM = &quot;dm&quot;; String DM_DRIVER = &quot;dm.jdbc.driver.DmDriver&quot;; String KINGBASE = &quot;kingbase&quot;; String KINGBASE_DRIVER = &quot;com.kingbase.Driver&quot;; String GBASE = &quot;gbase&quot;; String GBASE_DRIVER = &quot;com.gbase.jdbc.Driver&quot;; String XUGU = &quot;xugu&quot;; String XUGU_DRIVER = &quot;com.xugu.cloudjdbc.Driver&quot;; String OCEANBASE = &quot;oceanbase&quot;; String OCEANBASE_DRIVER = &quot;com.mysql.jdbc.Driver&quot;; String INFORMIX = &quot;informix&quot;; /** * 阿里云odps */ String ODPS = &quot;odps&quot;; String ODPS_DRIVER = &quot;com.aliyun.odps.jdbc.OdpsDriver&quot;; String TERADATA = &quot;teradata&quot;; String TERADATA_DRIVER = &quot;com.teradata.jdbc.TeraDriver&quot;; /** * Log4JDBC */ String LOG4JDBC = &quot;log4jdbc&quot;; String LOG4JDBC_DRIVER = &quot;net.sf.log4jdbc.DriverSpy&quot;; String PHOENIX = &quot;phoenix&quot;; String PHOENIX_DRIVER = &quot;org.apache.phoenix.jdbc.PhoenixDriver&quot;; String ENTERPRISEDB = &quot;edb&quot;; String ENTERPRISEDB_DRIVER = &quot;com.edb.Driver&quot;; String KYLIN = &quot;kylin&quot;; String KYLIN_DRIVER = &quot;org.apache.kylin.jdbc.Driver&quot;; String SQLITE = &quot;sqlite&quot;; String SQLITE_DRIVER = &quot;org.sqlite.JDBC&quot;; String ALIYUN_ADS = &quot;aliyun_ads&quot;; String ALIYUN_DRDS = &quot;aliyun_drds&quot;; String PRESTO = &quot;presto&quot;; String PRESTO_DRIVER = &quot;com.facebook.presto.jdbc.PrestoDriver&quot;; String ELASTIC_SEARCH = &quot;elastic_search&quot;; String ELASTIC_SEARCH_DRIVER = &quot;com.alibaba.xdriver.elastic.jdbc.ElasticDriver&quot;; String CLICKHOUSE = &quot;clickhouse&quot;; String CLICKHOUSE_DRIVER = &quot;ru.yandex.clickhouse.ClickHouseDriver&quot;;} github","link":"/posts/547140718.html"},{"title":"AccessibilityService自动刷视频","text":"使用AccessibilityService实现自动刷视频，赚点小钱 最近很多人在推精简版的快手、抖音等app来赚钱，邀请人之后，可得佣金，每天刷视频也可以的金币换RMB，所以一些闲来无事的人就整体拿着手机刷刷刷，那么有没有什么自动的方式，让我们解放双手呢？Android自动化测试的框架比较多，大家可以自行百度。下面就介绍几种常用的： 模拟MotionEvent这个功能只能给自己本身的app发送Event，无法跨App。 Instrumentation现在有很多基于Instrumentation的自动化测试框架，但是也无法跨App操作，如果想要跨App的话，就只有获得root权限或者系统签名。这两种方式都有些麻烦。 ADB命令需要在PC端执行adb命令，那么就需要USB连接到电脑上，在PC端发送input的shell脚本命令，如果再手机端执行input tap等命令，也需要root权限。操作还是很麻烦 AccessibilityService服务现在很多复制工具都是基于AccessibilityService开发的，通过给予一定的权限，监听手机端的动作，然后查找相应节点，向指定节点发送相应的指令。为了节约时间，我在百度找到一篇文章AccessibilityService实现文本的自动全选黏贴、点击、滑动。利用dispatchGesture+AccessibilityService来实现自动刷新视频，我将其稍作修改，就可以对刷抖音、快手、趣多多等app进行跨应用刷视频，这样一来省了不少时间。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private void slideVertical(int startSlideRatio, int stopSlideRatio) { sliderCount++; int screenHeight = ScreenUtils.getScreenHeight(getApplicationContext()); int screenWidth = ScreenUtils.getScreenWidth(getApplicationContext()); L.e(&quot;屏幕：&quot; + (screenHeight - (screenHeight / 10)) + &quot;/&quot; + (screenHeight - (screenHeight - (screenHeight / 10))) + &quot;/&quot; + screenWidth / 2); Path path = new Path(); int start = (screenHeight / 20) * startSlideRatio; int stop = (screenHeight / 20) * stopSlideRatio; L.e(&quot;屏幕：&quot; + start + &quot;/&quot; + stop + &quot;/&quot; + screenWidth / 2); path.moveTo(screenWidth / 2, start);//如果只是设置moveTo就是点击 path.lineTo(screenWidth / 2, stop);//如果设置这句就是滑动 StringBuffer sb = new StringBuffer(); sb.append(&quot;滑动次数&quot; + sliderCount + &quot;次\\n&quot;); sb.append(&quot;滑动时间&quot; + Utils.formatUTC(System.currentTimeMillis(),null) + &quot;\\n&quot;); sb.append(&quot;开始位置&quot; + start + &quot;\\n&quot;); sb.append(&quot;结束位置&quot; + stop + &quot;\\n&quot;); Intent mIntent = new Intent(MainActivity.RECEIVER_ACTION); mIntent.putExtra(&quot;result&quot;, sb.toString()); //发送广播 sendBroadcast(mIntent); GestureDescription.Builder builder = new GestureDescription.Builder(); GestureDescription gestureDescription = builder .addStroke(new GestureDescription. StrokeDescription(path, 200, 200)) .build(); dispatchGesture(gestureDescription, new GestureResultCallback() { @Override public void onCompleted(GestureDescription gestureDescription) { super.onCompleted(gestureDescription); L.w(&quot;滑动结束&quot; + gestureDescription.getStrokeCount()); } @Override public void onCancelled(GestureDescription gestureDescription) { super.onCancelled(gestureDescription); L.w(&quot;滑动取消&quot;); } }, null); } 效果图:","link":"/posts/1720353157.html"},{"title":"Go在Windows下编译linux可执行二进制文件","text":"完成Golang应用开发之后，接下来肯定就是编译成可执行文件，如果开发环境是Windows的话，我们想要编译成可执行文件会非常方便,执行go build即可。但是我们想要编译成Linux环境下可执行的文件呢？ 完成Golang应用开发之后，接下来肯定就是编译成可执行文件，如果开发环境是Windows的话，我们想要编译成可执行文件会非常方便,执行go build即可。但是我们想要编译成Linux环境下可执行的文件呢？我们需要修改几个参数： 1234SET CGO_ENABLED=0SET GOARCH=amd64SET GOOS=linuxgo build main.go 一次执行以上命令，就可以在项目目录生成一个二进制文件，所修改的变量值仅对当前窗口有效，可以如果我们每次都需要输入这些命令的话，着实还是有些繁琐，那么，我们可以新建一个bat文件，将命令复制到bat文件中，以后每次就只需要执行这个bat文件即可。如果又想编译成Windows文件，要么就是关闭窗口重新打开，要么就是修改变量值： 1234SET CGO_ENABLED=1SET GOARCH=SET GOOS=windowsgo build main.go","link":"/posts/1275392227.html"},{"title":"Golang基础学习总结之变量","text":"变量来源于数学，是计算机语言中能储存计算结果或能表示值抽象概念。变量可以通过变量名访问。Go 语言变量名由字母、数字、下划线组成，其中首个字符不能为数字。来源于https://www.runoob.com/go/go-variables.html Go语言定义变量有多种方式，下面我们来看看具体的示例var [变量名] [变量类型] = [值] 1var a int = 10 //定义变量并初始化值 也可以不赋值，如果未显示初始化值的时候，Go会自动为对应数据类型初始化一个默认值(零值)，: 1var a int //自动初始化零值 Go 语言中的零值大概有以下几种: 类型 零值 数值 0 布尔 false 字符串 “”(空字符串) slice nil map nil 指针 nil 函数 nil 接口 nil 信道 nil Go变量会根据初始值自动判断数据类型: 1var a = 10 简短定义: 1a := 10 多变量同类型定义: 12var a,b intvar a,b int = 0,1 多变量不同类型定义: 123456789101112var a ,b = 0,&quot;b&quot;var( a int b string )var( a int=0 b string=&quot;b&quot; )a ,b :=1,2 变量作用域,我理解的变量作用域有三种: 全局作用域 局部作用域 块级作用域 1234567891011121314151617package mainvar a=10 //全局作用域func main() { a := 9 //局部作用域 { a :=8 //块级作用域 println(a) } println(a) println(Get())}func Get() int { return a} 打印结果： 1238910 如有错误之处，敬请提建议，多谢。","link":"/posts/3566702391.html"},{"title":"Golang基础学习总结之开篇","text":"Go, also known as Golang, is a statically typed, compiled programming language designed at Google by Robert Griesemer, Rob Pike, and Ken Thompson. Go is syntactically similar to C, but with memory safety, garbage collection, structural typing, and CSP-style concurrency 几个月前，兴趣使然，我偶然学了一个月的go语言，并开发了两个简单的小程序(短域名服务和微信记事本小程序)，感受到了golang的简洁和强大。可是我并没有做一个学习总结，所以接下来，我将简单是分几篇文章来简单的总结一下golang的基础内容。这里我们先简单列举几个Golang的优点： 1、语法简洁 2、强大的标准库 3、性能好 4、开发效率高 5、天生支持并发 6、编译效率高 …… Hello World一般学习一门新语言，都是从Hello world开始的，下面，我就来感受一下Golang的简洁，在这里我推荐大家使用Goland这款IDE,当然如果vs code用得很6的话也可以。 12345678package main //包名import &quot;fmt&quot; 导入fmt包//main主函数func main() { fmt.Println(&quot;Hello World&quot;) //打印输出Hello World} Go语言以包(package)作为管理单位，每个Go 源文件都必须先声明它所属的包，Go语言的包与文件夹是一一对应的，默认情况下同级目录下的文件属于同一个包，包名也可以与目录名不同。main包是Go语言程序的入口包，一个Go语言程序必须有且仅有一个 main 包。如果一个程序没有 main 包，那么编译时将会出错，无法生成可执行文件。main函数是main包里面的启动函数，如果没有main函数启动也是会报错。go预览编译也很简单，直接到项目目录执行go build 即可在同级目录生成一个exe的可执行文件。","link":"/posts/4272109955.html"},{"title":"工作中常用的抓包工具-Fiddler和Wireshark介绍","text":"我们这个信息化时代，每天都不知不觉的会给不知道哪些软件运营商偷偷的说一些悄悄话，特别是我们程序猿，在开发过程中更是，偶尔会遇到不知道TCP交互过程中到底传递或者接受了哪些信息，给我们的Debug蒙上了一层神秘的面纱，这时候，我们需要一些抓包工具帮助我们轻松Debug。一般的请求可以借助浏览器自带的NetWork抓包工具，移动端一些网页的话可以借助腾讯开源的vConsole，再高端一点就是借助Fiddler来抓取http或者https请求。但是有时候这样也满足不了我们的需要，那么就再祭出Wireshark神器来抓取tcp和udp请求。 浏览器自带的抓包工具一些主流浏览器都自带有Network抓包工具,只需F12即可唤出,非常方便,我这里只演示了 Chrome浏览器的抓包过程。 Fiddler简单操作Fiddler是一款http\\https协议代理调试工具，它能够获取请求之间的数据和状态，设置断点，以及修改数据。首先去Fiddler网站下载并手动安装。安装成功后打开Fiddler，界面如下:默认情况下，它是抓取我们所有的请求。我们需要对Fiddler进行过滤设置。假如我们只想抓取www.cnblogs.com的请求，我们需要这样处理：,最后我们可以得到这样的效果:,还可以指定客户端的进程:进程方式除了刚才在Filters中设置外，还可以使用拖动方式，下面，我们来试试抓取手机端的请求，在抓取手机端请求前，请确保手机和电脑在同一个局域网中，然后进入Tools-&gt;Options-&gt;Connections对Fiddler进行如下设置:,由于我在PC端映射了一个wifi供手机连接，故我手机代理设置如下:假如我们抓取美团App的请求，现在打开App即可看到:这里只是一个简单的示例介绍，更多高深技巧请自行探索。 Wireshark简单操作首先Wireshark下载，安装完成后打开可以看到我们本地有很多网卡驱动，我们怎么知道现在网络是哪个网卡呢？有两个方式，一个是查看本地连接，另外一个就是看界面的网络流量。可以看到Wireshark开始为我们抓包，由于Wireshark为我们抓取了TCP和HTTP，会有很多无用的信息导致我们无从下手，这时候可以使用Wireshark的过滤规则:,在过滤栏输入关键字就会有相应的智能提示。 在Wireshark抓包中有多层信息: Frame 15788: 79 bytes on wire (632 bits), 79 bytes captured (632 bits) on interface \\Device\\NPF_{69B14E8C-A0A5-4064-9CF7-EB4651D24A99}, id 0：物理层的数据帧概况。 Ethernet II, Src: HuaweiTe_22:5a:c6 (fc:48:ef:22:5a:c6), Dst: WistronI_fd:67:50 (54:ee:75:fd:67:50)：数据链路层以太网帧头部信息。 Internet Protocol Version 4, Src: 10.101.11.97, Dst: 10.101.27.241：互联网层IP包头部信息,源地址和目的地址。 Transmission Control Protocol, Src Port: 80, Dst Port: 50773, Seq: 2606, Ack: 3560, Len: 25：传输层的数据段头部信息，TCP协议层信息。 Hypertext Transfer Protocol：应用层的信息，此处是HTTP\\HTTPS协议会现在该项。 Wireshark常用过滤条件: 地址过滤。ip.dst==xxx.xx.xxx.xxx 过滤目的地址为xxx.xx.xxx.xxx，ip.src==xxx.xx.xxx.xxx 过滤原地址地址为xxx.xx.xxx.xxx,ip.dst==xxx.xx.xxx.xxx &amp;&amp; ip.src==xxx.xx.xxx.xxx 过滤源地址为xxx.xx.xxx.xxx且目的地址xxx.xx.xxx.xxx 协议过滤。直接在过滤栏输入协议即可 tcp、udp、arp、http、ftp、ssl、smtp、dns、ip、ardp 端口过滤。在过滤栏输入tcp.port==端口号(目的端口和源端口)或者 tcp.dstport==端口号(目的端口)，tcp.srcport==端口号(源端口) HTTP过滤。这个就比较多了，暂时详述: 通过Wireshark可以很方便的分析TCP的三次握手和四次挥手的过程，有助于我们更佳直观的理解TCP协议。 好了，暂时先告一段落吧，这里只是简单是写了软件的基本使用，如果要抓取HTTP和HTTPS协议的数据的话，个人建议使用Fiddler。其他更加高级的应用在后面再分享吧。","link":"/posts/62780299.html"},{"title":"Hexo SEO优化记录","text":"写博客其实是对自己工作经验和学过程的总结，可能写得不好，但是还是希望自己踩过的坑能让更多人去避开我们踩过的坑。所以，在自己记录的同时，更希望帮助更多的人去解决一些问题。但是我们写的东西别人一直都搜索不到，怎么办呢？要让别人知道，要么就是自己主动分享，要么就是朋友们自己搜索，我们今天是来说后者，那就是如何让百度收录我们的博客。 百度搜索其实还是挺严格的，它回去分析你博客的质量，如果质量太差也不会收录，当然这需要多学多练多写多总结。我们这次只总结搜索引擎的SEO优化。 生成sitemap这里我们需要安装三个插件 12npm install hexo-generator-sitemap --save-dev # 传统的sitemapnpm install hexo-generator-baidu-sitemap --save-dev # 百度sitemap 插件安装完成后，执行hexo g后，会在public目录下生成sitemap.xml和baidusitemap.xml文件,我们留着备用。 缩短Hexo生成的URL目录层级根据其他的SEO经验,URL层级越短越容易被抓取,有助于提升搜索结果排名,建议保持最多三个层级。默认情况下，Hexo给我们生成的规则是: 1permalink: :year/:month/:day/:title/ # 目录为 https://domain/year/month/day/title/ 目录层级有点，我们需要优化一下: 1permalink: :year-:month-:day/:title.html # 目录为 https://domain/year-month-day/title.html 或者安装另外一个插件hexo-abbrlink,它(github)可以简化我们的URL,它的配置参数效果如下: 123456789101112crc16 &amp; hexhttps://blogs.52fx.biz/posts/66c8.htmlcrc16 &amp; dechttps://blogs.52fx.biz/posts/65535.htmlcrc32 &amp; hexhttps://blogs.52fx.biz/posts/8ddf18fb.htmlcrc32 &amp; dechttps://blogs.52fx.biz/posts/1690090958.html 1npm install hexo-abbrlink --save 然后修改根目录_config.yml中的配置, 12345permalink: posts/:abbrlink.html# abbrlink configabbrlink: alg: crc32 #support crc16(default) and crc32 rep: dec #support dec(default) and hex 添加 nofollow 标签据百度百科介绍: nofollow标签是由谷歌领头创新的一个“反垃圾链接”的标签，并被百度、yahoo等各大搜索引擎广泛支持，引用nofollow标签的目的是：用于指示搜索引擎不要追踪（即抓取）网页上的带有nofollow属性的任何出站链接，以减少垃圾链接的分散网站权重！简单的说就是，如果A网页上有一个链接指向B网页，但A网页给这个链接加上了 rel=”nofollow” 标注，则搜索引擎不把A网页计算入B网页的反向链接。搜索引擎看到这个标签就可能减少或完全取消链接的投票权重。 所以我们需要对我们的Hexo的超链接加上这个标签。我的路径是themes\\icarus\\layout\\common\\footer.ejs: 12Powered by &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot;&gt;Hexo&lt;/a&gt; &amp; &lt;a href=&quot;https://github.com/ppoffice/hexo-theme-icarus&quot; target=&quot;_blank&quot;&gt;Icarus&lt;/a&gt; 改为: 12Powered by &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow&quot;&gt;Hexo&lt;/a&gt; &amp; &lt;a href=&quot;https://github.com/ppoffice/hexo-theme-icarus&quot; target=&quot;_blank&quot; rel=&quot;external nofollow&quot;&gt;Icarus&lt;/a&gt; 百度站长提交连接我们先登录到百度的站点管理,没有账号的请自行注册。然后我们点击添加站点这里根据自己的实际情况填写然后下一步,自己勾选，然后再下一步。就是验证域名这里于三种方式，都有详细的说明，我选择的第三种CNAME验证,大家自行选择。解析完域名后稍等数分钟，点击完成验证，然后就OK，接下来我们生成sitemap文件并上传到七牛云，为什么要上传到七牛云呢，因为github禁止了搜索引擎爬虫，会导致百度抓取失败。或者把静态页面分别放置在github和coding上面，针对百度解析到不同的服务器上。至于如何提交到七牛云，请翻看我之前的文章，这里不再赘述。在生成之前切记要修改_config.yml的url属性.现在我们来访问一下http://blogs.52fx.biz/sitemap.xml可以看到接下来，把我们的sitemap文件提交到百度自动提交配置中.进入链接提交-&gt;自动提交-&gt;sitemap进行配置.提交后会有一个抓取队列,可以时刻关注状态，但是这种方式有点慢。大家看上面的图片，也能知道有三种方式提交，上面只是其中一种，还有两种，我们来分别讲解一下自动推送这种方式，也比较简单，在对象的主题的footer.ejs中加入: 1234567891011121314&lt;script&gt;(function(){ var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) { bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; } else { bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; } var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);})();&lt;/script&gt; 即可，每次访问的时候，页面就会向百度提交。主动推送(实时),这个就需要我们安装一个插件： 1npm install hexo-baidu-url-submit --save 然后在_config.yml配置 12345678910111213141516deploy: - type: git repository: github: git@github.com:51offer-tech/51offer-tech.github.io.git #coding: git@git.coding.net:offer-tech/51offer-tech.git coding: https://e.coding.net/offer-tech/offer-tech.git gitee: https://gitee.com/eyiadmin/offer-tech.git branch: master - type: baidu_url_submitter# 百度链接自动提交baidu_url_submit: count: 1000 ## 提交最新的一个链接 host: http://blogs.52fx.biz ## 在百度站长平台中注册的域名 token: xxxxxxxx ## 请注意这是您的秘钥， 所以请不要把博客源代码发布在公众仓库里! path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里 详细的资料大家可以去hexo-baidu-url-submit 的github查看. Hexo Keywords关键字设置由于我用的hexo-theme-icarus,我也没有过多的去查找资料，就直接在page.js中手动加了一个: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354hexo.extend.helper.register(&apos;page_keywords&apos;, function (page = null) { page = page === null ? this.page : page; let title = page.title; if (this.is_archive()) { title = this._p(&apos;common.archive&apos;, Infinity); if (this.is_month()) { title += &apos;: &apos; + page.year + &apos;/&apos; + page.month; } else if (this.is_year()) { title += &apos;: &apos; + page.year; } } else if (this.is_category()) { title = this._p(&apos;common.category&apos;, 1) + &apos;: &apos; + page.category; } else if (this.is_tag()) { title = this._p(&apos;common.tag&apos;, 1) + &apos;: &apos; + page.tag; } else if (this.is_categories()) { title = this._p(&apos;common.category&apos;, Infinity); } else if (this.is_tags()) { title = this._p(&apos;common.tag&apos;, Infinity); } let keywords=page.keywords; const siteTitle = hexo.extend.helper.get(&apos;get_config&apos;).bind(this)(&apos;title&apos;, &apos;&apos;, true); keywords=keywords||hexo.extend.helper.get(&apos;get_config&apos;).bind(this)(&apos;keywords&apos;, &apos;&apos;, true); return [title, siteTitle,keywords].filter(str =&gt; typeof (str) !== &apos;undefined&apos; &amp;&amp; str.trim() !== &apos;&apos;).join(&apos; = &apos;); }); hexo.extend.helper.register(&apos;page_description&apos;, function (page = null) { page = page === null ? this.page : page; let title = page.title; if (this.is_archive()) { title = this._p(&apos;common.archive&apos;, Infinity); if (this.is_month()) { title += &apos;: &apos; + page.year + &apos;/&apos; + page.month; } else if (this.is_year()) { title += &apos;: &apos; + page.year; } } else if (this.is_category()) { title = this._p(&apos;common.category&apos;, 1) + &apos;: &apos; + page.category; } else if (this.is_tag()) { title = this._p(&apos;common.tag&apos;, 1) + &apos;: &apos; + page.tag; } else if (this.is_categories()) { title = this._p(&apos;common.category&apos;, Infinity); } else if (this.is_tags()) { title = this._p(&apos;common.tag&apos;, Infinity); } let description=page.description; const siteTitle = hexo.extend.helper.get(&apos;get_config&apos;).bind(this)(&apos;title&apos;, &apos;&apos;, true); description=description||hexo.extend.helper.get(&apos;get_config&apos;).bind(this)(&apos;description&apos;, &apos;&apos;, true); return [title, siteTitle,description].filter(str =&gt; typeof (str) !== &apos;undefined&apos; &amp;&amp; str.trim() !== &apos;&apos;).join(&apos; , &apos;); }); 然后在主题的head.ejs中加上： 123&lt;meta name=&quot;keywords&quot; content=&quot;&lt;%= page_keywords() %&gt;&quot;&gt;&lt;meta name=&quot;description&quot; content=&quot;&lt;%= page_description() %&gt;&quot;&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no&quot;/&gt; 最终效果 使用HTTPS 据说百度会给https的网站加权。我这里是直接配置的七牛云免费的SSL证书，当然是需要费用的，不过也很便宜 123第 0 GB 至 100 TB (含) 0.28 元/GB第 100 TB 至 1024 TB (含) 0.23 元/GB第 1024 TB 以上 0.18 元/GB 未完待续，持续更新中.","link":"/posts/747951114.html"},{"title":"NSQ实现消息延迟执行","text":"最近我给自己做了一个记事微信小程序,主要是怕自己把事情给忘了，虽然现在市面上有很多成熟的应用，但是作为程序猿，而且最近工作也不是那么忙，就想着自己折腾一个。既然是备忘，当然得有一个消息提醒，这就需要涉及到延迟执行。现在针对延迟执行也有许多方案，比如:定时扫描、消息队列等。定时扫描在时间上有延迟而且扫描频率影响着数据库的性能，另外还有定时轮(TimingWheel)算法，不过这里我还是选择了消息队列的方式。 支持延迟消息投递的消息队列也比较多,如:rabbitmq、rocketmq、kafka等都可以实现延迟消息。不过 NSQ具有分布式、无单点故障、故障容错、高可用性以及高可靠等特征,已有一些公司用于生产。毕竟我只有2台1核2G的服务器，所以我还是更加青睐于它。 NSQ部署在部署NSQ之前，我们首先得了解它的三个组件nsqd、nsqlookupd、nsqadmin: nsqd主要负责接收、存储和转发消息发给客户端 nsqlookupd主要负责nsqd的注册以及心跳、状态监控。 nsqadmin是消息的web管理界面 Windows部署NSQ我们这里先实践一下Windows的安装方式，首先需要去NSQ下载Windows版本并解压. 首先启动nsqlookupd: 1234E:\\tool\\nsq-1.2.0\\bin&gt;nsqlookupd[nsqlookupd] 2020/01/14 09:26:37.816513 INFO: nsqlookupd v1.2.0 (built w/go1.12.9)[nsqlookupd] 2020/01/14 09:26:37.882337 INFO: HTTP: listening on [::]:4161[nsqlookupd] 2020/01/14 09:26:37.882337 INFO: TCP: listening on [::]:4160 默认端口为http:4161、tcp:4160,可以通过分别指定-http-address和-tcp-address两个参数来修改。 启动nsqd实例: 123456789E:\\tool\\nsq-1.2.0\\bin&gt;nsqd --lookupd-tcp-address=0.0.0.0:4160 -data-path=&quot;D:/nsqdata&quot; --broadcast-address=0.0.0.0[nsqd] 2020/01/14 09:38:00.365419 INFO: nsqd v1.2.0 (built w/go1.12.9)[nsqd] 2020/01/14 09:38:00.403363 INFO: ID: 632[nsqd] 2020/01/14 09:38:00.408304 INFO: NSQ: persisting topic/channel metadata to D:/nsqdata/nsqd.dat[nsqd] 2020/01/14 09:38:00.453186 INFO: TCP: listening on [::]:4150[nsqd] 2020/01/14 09:38:00.453186 INFO: HTTP: listening on [::]:4151[nsqd] 2020/01/14 09:38:00.453186 INFO: LOOKUP(0.0.0.0:4160): adding peer[nsqd] 2020/01/14 09:38:00.500058 INFO: LOOKUP connecting to 0.0.0.0:4160[nsqd] 2020/01/14 09:38:00.523995 INFO: LOOKUPD(0.0.0.0:4160): peer info {TCPPort:4160 HTTPPort:4161 Version:1.2.0 BroadcastAddress:2CNU7X5OLAUE004} -lookupd-tcp-address:nsqlookupd的IP和tcp端口 -broadcast-address:会注册到nsqlookupd，填写自己主机IP -data-path:消息持久化的存储路径 NSQ支持延时消息的默认最长时间为3600000(60分钟),可以通过-max-req-timeout参数修改 最后启动我们的nsqadmin实例:123E:\\tool\\nsq-1.2.0\\bin&gt;nsqadmin --lookupd-http-address=0.0.0.0:4161[nsqadmin] 2020/01/14 09:38:34.709978 INFO: nsqadmin v1.2.0 (built w/go1.12.9)[nsqadmin] 2020/01/14 09:38:34.745941 INFO: HTTP: listening on [::]:4171 -lookupd-http-address:nsqlookupd的IP和http端口 上面我们演示的均是单个实例，仅适合于开发环境，以后有机会我们再来探究如何部署多实例搭建高可用NSQ。 Docker部署NSQNSQ支持跨平台，如果是部署在Linux的话，建议还是使用Docker部署，在NSQ官网有详细的Docker安装说明，这里我们参照官网实践一下。编辑我们的docker-compose文件,这里我命名为nsq.yml: 12345678910111213141516171819202122232425version: &apos;3&apos;services: nsqlookupd: image: nsqio/nsq command: /nsqlookupd ports: - &quot;4160&quot; - &quot;4161&quot; nsqd: image: nsqio/nsq command: /nsqd --lookupd-tcp-address=nsqlookupd:4160 –data-path=/data/nsqdata depends_on: - nsqlookupd ports: - &quot;4150&quot; - &quot;4151&quot; volumes: - /root/nsqdata:/data/nsqdata nsqadmin: image: nsqio/nsq command: /nsqadmin --lookupd-http-address=nsqlookupd:4161 depends_on: - nsqlookupd ports: - &quot;4171&quot; 我们稍作修改，将NSQD数据目录挂载到物理机。创建好yml配置文件后，就运行命令创建我们的容器: 123456789101112[root@instance-p0a4erj8 ~]# docker-compose -f nsq.yml up -dCreating network &quot;root_default&quot; with the default driverPulling nsqlookupd (nsqio/nsq:)...latest: Pulling from nsqio/nsq5d20c808ce19: Pull completee43ee7addbdb: Pull completecbc99497dda7: Pull completeDigest: sha256:78b986254986c4ae1237b32219a83c5a23354a6c30c18817597f776a4edcac41Status: Downloaded newer image for nsqio/nsq:latestCreating root_nsqlookupd_1 ... doneCreating root_nsqd_1 ... doneCreating root_nsqadmin_1 ... done 执行docker-compose ps查看我们的容器: 123456[root@instance-p0a4erj8 ~]# docker-compose -f nsq.yml ps Name Command State Ports -----------------------------------------------------------------------------------------------------------------------------------------------------root_nsqadmin_1 /nsqadmin --lookupd-http-a ... Up 4150/tcp, 4151/tcp, 4160/tcp, 4161/tcp, 4170/tcp, 0.0.0.0:32772-&gt;4171/tcp root_nsqd_1 /nsqd --lookupd-tcp-addres ... Up 0.0.0.0:32771-&gt;4150/tcp, 0.0.0.0:32770-&gt;4151/tcp, 4160/tcp, 4161/tcp, 4170/tcp, 4171/tcproot_nsqlookupd_1 /nsqlookupd Up 4150/tcp, 4151/tcp, 0.0.0.0:32769-&gt;4160/tcp, 0.0.0.0:32768-&gt;4161/tcp, 4170/tcp, 4171/tcp 如果需要删除,需要先stop,在rm: 12345678910[root@instance-p0a4erj8 ~]# docker-compose -f nsq.yml stopStopping root_nsqadmin_1 ... doneStopping root_nsqd_1 ... doneStopping root_nsqlookupd_1 ... done[root@instance-p0a4erj8 ~]# docker-compose -f nsq.yml rmGoing to remove root_nsqadmin_1, root_nsqd_1, root_nsqlookupd_1Are you sure? [yN] yRemoving root_nsqadmin_1 ... doneRemoving root_nsqd_1 ... doneRemoving root_nsqlookupd_1 ... done 刚才看到，如果不指定端口，容器启动时随机分配绑定的主机端口号，这样就会比较乱，所以要指定端口: 12345678910111213141516171819202122232425version: &apos;3&apos;services: nsqlookupd: image: nsqio/nsq command: /nsqlookupd ports: - &quot;14160:4160&quot; - &quot;14161:4161&quot; nsqd: image: nsqio/nsq command: /nsqd --lookupd-tcp-address=nsqlookupd:4160 –data-path=/data/nsqdata depends_on: - nsqlookupd ports: - &quot;14150:4150&quot; - &quot;14151:4151&quot; volumes: - /root/nsqdata:/data/nsqdata nsqadmin: image: nsqio/nsq command: /nsqadmin --lookupd-http-address=nsqlookupd:4161 depends_on: - nsqlookupd ports: - &quot;14171:4171&quot; 我们在重新创建NSQ容器: 12345678910[root@instance-p0a4erj8 ~]# docker-compose -f nsq.yml up -dCreating root_nsqlookupd_1 ... doneCreating root_nsqadmin_1 ... doneCreating root_nsqd_1 ... done[root@instance-p0a4erj8 ~]# docker-compose -f nsq.yml ps Name Command State Ports -----------------------------------------------------------------------------------------------------------------------------------------------------root_nsqadmin_1 /nsqadmin --lookupd-http-a ... Up 4150/tcp, 4151/tcp, 4160/tcp, 4161/tcp, 4170/tcp, 0.0.0.0:14171-&gt;4171/tcp root_nsqd_1 /nsqd --lookupd-tcp-addres ... Up 0.0.0.0:14150-&gt;4150/tcp, 0.0.0.0:14151-&gt;4151/tcp, 4160/tcp, 4161/tcp, 4170/tcp, 4171/tcproot_nsqlookupd_1 /nsqlookupd Up 4150/tcp, 4151/tcp, 0.0.0.0:14160-&gt;4160/tcp, 0.0.0.0:14161-&gt;4161/tcp, 4170/tcp, 4171/tcp 现在我们就可以查看NSQ的状态: 123456789101112131415161718192021[root@instance-p0a4erj8 ~]# curl http://127.0.0.1:14151/statsnsqd v1.2.0 (built w/go1.12.9)start_time 2020-01-14T08:50:35Zuptime 1m9.272087092sHealth: OKMemory: heap_objects 1863 heap_idle_bytes 65486848 heap_in_use_bytes 1130496 heap_released_bytes 0 gc_pause_usec_100 0 gc_pause_usec_99 0 gc_pause_usec_95 0 next_gc_bytes 4473924 gc_total_runs 0Topics: NoneProducers: None 关于NSQ AUTH方面的知识点，大家可以看看https://nsq.io/components/nsqd.html#auth。 代码示例在NSQ官网我们可以看到支持多种语言，这里我还是使用Golang,Producer: 1234567891011121314151617181920func main() { config := nsq.NewConfig() topicName := &quot;nsq_demo&quot; msgCount := 10 producer, _ := nsq.NewProducer(&quot;127.0.0.1:4150&quot;, config) err := producer.Ping() if err != nil { log.Fatal(&quot;should not be able to ping after Stop()&quot;) return } defer producer.Stop() for i := 1; i &lt; msgCount; i++ { fmt.Println(fmt.Sprintf(&quot;test nsq message index:%d&quot;, i)) err1 := producer.Publish(topicName, []byte(fmt.Sprintf(&quot;test nsq message index:%d&quot;, i))) if err1 != nil { log.Fatal(&quot;error&quot;,err1) } }} Comsumer: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354type ComsumerHandler struct {}func (h *ComsumerHandler) HandleMessage(message *nsq.Message) error { if string(message.Body) == &quot;TOBEFAILED&quot; { return errors.New(&quot;fail this message&quot;) } fmt.Println(&quot;receive&quot;, message.NSQDAddress, &quot;message:&quot;, string(message.Body)) return nil}func main() { waiter := sync.WaitGroup{} waiter.Add(1) go func() { defer waiter.Done() config := nsq.NewConfig() laddr := &quot;127.0.0.1&quot; // so that the test can simulate binding consumer to specified address config.LocalAddr, _ = net.ResolveTCPAddr(&quot;tcp&quot;, laddr+&quot;:0&quot;) // so that the test can simulate reaching max requeues and a call to LogFailedMessage config.DefaultRequeueDelay = 0 // so that the test wont timeout from backing off config.MaxBackoffDuration = time.Millisecond * 50 topicName := &quot;nsq_demo&quot; if config.Deflate { topicName = topicName + &quot;_deflate&quot; } else if config.Snappy { topicName = topicName + &quot;_snappy&quot; } if config.TlsV1 { topicName = topicName + &quot;_tls&quot; } consumer, _ := nsq.NewConsumer(topicName, &quot;ch&quot;, config) handler := &amp;ComsumerHandler{ } consumer.AddHandler(handler) err := consumer.ConnectToNSQD(&quot;127.0.0.1:4150&quot;) if nil != err { fmt.Println(&quot;err&quot;, err) return } select{} }() waiter.Wait()} ,我主要的目的是实现延迟执行，所以这里需要尝试NSQ延迟消息投递功能,我们稍作修改:Producer: 1234567891011121314151617181920212223unc main() { config := nsq.NewConfig() topicName := &quot;nsq_demo&quot; msgCount := 10 producer, _ := nsq.NewProducer(&quot;127.0.0.1:4150&quot;, config) err := producer.Ping() if err != nil { log.Fatal(&quot;should not be able to ping after Stop()&quot;) return } defer producer.Stop() for i := 1; i &lt; msgCount; i++ { fmt.Println(fmt.Sprintf(&quot;test nsq message index:%d&quot;, i)) //err1 := producer.Publish(topicName, []byte(fmt.Sprintf(&quot;test nsq message index:%d&quot;, i))) delay := rand.Intn(10) //延时消息，delay为延迟多少秒投递。 err1 := producer.DeferredPublish(topicName, time.Second * time.Duration(delay), []byte(fmt.Sprintf(&quot;test nsq message index:%d,time:%s,delay:%d&quot;, i, time.Now().Format(&quot;2020-01-14 15:04:05&quot;), delay))) if err1 != nil { log.Fatal(&quot;error&quot;, err1) } }} Comsumer可以不用修改，但是我还是加上时间打印: 1fmt.Println(&quot;time&quot;,time.Now().Format(&quot;2020-01-14 15:04:05&quot;),&quot;receive&quot;, message.NSQDAddress, &quot;message:&quot;, string(message.Body)) 我们再来看看效果: 123456789time 14140-01-119 16:19:44 receive 127.0.0.1:4150 message: test nsq message index:8,time:14140-01-119 16:19:44,delay:0time 14140-01-119 16:19:45 receive 127.0.0.1:4150 message: test nsq message index:1,time:14140-01-119 16:19:44,delay:1time 14140-01-119 16:19:45 receive 127.0.0.1:4150 message: test nsq message index:5,time:14140-01-119 16:19:44,delay:1time 14140-01-119 16:19:49 receive 127.0.0.1:4150 message: test nsq message index:7,time:14140-01-119 16:19:44,delay:5time 14140-01-119 16:19:50 receive 127.0.0.1:4150 message: test nsq message index:9,time:14140-01-119 16:19:44,delay:6time 14140-01-119 16:19:51 receive 127.0.0.1:4150 message: test nsq message index:2,time:14140-01-119 16:19:44,delay:7time 14140-01-119 16:19:51 receive 127.0.0.1:4150 message: test nsq message index:3,time:14140-01-119 16:19:44,delay:7time 14140-01-119 16:19:52 receive 127.0.0.1:4150 message: test nsq message index:6,time:14140-01-119 16:19:44,delay:8time 14140-01-119 16:19:53 receive 127.0.0.1:4150 message: test nsq message index:4,time:14140-01-119 16:19:44,delay:9","link":"/posts/3853429767.html"},{"title":"IntelliJ IDEA 常用快捷键整理","text":"IntelliJ IDEA是为了最大限度地提高开发者的生产力而设计的，其强大的静态代码分析和人机工程学设计，为源代码编制了索引后，通过在各种上下文中提供相关建议，而且提供了快速而智能的体验,比如代码自动补全、即时的代码分析和可靠的重构工具。 IntelliJ 最常用的几个快捷键组合Ø Top #10切来切去：Ctrl+Tab Ø Top #9选你所想：Ctrl+W Ø Top #8代码生成：Template/Postfix +Tab Ø Top #7发号施令：Ctrl+Shift+A Ø Top #6无处藏身：Shift+Shift Ø Top #5自动完成：Ctrl+Shift+Enter Ø Top #4创造万物：Alt+Insert Ø Top #3智能补全：Ctrl+Shift+Space Ø Top #2自我修复：Alt+Enter Ø Top #1重构一切：Ctrl+Shift+Alt+T 代码快速生成缩写 缩写 内容 fori for (int i = 0; i &lt; ; i++) {} psvm public static void main(String[] args){} sout System.out.println(); (在 Kotlin 中是 println()) souf System.out.printf(); serr System.err.println(); psf public static final psfi public static final int psfs public static final String 调试快捷键 快捷键 作用 Alt+F8 debug时选中查看值 Alt+Shift+F9 选择Debug Alt+Shift+F10 选择Run Ctrl+Shift+F9 编译 Ctrl+Shift+F8 查看断点 F7 步入 Shift+F7 智能步入 Alt+Shift+F7 强制步入 F8 步过 Shift+F8 步出 Alt+Shift+F8 强制步过 F9 恢复程序 Alt+F9 运行至光标处 Ctrl+Alt+F9 强制运行至光标处 Alt+F10 定位到断点 查询快捷键 快捷键 作用 Ctrl＋Shift＋Backspace 可以跳转到上次编辑的地 Ctrl+Alt+ left/right 前后导航编辑过的地方 ALT+7 靠左窗口显示当前文件的结构 Ctrl+F12 浮动显示当前文件的结构 Alt+F7 找到你的函数或者变量或者类的所有引用到的地方 Ctrl+Alt+F7 找到你的函数或者变量或者类的所有引用到的地方 Ctrl+Shift+Alt+N 查找类中的方法或变量 Ctrl+N 查找类 Ctrl+Shift+N 查找文件 Ctrl+G 定位行 Ctrl+F 在当前窗口查找文本 Ctrl+Shift+F 在指定窗口查找文本 Ctrl+R 在当前窗口替换文本 Ctrl+Shift+R 在指定窗口替换文本 Alt+Shift+C 查找修改的文件 Ctrl+E 最近打开的文件 F3 向下查找关键字出现位置 Shift+F3 向上一个关键字出现位置 选中文本，按Alt+F3 高亮相同文本，F3逐个往下查找相同文本 F4 查找变量来源 Ctrl+Shift+O 弹出显示查找内容 Ctrl+W 选中代码，连续按会有其他效果 F2 或Shift+F2 高亮错误或警告快速定位 Ctrl+Up/Down 光标跳转到第一行或最后一行下 Ctrl+B 快速打开光标处的类或方法 Ctrl+Alt+B 找所有的子类 Ctrl+Shift+B 找变量的类 Ctrl+Shift+上下键 上下移动代码 Ctrl+Alt+left/right 返回至上次浏览的位置 Ctrl+X 删除行 Ctrl+D 复制行 Ctrl+/ 或 Ctrl+Shift+/ 注释(// 或者/…/) Ctrl+H 显示类结构图 Ctrl+Q 显示注释文档 Alt+F1 查找代码所在位置 Alt+1 快速打开或隐藏工程面板 Alt+left/right 切换代码视图 Alt+↑/↓ 在方法间快速移动定位 Ctrl+Alt+ left/right 前后导航编辑过的地方 Ctrl＋Shift＋Backspace 可以跳转到上次编辑的地 Alt+6 查找TODO 代码相关 快捷键 作用 Ctrl+Alt+O 优化导入的类和包 Alt+Insert 生成代码(如Getter,Setter方法,构造函数等)或者右键(Generate) Ctrl+Alt+T 生成try catch 或者 Alt+enter Ctrl+Alt+T 把选中的代码放在 TRY{} IF{} ELSE{} 里 Ctrl+O 重写方法 Ctrl+I 实现方法 Ctr+Shift+U 大小写转化 Alt+回车 导入包,自动修正 Alt+/ 代码提示 Ctrl+J 自动代码 Ctrl+Shift+J 整合两行为一行 Ctrl+空格 代码提示 Ctrl+Shift+Space 自动补全代码 Ctrl+Alt+L 格式化代码 Ctrl+Alt+I 自动缩进 Ctrl+E 最近更改的代码 Ctrl+Alt+Space 类名或接口名提示 Ctrl+P 方法参数提示 Ctrl+Q 可以看到当前方法的声明 Shift+F6 重构-重命名 (包、类、方法、变量、甚至注释等) Ctrl+Alt+V 提取变量","link":"/posts/252628913.html"},{"title":".Net Core 3.X WebApi 自宿主并注册成Windows服务","text":".net core跨平台之后，部署方式也变得多了。在Windows上可以IIS、Kestrel、Windows 服务,我之前做的一个项目,用的Kestrel前面再加了一层Nginx代理。因为之前.net的时候部署老是用IIS,感觉有点繁琐，所以这次就来探究一下Windows 服务的方式。 基于WindowsServices创建服务首先，我们需要安装一个组件Microsoft.Extensions.Hosting.WindowsServices,然后在Program.cs使用UseWindowsService()即可,完整代码如下: 12345678910111213141516171819public class Program { public static async Task Main(string[] args) { await CreateHostBuilder(args).Build().RunAsync(); } public static IHostBuilder CreateHostBuilder(string[] args) =&gt; Host.CreateDefaultBuilder(args) .UseServiceProviderFactory(new AutofacServiceProviderFactory()) .ConfigureWebHostDefaults(webBuilder =&gt; { webBuilder.UseStartup&lt;Startup&gt;(); }).ConfigureLogging(logging =&gt; { logging.ClearProviders(); //删除日志组件 logging.SetMinimumLevel(Microsoft.Extensions.Logging.LogLevel.Trace); }).UseNLog().UseWindowsService(); } 我们发布尝试一下。 1dotnet publish -r win7-x64 -c release -o ./bin/output 发布完成后，我们有两种方式创建服务: 基于Windows的SC 基于PowerShell的New-Service 基于Windows的SC创建Windows服务首先，我们熟悉一下sc的用法: 1sc [ServerName] create Servicename [Optionname= Optionvalues] [ServerName]可选，可以操作远程计算机。如果在本地计算机上操作就不用添加任何参数。 Servicename 注册的服务名称 Optionname &amp; Optionvalues 注册服务时指定的参数名&amp;参数值 123456type=[own|share|interact|kernel|filesys] #关于建立服务的类型，默认是share。start=[boot|system|auto|demand|disabled] #关于启动服务的类型，默认是demand（手动）。error=[normal|severe|critical|ignore] #当服务在导入失败错误的严重性，默认是normal。binPath=[binary path] #服务二进制文件的路径名。displayname=[服务显示名称]password=[用户密码] #如果一个不同于localsys tem的账号使用时需要使用这个。 sc 删除: 1sc [ServerName] delete [ServiceName] sc start/stop 1sc start/stop [ServiceName] SC创建服务我们现在就把我们刚才发布的二进制文件注册成Windows服务 12 C:\\Users\\lenovo\\source\\repos\\Swagger.Demo\\Web\\bin\\output&gt;sc create SwaggerDemo start=auto binpath=&quot;\\&quot;C:\\Users\\lenovo\\source\\repos\\Swagger.Demo\\Web\\bin\\output\\Web.exe\\&quot;[SC] CreateService 成功 现在我们就启动起来看看效果: 123456789101112 C:\\Users\\lenovo\\source\\repos\\Swagger.Demo\\Web\\bin\\output&gt;sc start SwaggerDemoSERVICE_NAME: SwaggerDemo TYPE : 10 WIN32_OWN_PROCESS STATE : 2 START_PENDING (NOT_STOPPABLE, NOT_PAUSABLE, IGNORES_SHUTDOWN) WIN32_EXIT_CODE : 0 (0x0) SERVICE_EXIT_CODE : 0 (0x0) CHECKPOINT : 0x0 WAIT_HINT : 0x7d0 PID : 14428 FLAGS : 基于PowerShell New-Service创建服务语法格式: 123456789101112New-Service [-Name] &lt;String&gt; [-BinaryPathName] &lt;String&gt; [-DisplayName &lt;String&gt;] [-Description &lt;String&gt;] [-SecurityDescriptorSddl &lt;String&gt;] [-StartupType &lt;ServiceStartupType&gt;] [-Credential &lt;PSCredential&gt;] [-DependsOn &lt;String[]&gt;] [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;] 我们使用New-Service重新注册服务。 12345PS C:\\Users\\lenovo&gt; New-Service -Name SwaggerDemoByPowerShell -BinaryPathName C:\\Users\\lenovo\\source\\repos\\Swagger.Demo\\Web\\bin\\output\\Web.exe -Description &quot;SwaggerDemoByPowerShell&quot; -DisplayName &quot;SwaggerDemoByPowerShell&quot; -StartupType AutomaticStatus Name DisplayName------ ---- -----------Stopped SwaggerDemoByPo... SwaggerDemoByPowerShell [参考]https://www.cnblogs.com/inuex/p/4299690.htmlhttps://docs.microsoft.com/zh-cn/aspnet/core/host-and-deploy/windows-service?view=aspnetcore-3.1&amp;tabs=visual-studio","link":"/posts/1795956873.html"},{"title":"NSQ的可用性和可靠性","text":"在前面，NSQ实现消息延迟执行中我简单的介绍了NSQ的搭建和使用，在此，我们继续探究一下NSQ的高可用性。毕竟开发的服务都希望7*24小时都能正常使用，虽然不能保证100%的可用性，但是我们也希望无限趋近于100%。 NSQ数据持久化NSQ是一个内存消息队列，默认情况下，消息是存储在内存中不会立刻持久化到磁盘，只有当消息数量达到MemQueueSize的默认阀值10000时会进行持久化。如果想确保所有传入消息都持久化到磁盘，可以设置–mem-queue-size为0，但是这样必定会影响NSQ的性能。 NSQ的可用性单节点部署都会存在可用性风险，万一节点挂掉，会导致业务不可用。所以，我们要避免单节点。接下来，我就进行单机部署多节点的演示。 1.启动N(大于等于2)个nsqlookupd节点首先我们需要保证我们的注册中心不是单节点。这里我在cmd启动两个nsqlookupd节点: 123456789E:\\tool\\nsq-1.2.0\\bin&gt;nsqlookupd -http-address=&quot;:4161&quot; -tcp-address=&quot;:4160&quot;[nsqlookupd] 2020/01/16 09:38:09.605212 INFO: nsqlookupd v1.2.0 (built w/go1.12.9)[nsqlookupd] 2020/01/16 09:38:09.660066 INFO: HTTP: listening on [::]:4161[nsqlookupd] 2020/01/16 09:38:09.660066 INFO: TCP: listening on [::]:4160E:\\tool\\nsq-1.2.0\\bin&gt;nsqlookupd -http-address=&quot;:4171&quot; -tcp-address=&quot;:4170&quot;[nsqlookupd] 2020/01/16 09:38:51.065321 INFO: nsqlookupd v1.2.0 (built w/go1.12.9)[nsqlookupd] 2020/01/16 09:38:51.102226 INFO: HTTP: listening on [::]:4171[nsqlookupd] 2020/01/16 09:38:51.102226 INFO: TCP: listening on [::]:4170 2.启动N(大于等于2)个nsqd节点这里我在cmd启动三个nsqd节点: 123456789101112131415161718192021222324252627282930313233E:\\tool\\nsq-1.2.0\\bin&gt;nsqd --lookupd-tcp-address=&quot;0.0.0.0:4160&quot; --lookupd-tcp-address=&quot;0.0.0.0:4170&quot; -data-path=&quot;D:/nsqdata1&quot; --mem-queue-size=0 -tcp-address=&quot;:4150&quot; -http-address=&quot;:4151&quot;[nsqd] 2020/01/16 09:55:25.104651 INFO: nsqd v1.2.0 (built w/go1.12.9)[nsqd] 2020/01/16 09:55:25.138560 INFO: ID: 632[nsqd] 2020/01/16 09:55:25.183440 INFO: TOPIC(nsq_producer): created[nsqd] 2020/01/16 09:55:25.183440 INFO: TOPIC(nsq_demo): created[nsqd] 2020/01/16 09:55:25.185434 INFO: TOPIC(nsq_demo): new channel(ch)[nsqd] 2020/01/16 09:55:25.185434 INFO: TOPIC(delay): created[nsqd] 2020/01/16 09:55:25.186431 INFO: TOPIC(delay): new channel(ch)[nsqd] 2020/01/16 09:55:25.186431 INFO: NSQ: persisting topic/channel metadata to D:/nsqdata1/nsqd.dat[nsqd] 2020/01/16 09:55:25.260526 INFO: TCP: listening on [::]:4150[nsqd] 2020/01/16 09:55:25.260526 INFO: HTTP: listening on [::]:4151[nsqd] 2020/01/16 09:55:25.260526 INFO: LOOKUP(0.0.0.0:4160): adding peer[nsqd] 2020/01/16 09:55:25.262521 INFO: LOOKUP connecting to 0.0.0.0:4160[nsqd] 2020/01/16 09:55:25.269502 INFO: LOOKUPD(0.0.0.0:4160): peer info {TCPPort:4160 HTTPPort:4161 Version:1.2.0 BroadcastAddress:2CNU7X5OLAUE004}[nsqd] 2020/01/16 09:55:25.269502 INFO: LOOKUPD(0.0.0.0:4160): REGISTER nsq_producer[nsqd] 2020/01/16 09:55:25.270500 INFO: LOOKUPD(0.0.0.0:4160): REGISTER nsq_demo ch[nsqd] 2020/01/16 09:55:25.273546 INFO: LOOKUPD(0.0.0.0:4160): REGISTER delay ch[nsqd] 2020/01/16 09:55:25.278485 INFO: LOOKUP(0.0.0.0:4170): adding peer[nsqd] 2020/01/16 09:55:25.278485 INFO: LOOKUP connecting to 0.0.0.0:4170[nsqd] 2020/01/16 09:55:25.284462 INFO: LOOKUPD(0.0.0.0:4170): peer info {TCPPort:4170 HTTPPort:4171 Version:1.2.0 BroadcastAddress:2CNU7X5OLAUE004}[nsqd] 2020/01/16 09:55:25.284462 INFO: LOOKUPD(0.0.0.0:4170): REGISTER nsq_demo ch[nsqd] 2020/01/16 09:55:25.287454 INFO: LOOKUPD(0.0.0.0:4170): REGISTER delay ch[nsqd] 2020/01/16 09:55:25.291443 INFO: LOOKUPD(0.0.0.0:4170): REGISTER nsq_producer[nsqd] 2020/01/16 09:55:25.292444 INFO: LOOKUPD(0.0.0.0:4160): topic REGISTER nsq_producer[nsqd] 2020/01/16 09:55:25.294437 INFO: LOOKUPD(0.0.0.0:4170): topic REGISTER nsq_producer[nsqd] 2020/01/16 09:55:25.296430 INFO: LOOKUPD(0.0.0.0:4160): topic REGISTER nsq_demo[nsqd] 2020/01/16 09:55:25.296430 INFO: LOOKUPD(0.0.0.0:4170): topic REGISTER nsq_demo[nsqd] 2020/01/16 09:55:25.297427 INFO: LOOKUPD(0.0.0.0:4160): channel REGISTER nsq_demo ch[nsqd] 2020/01/16 09:55:25.297427 INFO: LOOKUPD(0.0.0.0:4170): channel REGISTER nsq_demo ch[nsqd] 2020/01/16 09:55:25.297427 INFO: LOOKUPD(0.0.0.0:4160): topic REGISTER delay[nsqd] 2020/01/16 09:55:25.298425 INFO: LOOKUPD(0.0.0.0:4170): topic REGISTER delay[nsqd] 2020/01/16 09:55:25.298425 INFO: LOOKUPD(0.0.0.0:4160): channel REGISTER delay ch[nsqd] 2020/01/16 09:55:25.299423 INFO: LOOKUPD(0.0.0.0:4170): channel REGISTER delay ch 其他NSQD节点: 12nsqd --lookupd-tcp-address=&quot;0.0.0.0:4160&quot; --lookupd-tcp-address=&quot;0.0.0.0:4170&quot; -data-path=&quot;D:/nsqdata1&quot; --mem-queue-size=0 -tcp-address=&quot;:4140&quot; -http-address=&quot;:4141&quot;nsqd --lookupd-tcp-address=&quot;0.0.0.0:4160&quot; --lookupd-tcp-address=&quot;0.0.0.0:4170&quot; -data-path=&quot;D:/nsqdata1&quot; --mem-queue-size=0 -tcp-address=&quot;:4130&quot; -http-address=&quot;:4131&quot; 3.启动nsqadmin节点nsqadmin只是监控，这里我暂时只部署一个节点： 123E:\\tool\\nsq-1.2.0\\bin&gt;nsqadmin --lookupd-http-address=0.0.0.0:4161 --lookupd-http-address=0.0.0.0:4171 -http-address=&quot;:4121&quot;[nsqadmin] 2020/01/16 10:00:57.383466 INFO: nsqadmin v1.2.0 (built w/go1.12.9)[nsqadmin] 2020/01/16 10:00:57.420368 INFO: HTTP: listening on [::]:4121 最终效果: 代码示例在之前，我们是直接连接的NSQD这样做的话很多工作就需要客户端来处理。所以官方推荐我们连接NSQD的注册中心nsqlookupd。所以我们Comsumer的代码需要调整一下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( &quot;errors&quot; &quot;fmt&quot; &quot;github.com/nsqio/go-nsq&quot; &quot;sync&quot; &quot;time&quot;)type ComsumerHandler struct {}func (h *ComsumerHandler) HandleMessage(message *nsq.Message) error { if string(message.Body) == &quot;TOBEFAILED&quot; { return errors.New(&quot;fail this message&quot;) } fmt.Println(&quot;time&quot;,time.Now().Format(&quot;2020-01-14 15:04:05&quot;),&quot;receive&quot;, message.NSQDAddress, &quot;message:&quot;, string(message.Body)) return nil}func main() { waiter := sync.WaitGroup{} waiter.Add(1) go func() { defer waiter.Done() config := nsq.NewConfig() topicName := &quot;nsq_demo&quot; adds := []string{&quot;127.0.0.1:4161&quot;, &quot;127.0.0.1:4171&quot;} config.MaxInFlight = 1000 config.MaxBackoffDuration = 5 * time.Second config.DialTimeout = 10 * time.Second consumer, _ := nsq.NewConsumer(topicName, &quot;ch&quot;, config) handler := &amp;ComsumerHandler{ } consumer.AddHandler(handler) err := consumer.ConnectToNSQLookupds(adds) if nil != err { fmt.Println(&quot;err&quot;, err) return } select{} }() waiter.Wait()} 启动后会看到连接日志: 12342020/01/16 10:39:34 INF 1 [nsq_demo/ch] querying nsqlookupd http://127.0.0.1:4161/lookup?topic=nsq_demo2020/01/16 10:39:34 INF 1 [nsq_demo/ch] (2CNU7X5OLAUE004:4130) connecting to nsqd2020/01/16 10:39:34 INF 1 [nsq_demo/ch] (2CNU7X5OLAUE004:4150) connecting to nsqd2020/01/16 10:39:35 INF 1 [nsq_demo/ch] (2CNU7X5OLAUE004:4140) connecting to nsqd 小结由于NSQ的producer是直接连接NSQD,如果直接连接单节点NSQD，那么对于producer来说会存在单点问题，据我所知NSQ不支持副本机制，所以对于topic来说也存在单点问题，一般解决单点问题的方式就是冗余，那么要解决这些问题，简单点的方式可能就是自己在producer来维护一个NSQD的服务器列表信息，自己检测其健康状态，为了避免topic的单点问题的话，我们可以往多个NSQD发送消息。由于NSQ支持Comsumer连接nsqlookupd来避免单点问题，如果在使用中存在消息发往多个NSQD,那么Comsumer要确保幂等消费。 推荐阅读我们也知道了NSQ的一些弊端，有些公司也有了解决方案，我们可以去借鉴他们的处理方式。How we redesign the NSQ-NSQ重塑之客户端How we redesigned the NSQ - NSQ重塑之详细设计https://nsq.io/deployment/topology_patterns.html 文中如有理解错误之处还请指正。","link":"/posts/2559044759.html"},{"title":".Net Core3.x使用NLog记录日志","text":"在.net core中，常用的日志组件大概就是Logging(自带)、Log4net和NLog等，其他的我目前还没有用到。我觉得NLog简单易用，性能也不错,支持多种日志写入方式。 安装NLog首先，我们通过Nuget安装NLog和NLog.Web.AspNetCore两个组件，我安装的版本是： 创建NLog配置文件在web目录新建一个nlog.config文件，内容 如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;&lt;nlog xmlns=&quot;http://www.nlog-project.org/schemas/NLog.xsd&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; autoReload=&quot;true&quot; internalLogLevel=&quot;Info&quot; internalLogFile=&quot;logs\\internal-nlog.txt&quot;&gt; &lt;!-- enable asp.net core layout renderers --&gt; &lt;extensions&gt; &lt;add assembly=&quot;NLog.Web.AspNetCore&quot;/&gt; &lt;/extensions&gt; &lt;!-- the targets to write to --&gt; &lt;targets&gt; &lt;!-- write logs to file --&gt; &lt;target xsi:type=&quot;File&quot; name=&quot;allfile&quot; fileName=&quot;logs\\nlog-all-${shortdate}.log&quot; layout=&quot;${longdate}|${event-properties:item=EventId_Id}|${threadid}|${activityid}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}&quot; maxArchiveFiles=&quot;4&quot; archiveAboveSize=&quot;10240&quot; archiveEvery=&quot;Day&quot;/&gt; &lt;!-- another file log, only own logs. Uses some ASP.NET core renderers --&gt; &lt;target xsi:type=&quot;File&quot; name=&quot;ownFile-web&quot; fileName=&quot;logs\\nlog-own-${shortdate}.log&quot; layout=&quot;${longdate}|${event-properties:item=EventId_Id}|threadid-${threadid}|activityid-${activityid}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}|url: ${aspnet-request-url}|action: ${aspnet-mvc-action}&quot; maxArchiveFiles=&quot;4&quot; archiveAboveSize=&quot;10240&quot; archiveEvery=&quot;Day&quot;/&gt; &lt;target name=&quot;file&quot; xsi:type=&quot;AsyncWrapper&quot; queueLimit=&quot;5000&quot; overflowAction=&quot;Discard&quot;&gt; &lt;target xsi:type=&quot;File&quot; fileName=&quot;${basedir}/logs/${level}.txt&quot; keepFileOpen=&quot;true&quot; layout=&quot;${longdate}|${event-properties:item=EventId_Id}|threadid-${threadid}|activityid-${activityid}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}|url: ${aspnet-request-url}|action: ${aspnet-mvc-action}&quot; maxArchiveFiles=&quot;4&quot; archiveAboveSize=&quot;10240&quot; archiveEvery=&quot;Day&quot;/&gt; &lt;/target&gt; &lt;/targets&gt; &lt;!-- rules to map from logger name to target --&gt; &lt;rules&gt; &lt;!--All logs, including from Microsoft--&gt; &lt;logger name=&quot;*&quot; minlevel=&quot;Trace&quot; writeTo=&quot;allfile&quot; /&gt; &lt;!--Skip non-critical Microsoft logs and so log only own logs--&gt; &lt;logger name=&quot;Microsoft.*&quot; maxlevel=&quot;Info&quot; final=&quot;true&quot; /&gt; &lt;!-- BlackHole without writeTo --&gt; &lt;logger name=&quot;*&quot; minlevel=&quot;Trace&quot; writeTo=&quot;ownFile-web&quot; /&gt; &lt;logger name=&quot;*&quot; minlevel=&quot;Debug&quot; writeTo=&quot;file&quot; /&gt; &lt;/rules&gt;&lt;/nlog&gt; 使用NLog我们首先在Program使用注册NLog: 1234567891011public static IHostBuilder CreateHostBuilder(string[] args) =&gt; Host.CreateDefaultBuilder(args) .UseServiceProviderFactory(new AutofacServiceProviderFactory()) .ConfigureWebHostDefaults(webBuilder =&gt; { webBuilder.UseStartup&lt;Startup&gt;(); }).ConfigureLogging(logging =&gt; { logging.ClearProviders(); //删除日志组件 logging.SetMinimumLevel(Microsoft.Extensions.Logging.LogLevel.Trace); }).UseNLog(); 然后，我们就可以再相应的地方调用: 123456789101112131415[ApiVersion(&quot;1.0&quot;)] [Route(&quot;api/v{version:apiVersion}/[controller]&quot;)] [ApiController] public class ExampleController : ControllerBase { private readonly ILogger&lt;ExampleController&gt; _logger; IExampleService _exampleService; public ExampleController(IExampleService exampleService, ILogger&lt;ExampleController&gt; logger) { _logger = logger; _exampleService = exampleService; _logger.LogDebug(&quot;NLog injected&quot;); } } 效果如下: 12019-12-25 17:09:48.5355||DEBUG|Web.Controllers.v1.ExampleController|NLog injected NLog类似于string.Format的格式,如： 1_logger.LogInformation(&quot;Hello, {0}&quot;, Name); 最终结果: 12019-12-25 17:35:57.4098||threadid-10||INFO|Web.Controllers.v1.ExampleController|Hello, Jane |url: http://localhost/api/v1.0/Example/GetName/Jane|action: GetName 也支持格式化和对象: 12_logger.LogInformation(&quot;Hello,{name}&quot;,Name);_logger.LogInformation(&quot;Hello,{Name}&quot;, new { Name = Name }); 效果： 1232019-12-25 17:35:57.4288||threadid-10||INFO|Web.Controllers.v1.ExampleController|Hello,Jane |url: http://localhost/api/v1.0/Example/GetName/Jane|action: GetName2019-12-25 17:35:57.4288||threadid-10||INFO|Web.Controllers.v1.ExampleController|Hello,{ Name = Jane } |url: http://localhost/api/v1.0/Example/GetName/Jane|action: GetName 更多配置看NLog的文档。 上下文信息如果是单体应用的话，用上面的配置即可, 但是，我们现在经常会用到微服务或者是异步。微服务一般使用MDC的方式，这里我们暂时只说说异步，大家都知道异步是会在创新新线程执行，这时候的线程Id就不和主线程在同一个上下文中了。我们来试试看: 123456789101112131415161718/// &lt;summary&gt; /// 异步调用 /// &lt;/summary&gt; /// &lt;param name=&quot;Name&quot;&gt;&lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; [HttpGet] [Route(&quot;GetNameAsync/{Name}&quot;)] public async Task&lt;string&gt; GetNameAsync(string Name) { _logger.LogInformation(&quot;Hello,{0}&quot;, Name); string result = &quot;&quot;; await Task.Run(() =&gt; { _logger.LogInformation(&quot;Hello,{0}&quot;, Name); result = $&quot;Hello,{Name}&quot;; }); return result; } 打印两次, 最终日志为: 122019-12-25 17:55:45.5634||threadid-5|activityid-|INFO|Web.Controllers.v1.ExampleController|Hello,Jane |url: http://localhost/api/v1.0/Example/GetNameAsync/Jane|action: GetName2019-12-25 17:55:45.5634||threadid-4|activityid-|INFO|Web.Controllers.v1.ExampleController|Hello,Jane |url: http://localhost/api/v1.0/Example/GetNameAsync/Jane|action: GetName 可以看到我们的threadid不一样。这样在我们捕捉日志的时候会变得很困难，这时候，NLog的ndlc就可以帮我们解决这个问题,我们修改一下nlog.config的layout配置: 1${longdate}|${event-properties:item=EventId_Id}|NDLC-${ndlc}|threadid-${threadid}|activityid-${activityid}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}|url: ${aspnet-request-url}|action: ${aspnet-mvc-action} 效果如下: 122019-12-26 08:58:30.6271||NDLC-ConnectionId:0HLS9JGU66BD8 RequestPath:/api/v1.0/Example/GetNameAsync/Jane RequestId:0HLS9JGU66BD8:00000001, SpanId:|59ece683-4c05d632d98d1cac., TraceId:59ece683-4c05d632d98d1cac, ParentId: Web.Controllers.v1.ExampleController.GetNameAsync (Web)|threadid-5|activityid-|INFO|Web.Controllers.v1.ExampleController|Hello,Jane |url: http://localhost/api/v1.0/Example/GetNameAsync/Jane|action: GetName2019-12-26 08:58:30.6598||NDLC-ConnectionId:0HLS9JGU66BD8 RequestPath:/api/v1.0/Example/GetNameAsync/Jane RequestId:0HLS9JGU66BD8:00000001, SpanId:|59ece683-4c05d632d98d1cac., TraceId:59ece683-4c05d632d98d1cac, ParentId: Web.Controllers.v1.ExampleController.GetNameAsync (Web)|threadid-10|activityid-|INFO|Web.Controllers.v1.ExampleController|Hello,Jane |url: http://localhost/api/v1.0/Example/GetNameAsync/Jane|action: GetName 但是有些信息可能我们不需要,我先来看看NDLC的代码: 123456789101112131415161718192021222324252627282930313233343536protected override void Append(StringBuilder builder, LogEventInfo logEvent) { if (TopFrames == 1) { // Allows fast rendering of topframes=1 var topFrame = NestedDiagnosticsLogicalContext.PeekObject(); if (topFrame != null) AppendAsString(topFrame, GetFormatProvider(logEvent), builder); return; } var messages = NestedDiagnosticsLogicalContext.GetAllObjects(); if (messages.Length == 0) return; int startPos = 0; int endPos = messages.Length; if (TopFrames != -1) { endPos = Math.Min(TopFrames, messages.Length); } else if (BottomFrames != -1) { startPos = messages.Length - Math.Min(BottomFrames, messages.Length); } var formatProvider = GetFormatProvider(logEvent); string currentSeparator = string.Empty; for (int i = endPos - 1; i &gt;= startPos; --i) { builder.Append(currentSeparator); AppendAsString(messages[i], formatProvider, builder); currentSeparator = Separator; } } 在VS2019启用Source Link,就可以进入到我们的NLog组件,可以看到:可以看到message是一个数组,那么我们只想要ConnectionId的话，可以做如下修改: 1${longdate}|${event-properties:item=EventId_Id}|NDLC-${ndlc:bottomFrames=1}|threadid-${threadid}|activityid-${activityid}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}|url: ${aspnet-request-url}|action: ${aspnet-mvc-action} 效果如下: 122019-12-26 10:56:53.0233||NDLC-ConnectionId:0HLS9LSTP4B7D|threadid-4|activityid-|INFO|Web.Controllers.v1.ExampleController|Hello,Jane |url: http://localhost/api/v1.0/Example/GetNameAsync/Jane|action: GetName2019-12-26 10:56:53.4459||NDLC-ConnectionId:0HLS9LSTP4B7D|threadid-15|activityid-|INFO|Web.Controllers.v1.ExampleController|Hello,Jane |url: http://localhost/api/v1.0/Example/GetNameAsync/Jane|action: GetName 这样一来，我们就可以看到我们是否是同一个请求处理，其他更加强大的功能，有待探究。在这里我们有用到Source Link,确实还是比较方便，但是有一个更方便的工具,就是ReSharper,但是安装ReSharper后，咱们的VS会有一点小卡。先到这里吧，后面有机会再单独记录一下Source Link以及ReSharper神器的强大功能。","link":"/posts/65255459.html"},{"title":"Linux中inode引起的故障","text":"inode是什么？ ETL脚本调度故障今日一大早，用户群就炸了，很多人说脚本调度无法运行，一直是未开始状态，起初，我并未引起重视，猜想可能是pieline服务阻塞引起的，由于情况紧急，就直接进入到tomcat manager进行reload操作，但是奇怪的是，reload之后，大批量的作业都直接终止掉，查看日志，发现是ETL脚本日志创建失败， 遂想到，肯定是文件系统占满，就开始rm一部分，然后，df -h，发现还有160+G，以为问题就解决掉了，但是没过多久，依然有很多未开始的作业。 df -i 解围因为对linux使用不是很熟，也只会ps -ef、cd、kill等常用命令，在百度中得知一个命令df -i，执行命令后，发现使用率100%，原来，真正的原因是：etl工具每天产生的小文件过多，把inode占满，导致工具无法创建日志文件。既然找到原因，当然就只有找到目录，将历史文件给rm掉 df的用法df命令一般用于查看磁盘空间大小，du -sh 查看当前目录使用大小。df -h 大文件占用大量的磁盘容量。df -i 过多的文件占用了大量的inode号。 [参考]linux命令df中df -h和df -i的区别","link":"/posts/3528686084.html"},{"title":"Nginx Proxy_Pass和Rewirte的用法","text":"在Nginx中我们用的比较多的肯定就是server和loction模块，在模块中用的比较多的就是proxy_pass和rewrite，我们这里就来大概了解一下这二者的常用用法。 Nginx内置的常用变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960$args #请求中的参数值$query_string #同 $args$arg_NAME #GET请求中NAME的值$is_args #如果请求中有参数，值为&quot;?&quot;，否则为空字符串$uri #请求中的当前URI(不带请求参数，参数位于$args)，可以不同于浏览器传递的$request_uri的值，它可以通过内部重定向，或者使用index指令进行修改，$uri不包含主机名，如&quot;/foo/bar.html&quot;。$document_uri #同 $uri$document_root #当前请求的文档根目录或别名$host #优先级：HTTP请求行的主机名&gt;&quot;HOST&quot;请求头字段&gt;符合请求的服务器名.请求中的主机头字段，如果请求中的主机头不可用，则为服务器处理请求的服务器名称$hostname #主机名$https #如果开启了SSL安全模式，值为&quot;on&quot;，否则为空字符串。$binary_remote_addr #客户端地址的二进制形式，固定长度为4个字节$body_bytes_sent #传输给客户端的字节数，响应头不计算在内；这个变量和Apache的mod_log_config模块中的&quot;%B&quot;参数保持兼容$bytes_sent #传输给客户端的字节数$connection #TCP连接的序列号$connection_requests #TCP连接当前的请求数量$content_length #&quot;Content-Length&quot; 请求头字段$content_type #&quot;Content-Type&quot; 请求头字段$cookie_name #cookie名称$limit_rate #用于设置响应的速度限制$msec #当前的Unix时间戳$nginx_version #nginx版本$pid #工作进程的PID$pipe #如果请求来自管道通信，值为&quot;p&quot;，否则为&quot;.&quot;$proxy_protocol_addr #获取代理访问服务器的客户端地址，如果是直接访问，该值为空字符串$realpath_root #当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径$remote_addr #客户端地址$remote_port #客户端端口$remote_user #用于HTTP基础认证服务的用户名$request #代表客户端的请求地址$request_body #客户端的请求主体：此变量可在location中使用，将请求主体通过proxy_pass，fastcgi_pass，uwsgi_pass和scgi_pass传递给下一级的代理服务器$request_body_file #将客户端请求主体保存在临时文件中。文件处理结束后，此文件需删除。如果需要之一开启此功能，需要设置client_body_in_file_only。如果将次文件传 递给后端的代理服务器，需要禁用request body，即设置proxy_pass_request_body off，fastcgi_pass_request_body off，uwsgi_pass_request_body off，or scgi_pass_request_body off$request_completion #如果请求成功，值为&quot;OK&quot;，如果请求未完成或者请求不是一个范围请求的最后一部分，则为空$request_filename #当前连接请求的文件路径，由root或alias指令与URI请求生成$request_length #请求的长度 (包括请求的地址，http请求头和请求主体)$request_method #HTTP请求方法，通常为&quot;GET&quot;或&quot;POST&quot;$request_time #处理客户端请求使用的时间,单位为秒，精度毫秒； 从读入客户端的第一个字节开始，直到把最后一个字符发送给客户端后进行日志写入为止。$request_uri #这个变量等于包含一些客户端请求参数的原始URI，它无法修改，请查看$uri更改或重写URI，不包含主机名，例如：&quot;/cnphp/test.php?arg=freemouse&quot;$scheme #请求使用的Web协议，&quot;http&quot; 或 &quot;https&quot;$server_addr #服务器端地址，需要注意的是：为了避免访问linux系统内核，应将ip地址提前设置在配置文件中$server_name #服务器名$server_port #服务器端口$server_protocol #服务器的HTTP版本，通常为 &quot;HTTP/1.0&quot; 或 &quot;HTTP/1.1&quot;$status #HTTP响应代码$time_iso8601 #服务器时间的ISO 8610格式$time_local #服务器时间（LOG Format 格式）$cookie_NAME #客户端请求Header头中的cookie变量，前缀&quot;$cookie_&quot;加上cookie名称的变量，该变量的值即为cookie名称的值$http_NAME #匹配任意请求头字段；变量名中的后半部分NAME可以替换成任意请求头字段，如在配置文件中需要获取http请求头：&quot;Accept-Language&quot;，$http_accept_language即可$http_cookie$http_host #请求地址，即浏览器中你输入的地址（IP或域名）$http_referer #url跳转来源,用来记录从那个页面链接访问过来的$http_user_agent #用户终端浏览器等信息$http_x_forwarded_for$sent_http_NAME #可以设置任意http响应头字段；变量名中的后半部分NAME可以替换成任意响应头字段，如需要设置响应头Content-length，$sent_http_content_length即可$sent_http_cache_control$sent_http_connection$sent_http_content_type$sent_http_keep_alive$sent_http_last_modified$sent_http_location$sent_http_transfer_encoding proxy_pass的用法proxy_pass反向代理,主要用于请求的转发,先来看看常用的转发方式先配置一个简单的html服务器: 12345678910111213server{ listen 800; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; } } 123location /proxy1/{ proxy_pass http://localhost:800/; } 访问:http://localhost/proxy1/index.html，代理到 http://localhost:800/index.html 123location /proxy2/{ proxy_pass http://localhost:800; } 访问:http://localhost/proxy2/index.html，代理到 http://localhost:800/proxy2/index.html 123location /proxy3/{ proxy_pass http://localhost:800/web/; } 访问:http://localhost/proxy3/index.html，代理到 http://localhost:800/web/index.html 123location /proxy4/{ proxy_pass http://localhost:800/web; } 访问:http://localhost/proxy4/index.html，代理到 http://localhost:800/webindex.html 上面演示的只是简单的请求代理，一般我们还会用到较为高级一点的负载均衡，Nginx均衡策略有6种: 轮询：每个请求按顺序逐一分配到不同的后端服务器，如果后端服务器down掉，就不在分配； 1234upstream webapi { server 127.0.0.1:801 max_fails=5 fail_timeout=20; server 127.0.0.1:802 max_fails=5 fail_timeout=20; } 权重轮询：根据后端服务器性能不通配置轮询的权重比，权重越高访问的比重越高； 1234upstream webapi { server 127.0.0.1:801 weight=2 max_fails=5 fail_timeout=20; server 127.0.0.1:802 weight=6 max_fails=5 fail_timeout=20; } ip_hash：根据请求的ip地址hash结果进行分配，同一个IP的请求会落在同一个后端，可以解决Session同步的问题； 12345upstream webapi { ip_hash; server 127.0.0.1:801 max_fails=5 fail_timeout=20; server 127.0.0.1:802 max_fails=5 fail_timeout=20; } fair：按后端服务器的响应时间来分配请求，响应时间短的优先分配； 12345upstream webapi { fair; server 127.0.0.1:801 max_fails=5 fail_timeout=20; server 127.0.0.1:802 max_fails=5 fail_timeout=20; } url_hash：按访问url的hash结果来分配请求，使每个url落在同一个后端服务器，后端服务器为缓存时才用此种方式较妥。 12345upstream webapi { hash $request_uri； server 127.0.0.1:801 max_fails=5 fail_timeout=20; server 127.0.0.1:802 max_fails=5 fail_timeout=20; } least_conn:把请求转发给连接数较少的后端服务器 12345upstream webapi { least_conn; server 127.0.0.1:801 max_fails=5 fail_timeout=20; server 127.0.0.1:802 max_fails=5 fail_timeout=20; } 说明:在单位周期为fail_timeout设置的时间，中达到max_fails次数，这个周期次数内，如果后端同一个节点不可用，那么接将把节点标记为不可用，并等待下一个周期（同样时常为fail_timeout）再一次去请求，判断是否连接是否成功。默认：fail_timeout为10s,max_fails为1次。在做请求转发的时候，有时候可能会遇到端口丢失的情况,这时候就需要在location模块做如下配置: 1234proxy_set_header X-Forwarded-Host $host;proxy_set_header X-Forwarded-Server $host;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_set_header Host $host:$server_port; #这里是重点,这样配置才不会丢失端口 Rewrite的用法Rewrite顾名思义，就是规则重写,主要功能是实现浏览器访问 Http URL的跳转，语法格式:rewrite &lt;正则表达式&gt; &lt;代替的内容&gt; &lt;重写类型&gt;;,重写类型有4种: last：本条规则匹配完成后，继续向下匹配新的location URI规则；浏览器地址栏URL地址不变；一般写在server和if中; break：本条规则匹配完成后，终止匹配，不再匹配后面的规则，浏览器地址栏URL地址不变；一般使用在location中； redirect：返回302临时重定向，浏览器地址会显示跳转后的URL地址； permanent：返回301永久重定向，浏览器地址栏会显示跳转后的URL地址；示例说明：1rewrite /rewrite.html /index.html last; 访问/rewrite.html的时候,页面内容重写到/index.html中，并继续匹配，浏览器URL地址不变 1rewrite /break.html /index.html break; 访问/break.html的时候,页面内容重写到/index.html中，并停止后续匹配，浏览器URL地址不变 1rewrite /redirect.html /index.html redirect; 访问/redirect.html的时候，页面直接302重定向到/index.html中，浏览器URL跳为index.html 1rewrite /permanent.html /index.html permanent; 访问/permanent.html的时候，页面直接301重定向到/index.html中，浏览器URL跳为index.html 1rewrite ^/html/(.+?).html$ /permanent/$1.html permanent; 把/html/*.html301重定向到/permanent/*.html 1rewrite ^/(.*) https://www.baidu.com/$1 permanent; 把当前域名的请求，重定向到新域名上，URL域名变化，域名后的路径不变，访问:http://localhost/s?wd=nginx%20rewrite%E8%AF%A6%E8%A7%A3&amp;rsv_spt=1&amp;rsv_iqid=0xd27651610002bd69&amp;issp=1&amp;f=3&amp;rsv_bp=1&amp;rsv_idx=2&amp;ie=utf-8&amp;rqlang=cn&amp;tn=baiduhome_pg&amp;rsv_enter=0&amp;rsv_dl=ts_0&amp;oq=nginx%25E4%25BB%25A3%25E7%2590%2586%25E7%25AB%25AF%25E5%258F%25A3%25E4%25B8%25A2%25E5%25A4%25B1&amp;inputT=2230&amp;rsv_t=3e937ZY1a4uEO2TEPB%2BcJfOX%2FgNXA%2B26nrz3bOXnJHZUv6OAhomZhXh74aAaaNCm3897&amp;rsv_pq=b9fe0f2a0000ae48&amp;rsv_sug3=127&amp;rsv_sug1=88&amp;rsv_sug7=100&amp;rsv_sug2=1&amp;prefixsug=nginx%2520Rewrite&amp;rsp=0&amp;rsv_sug4=2409, 地址会变为:https://www.baidu.com/s?wd=nginx%20rewrite%E8%AF%A6%E8%A7%A3&amp;rsv_spt=1&amp;rsv_iqid=0xd27651610002bd69&amp;issp=1&amp;f=3&amp;rsv_bp=1&amp;rsv_idx=2&amp;ie=utf-8&amp;rqlang=cn&amp;tn=baiduhome_pg&amp;rsv_enter=0&amp;rsv_dl=ts_0&amp;oq=nginx%25E4%25BB%25A3%25E7%2590%2586%25E7%25AB%25AF%25E5%258F%25A3%25E4%25B8%25A2%25E5%25A4%25B1&amp;inputT=2230&amp;rsv_t=3e937ZY1a4uEO2TEPB%2BcJfOX%2FgNXA%2B26nrz3bOXnJHZUv6OAhomZhXh74aAaaNCm3897&amp;rsv_pq=b9fe0f2a0000ae48&amp;rsv_sug3=127&amp;rsv_sug1=88&amp;rsv_sug7=100&amp;rsv_sug2=1&amp;prefixsug=nginx%2520Rewrite&amp;rsp=0&amp;rsv_sug4=2409","link":"/posts/2176214718.html"},{"title":"Nginx HTTP/TCP代理配置","text":"众所周知，nginx是一款高性能的反向代理工具，在之前nginx只能代理应用层的应用，如果是要做TCP即网络第4层的代理就只有借助HaProxy等工具，在nginx1.9版本之后即可支持TCP的代理。下面，我们来展现一下nginx强大的代理能力。 我们的网络有内网和办公网还有互联网，首先所有项目部署在内网，使用只能在生产网(需要开通特殊策略)，特殊情况下，可以申请互联网的网络，但是比较麻烦，而且，我们进入生产服务器必须在内网的环境下通过堡垒机才能登陆，现在多个生产部署多个系统，但是映射到办公网只有个IP且只有一个端口。此时，我们就需要用到反向代理实现系统的访问。 Nginx基本配置说明 nginx有强大的url匹配功能，下面，我们一起来了解一下nginx的优先级匹配。以下是匹配语法模版 1234location [=|~|~*|^~] /uri/ { root html; index index.html index.htm;} 说明： 1234567= 表示精确匹配^~ 表示uri以某个常规字符串开头~ 表示区分大小写的正则匹配~* 表示不区分大小写的正则匹配 !~ 表示区分大小写不匹配的正则!~* 表示不区分大小写不匹配的正则/ 通用匹配，相当于根目录可以匹配到任何请求 优先级: 精确匹配&gt;普通匹配&gt;正则匹配 nginx对http进行反向代理： 123location / { proxy_pass http://127.0.0.1; } Nginx Http反向代理实践配置下面是我们为解决上面的问题所做的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445 //HTTP代理 location / { proxy_pass http://xx.xx.xxx.xxx:8081/; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_connect_timeout 1690; proxy_read_timeout 1690; proxy_send_timeout 1690; } location ^~ /Insight { proxy_pass http://xx.xxx.x.xxx:8083/BingoInsight; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_connect_timeout 1690; proxy_read_timeout 1690; proxy_send_timeout 1690; } location ^~ /etl{ proxy_pass http://xx.xxx.x.xxx:8080/etl; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #proxy_set_header Host $host:$server_port; #这里是重点,这样配置才不会丢失端口} //websocket 代理 location /gateone/{ proxy_pass http://xx.xx.xx.xx:58080/gateone/; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_connect_timeout 1690; proxy_read_timeout 1690; proxy_send_timeout 1690; client_max_body_size 300m; proxy_http_version 1.1; proxy_redirect off; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;;} ...... Nginx 代理转发TCP配置 我又有另外一个场景，也是内网和办公网的交互，但是这个场景不需要那么严格，可以直连服务器，但是我们是想在办公网直连数据库。这显然形成了一个屏障 这种情况下，我们想要连接到数据库就只有做一个TCP四层代理。我们就可以借助nginx upstream的功能实现TCP转发。配置如下 123456789upstream proxy_redis{ server xx.xxx.xx.xxx:6379;} server { listen 6379; #监听端口 proxy_pass proxy_redis; #转发请求} 然后刷新一下nginx配置: 1nginx -s reload 此时连接代理服务器响应的端口，就可以将TCP请求转发到实际服务器。 官方文档nginx下载地址","link":"/posts/1998726808.html"},{"title":"Nginx二级域名转发到不同端口/服务器","text":"有时候，我们资源有限，那么就会遇到一台主机对应1-N个域名，那么就会出现主机80端口占用完了，其他应用就只能占用其他的端口了，但是如果在域名后面带上端口号(如www.xxxx.com:8080 ),这显然很不优雅,而且像开发微信的时候，配置的域名只能使用80端口。那么有没有更好的处理方式能让我们不在域名后添加端口号呢？答案是肯定是，这里我们可以借助强大的Nginx来帮我们解决这个问题。 Nginx通过二级目录方式我们知道Nginx location强大的匹配模式，那么我们就可以通过location来匹配请求目录从而在内部转发到不同的端口/服务器。当然也可以使用Nginx+Lua(openresty、tengine)来判断请求参数也可以处理.但是现在我暂时还是才用的Nginx自身的一些API来实现. 1234567891011121314151617181920server { listen 443 ssl; server_name localhost; location / { root html; index index.html index.htm; } location /api/ { proxy_pass http://xxx.xxx.xxx.xx:8000/api/; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Cookie $http_cookie; #proxy_cookie_path chunked_transfer_encoding off; } 这时候，我们通过www.xxx.com/api 就可以转发到 http://xxx.xxx.xxx.xx:8000/api ,这里还有可以设置更高端的，比如才用Nginx Rewrite等。 Nginx匹配二级域名进行代理多个server模块通过server_name匹配代理server_name 是虚拟服务器的识别路径，不同的域名请求会带上相应的HOST请求头来匹配Nginx的server模块 12345678910111213141516171819202122232425262728293031323334353637383940414243server { listen 80; server_name www.cddj.top; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; } } server { listen 80; server_name www.51offer.wang; #charset koi8-r; #access_log logs/host.access.log main; location / { root html1; index index.html index.htm; } } server { listen 80; server_name sub.51offer.wang; #charset koi8-r; #access_log logs/host.access.log main; location / { root html2; index index.html index.htm; } } 我们可以看到访问不同的域名就转到了不同的server模块,这里只是简单是静态显示，我们可以再server模块中做转发,例如: 1234567891011121314151617181920server { listen 80; server_name sub.51offer.wang; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://localhost:8000/; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Cookie $http_cookie; #proxy_cookie_path chunked_transfer_encoding off; } } 多个server同端口的匹配规则是:完全匹配-&gt;通配符在前(如.biz)-&gt;通配符在后(如52fx.)-&gt;正则匹配(~^.52fx.biz$),如果都没匹配到就找默认server，如果还是匹配不到则去匹配端口的第一个server 一个server模块通过IF指令判断转发到不同端口/服务器可以借助Nginx if指令判断$host中的域名来转发到不同的端口: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { if ($host = &quot;www.cddj.top&quot;) { proxy_pass http://localhost:801; } if ($host = &quot;www.51offer.wang&quot;) { proxy_pass http://localhost:802; } if ($host = &quot;sub.51offer.wang&quot;) { proxy_pass http://localhost:803; } } }server { listen 801; server_name www.cddj.top; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; } }server { listen 802; server_name www.51offer.wang; #charset koi8-r; #access_log logs/host.access.log main; location / { root html1; index index.html index.htm; } }server { listen 803; server_name sub.51offer.wang; #charset koi8-r; #access_log logs/host.access.log main; location / { root html2; index index.html index.htm; } } 结果和上面的图片一样，这里就不追加了。 Nginx+Lua我们可以借助Nginx+Lua来判断转发，这里也暂时就不说明，有兴趣的 可以自己去查找资料，在使用Lua的时候需要自行下载Nginx源码加Lua模块编译进去，或者使用openresty、tengine这种已经集成好的工具，个人比较喜欢openresty,因为社区相对比较活跃，有很多强大的插件如waf等。 修改配置后记得刷新配置哟 12nginx -t #验证配置文件nginx -s reload 刷新配置文件 如果有更方便快捷且强大的方式，请加我QQ告知我哟！！！","link":"/posts/1940554430.html"},{"title":"尚无已经注册的AppId，请先使用AccessTokenContainer.Register完成注册（全局执行一次即可）！模块：WeChat_OfficialAccount","text":"最近一个朋友在咨询我微信公众号推送消息的问题。因为我在17年的时候做了一年的微信公众号开发，自然有一丢丢经验，但是那时的服务端是用Spring Boot 开发的，这次是.net开发，而且时间也有点久了。所以在开发过程中遇到了一些问题。接下来，我就来和盆友们唠嗑唠嗑。 用.net开发公众号的各位盆友应该知道Senparc这个神器，其封装了微信公众号/小程序等接口，开箱即用。因为我在15年的时，使用ABP+Senparc开发了一个公众号网页，深感其带来的方便快捷。下面，我们进入正题吧。因为我不能进入到微信公众号后端，所以朋友就把AppId和AppSecret给我，然后就只有这么搞： 1234567891011121314151617181920212223242526272829303132333435363738394041424344var isGLobalDebug = true;//设置全局 Debug 状态var senparcSetting = SenparcSetting.BuildFromWebConfig(isGLobalDebug);var register = RegisterService.Start(senparcSetting).UseSenparcGlobal();//CO2NET全局注册，必须！var isWeixinDebug = true;//设置微信 Debug 状态var senparcWeixinSetting = SenparcWeixinSetting.BuildFromWebConfig(isWeixinDebug);senparcWeixinSetting.WeixinAppId = appId;senparcWeixinSetting.WeixinAppSecret = appSecret; register.UseSenparcWeixin(senparcWeixinSetting, senparcSetting).RegisterMpAccount(senparcWeixinSetting, &quot;【盛派网络小助手】公众号&quot;);//根据appId判断获取 if (!AccessTokenContainer.CheckRegistered(appId)) //检查是否已经注册 { AccessTokenContainer.RegisterAsync(appId, appSecret); //如果没有注册则进行注册 }string linkUrl = &quot;http://www.baidu.com&quot;; //点击详情后跳转后的链接地址，为空则不跳转 //为模版中的各属性赋值 var templateData = new{ first = new TemplateDataItem(&quot;您好，您的订单已支付成功！&quot;, &quot;#000000&quot;), product = new TemplateDataItem(&quot;旺旺大礼包&quot;, &quot;#000000&quot;), price = new TemplateDataItem(&quot;99.8元&quot;, &quot;#000000&quot;), time = new TemplateDataItem(&quot;2016-11-09 16:50:38&quot;, &quot;#000000&quot;), remark = new TemplateDataItem(&quot;感谢您的光临~&quot;, &quot;#000000&quot;)};string access_token = AccessTokenContainer.GetAccessToken(appId);SendTemplateMessageResult sendResult = TemplateApi.SendTemplateMessage(access_token, openId, templateId, linkUrl, templateData);//发送成功 if (sendResult.errcode.ToString() == &quot;请求成功&quot;){ Response.Write(&quot;请求成功&quot;);}else{ Response.Write(&quot;出现错误：&quot; + sendResult.errmsg);}Response.Write(&quot;ok&quot;); 肯定是我太菜，所以我就百度一番，顺手又改了几行代码: 12345678910111213141516171819202122232425262728293031323334var isGLobalDebug = true;//设置全局 Debug 状态var senparcSetting = SenparcSetting.BuildFromWebConfig(isGLobalDebug);var register = RegisterService.Start(senparcSetting).UseSenparcGlobal();//CO2NET全局注册，必须！var isWeixinDebug = true;//设置微信 Debug 状态var senparcWeixinSetting = SenparcWeixinSetting.BuildFromWebConfig(isWeixinDebug);register.UseSenparcWeixin(senparcWeixinSetting, senparcSetting); AccessTokenContainer.RegisterAsync(appId, appSecret); //如果没有注册则进行注册 string linkUrl = &quot;http://www.baidu.com&quot;; //点击详情后跳转后的链接地址，为空则不跳转 //为模版中的各属性赋值 var templateData = new{ first = new TemplateDataItem(&quot;您好，您的订单已支付成功！&quot;, &quot;#000000&quot;), product = new TemplateDataItem(&quot;旺旺大礼包&quot;, &quot;#000000&quot;), price = new TemplateDataItem(&quot;99.8元&quot;, &quot;#000000&quot;), time = new TemplateDataItem(&quot;2016-11-09 16:50:38&quot;, &quot;#000000&quot;), remark = new TemplateDataItem(&quot;感谢您的光临~&quot;, &quot;#000000&quot;)};string access_token = AccessTokenContainer.GetAccessToken(appId);SendTemplateMessageResult sendResult = TemplateApi.SendTemplateMessage(access_token, openId, templateId, linkUrl, templateData);//发送成功 if (sendResult.errcode.ToString() == &quot;请求成功&quot;){ Response.Write(&quot;请求成功&quot;);}else{ Response.Write(&quot;出现错误：&quot; + sendResult.errmsg);}Response.Write(&quot;ok&quot;); 但是还是报同样的错误，顿时绝望至极，我到底是又多菜啊？但是这个人虽然菜，可是也不是刚才说的那样脆弱，所以我就去Senparc的github看看。嗯可以看到Senparc非常友好的抛出这个错误，那么它是什么时候给我们抛出错误呢？这就要看看代码了: 123456789101112131415161718192021222324/// &lt;summary&gt;/// 获取凭证接口/// &lt;/summary&gt;/// &lt;param name=&quot;grant_type&quot;&gt;获取access_token填写client_credential&lt;/param&gt;/// &lt;param name=&quot;appid&quot;&gt;第三方用户唯一凭证&lt;/param&gt;/// &lt;param name=&quot;secret&quot;&gt;第三方用户唯一凭证密钥，既appsecret&lt;/param&gt;/// &lt;returns&gt;&lt;/returns&gt;[ApiBind(NeuChar.PlatformType.WeChat_OfficialAccount, &quot;CommonApi.GetToken&quot;, true)]public static AccessTokenResult GetToken(string appid, string secret, string grant_type = &quot;client_credential&quot;){ //注意：此方法不能再使用ApiHandlerWapper.TryCommonApi()，否则会循环 var url = string.Format(Config.ApiMpHost + &quot;/cgi-bin/token?grant_type={0}&amp;appid={1}&amp;secret={2}&quot;, grant_type.AsUrlData(), appid.AsUrlData(), secret.AsUrlData()); AccessTokenResult result = Get.GetJson&lt;AccessTokenResult&gt;(url);//此处为最原始接口，不再使用重试获取的封装 if (Config.ThrownWhenJsonResultFaild &amp;&amp; result.errcode != ReturnCode.请求成功) { var unregisterAppIdEx = new UnRegisterAppIdException(null, $&quot;尚无已经注册的AppId，请先使用AccessTokenContainer.Register完成注册（全局执行一次即可）！模块：{NeuChar.PlatformType.WeChat_OfficialAccount}&quot;); throw unregisterAppIdEx;//抛出异常 } return result;} 就是说，只要不是成功状态，都是提示尚无已经注册的AppId，请先使用AccessTokenContainer.Register完成注册（全局执行一次即可）！模块：WeChat_OfficialAccount(我能说这样有点坑吗？),那么我猜想应该是调用接口出错了，那么就看看是怎么请求的接口呢？继续看代码吧: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/// &lt;summary&gt; /// 获取可用Token /// &lt;/summary&gt; /// &lt;param name=&quot;appId&quot;&gt;&lt;/param&gt; /// &lt;param name=&quot;getNewToken&quot;&gt;是否强制重新获取新的Token&lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; public static string GetAccessToken(string appId, bool getNewToken = false) { return GetAccessTokenResult(appId, getNewToken).access_token; } /// &lt;summary&gt; /// 获取可用AccessTokenResult对象 /// &lt;/summary&gt; /// &lt;param name=&quot;appId&quot;&gt;&lt;/param&gt; /// &lt;param name=&quot;getNewToken&quot;&gt;是否强制重新获取新的Token&lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; public static AccessTokenResult GetAccessTokenResult(string appId, bool getNewToken = false) { if (!BaseContainer&lt;AccessTokenBag&gt;.CheckRegistered(appId)) { throw new UnRegisterAppIdException(appId, $&quot;此appId（{appId}）尚未注册，请先使用AccessTokenContainer.Register完成注册（全局执行一次即可）！&quot;, null); } AccessTokenBag accessTokenBag = BaseContainer&lt;AccessTokenBag&gt;.TryGetItem(appId); using (BaseContainer&lt;AccessTokenBag&gt;.Cache.BeginCacheLock(&quot;MP.AccessTokenContainer&quot;, appId, 0, default(TimeSpan))) { if (getNewToken || accessTokenBag.AccessTokenExpireTime &lt;= SystemTime.Now) { accessTokenBag.AccessTokenResult = CommonApi.GetToken(accessTokenBag.AppId, accessTokenBag.AppSecret, &quot;client_credential&quot;); accessTokenBag.AccessTokenExpireTime = ApiUtility.GetExpireTime(accessTokenBag.AccessTokenResult.expires_in); BaseContainer&lt;AccessTokenBag&gt;.Update(accessTokenBag, null); } } return accessTokenBag.AccessTokenResult; } public class CommonApi { /// &lt;summary&gt; /// 获取凭证接口 /// &lt;/summary&gt; /// &lt;param name=&quot;grant_type&quot;&gt;获取access_token填写client_credential&lt;/param&gt; /// &lt;param name=&quot;appid&quot;&gt;第三方用户唯一凭证&lt;/param&gt; /// &lt;param name=&quot;secret&quot;&gt;第三方用户唯一凭证密钥，既appsecret&lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; [ApiBind(Senparc.NeuChar.PlatformType.WeChat_OfficialAccount, &quot;CommonApi.GetToken&quot;, true)] public static AccessTokenResult GetToken(string appid, string secret, string grant_type = &quot;client_credential&quot;) { AccessTokenResult json = Get.GetJson&lt;AccessTokenResult&gt;(string.Format(Config.ApiMpHost + &quot;/cgi-bin/token?grant_type={0}&amp;appid={1}&amp;secret={2}&quot;, grant_type.AsUrlData(), appid.AsUrlData(), secret.AsUrlData()), null, null); if (Config.ThrownWhenJsonResultFaild &amp;&amp; json.errcode != 0) { throw new UnRegisterAppIdException(null, $&quot;尚无已经注册的AppId，请先使用AccessTokenContainer.Register完成注册（全局执行一次即可）！模块：{Senparc.NeuChar.PlatformType.WeChat_OfficialAccount}&quot;, null); } return json; } } 根据代码我我就拼接了一个http的地址,直接在浏览器访问，得到了这样的结果: 1{&quot;errcode&quot;:40164,&quot;errmsg&quot;:&quot;invalid ip 183.221.39.24 ipv6 ::ffff:183.221.39.24, not in whitelist hint: [uBiTIa01561501]&quot;} 一看就知道，请求地址没有加入到白名单里面，所以我让朋友去登录公众平台，开发-&gt;基本配置-&gt;IP白名单-&gt;查看-&gt;修改-&gt;将IP地址添加进去,最后，OK了。","link":"/posts/2560766626.html"},{"title":".Net Core3.x部署到阿里云ACK中","text":"前面，我使用自己的服务器基于Docker部署了core程序，现在我们来使用一下新的方法，将我们的程序发布到阿里云ACK中，如果是IDEA的话，可以使用Alibaba Cloud Toolkit实现快速部署，但是现在仅支持IntelliJ IDEA、Eclipse、PyCharm 以及其他、Mave，不过据说VS CODE快要来了。那么现在我们暂时就先手动来操作一番吧 发布Core程序我习惯使用命令方式: 1dotnet publish -r linux-x64 -o ./bin/output -c release 具体命令说明详见https://docs.microsoft.com/zh-cn/dotnet/core/tools/dotnet-publish?tabs=netcore21,其中的RID也比较重要，大家也需要了解一下.我们开始编写Dockerfile: 123456FROM mcr.microsoft.com/dotnet/core/runtime:3.1WORKDIR /appEXPOSE 80EXPOSE 443COPY [&quot;.&quot;, &quot;.&quot;]ENTRYPOINT [&quot;dotnet&quot;, &quot;Web.dll&quot;] 开始构建： 12345678910111213141516171819202122232425262728293031[root@instance-p0a4erj8 core]# docker build -t core3.x-swagger -f Dockerfile .Sending build context to Docker daemon 125.3MBStep 1/6 : FROM mcr.microsoft.com/dotnet/core/runtime:3.13.1: Pulling from dotnet/core/runtime000eee12ec04: Pull complete 67bac0b5d3cc: Pull complete e8c80b499c83: Pull complete 77b73bc084ae: Pull complete Digest: sha256:9946cd419d740e903f94677ed57af28f56e77b967186b868ed64765b870bf49dStatus: Downloaded newer image for mcr.microsoft.com/dotnet/core/runtime:3.1 ---&gt; 08d8cf51bdfdStep 2/6 : WORKDIR /app ---&gt; Running in 6663a593f044Removing intermediate container 6663a593f044 ---&gt; bb8603da7b34Step 3/6 : EXPOSE 80 ---&gt; Running in 8c844a10467eRemoving intermediate container 8c844a10467e ---&gt; 61cf39dd9c13Step 4/6 : EXPOSE 443 ---&gt; Running in 7a616725163cRemoving intermediate container 7a616725163c ---&gt; 6ded800d3aecStep 5/6 : COPY [&quot;.&quot;, &quot;.&quot;] ---&gt; 4eb15d11ccb0Step 6/6 : ENTRYPOINT [&quot;dotnet&quot;, &quot;Web.dll&quot;] ---&gt; Running in a087553cf793Removing intermediate container a087553cf793 ---&gt; 90fc47847faaSuccessfully built 90fc47847faaSuccessfully tagged core3.x-swagger:latest 运行我们的镜像: 1docker run --name swagger -d -p 80:80 core3.x-swagger 可以看到我们的容器已经启动好了。 123[root@instance-p0a4erj8 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeb38557539a9 core3.x-swagger &quot;dotnet Web.dll&quot; About a minute ago Up About a minute 0.0.0.0:80-&gt;80/tcp, 443/tcp swagger 可以查看一下日志: 1234567891011[root@instance-p0a4erj8 ~]# docker logs eb38557539a9info: Microsoft.Hosting.Lifetime[0] Now listening on: http://[::]:80info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down.info: Microsoft.Hosting.Lifetime[0] Hosting environment: Productioninfo: Microsoft.Hosting.Lifetime[0] Content root path: /appwarn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3] Failed to determine the https port for redirect. 访问也是正常.好，接下来，我们就上传到阿里的ACR上面。先去https://cr.console.aliyun.com进行设置.首先需要提示设置密码.设置完后，会跳转到创建仓库页面创建完成后，我们就开始登陆到ACR 1234567[root@instance-p0a4erj8 ~]# docker login --username=xxxxxx registry.cn-hangzhou.aliyuncs.comPassword: WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded 推送镜像 1234567891011121314 docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/52fx/52fx:[镜像版本号] docker push registry.cn-hangzhou.aliyuncs.com/52fx/52fx:[镜像版本号] 示例:[root@instance-p0a4erj8 ~]# docker tag core3.x-swagger registry.cn-hangzhou.aliyuncs.com/52fx/52fx:1.0[root@instance-p0a4erj8 ~]# docker push registry.cn-hangzhou.aliyuncs.com/52fx/52fx:1.0The push refers to repository [registry.cn-hangzhou.aliyuncs.com/52fx/52fx]66fe7dbc7904: Pushed 3391145729e6: Pushed 52d5ea296228: Pushed 239bf536471e: Pushed cad0d4e88a35: Pushed 831c5620387f: Pushed 1.0: digest: sha256:b37a187840b68d1937ffef2c350bc190f7f435840ece7262ae9445533c5fb766 size: 1583 部署镜像 创建K8S集群，需要创建一个专有网络 需要开通两个授权的权限 创建成功后需要耐心的等待几分钟 接下来就是部署镜像,全是界面操作。 创建好Service-&gt;Ingress就可以访问了 service: 12345678910111213141516171819202122apiVersion: v1kind: Servicemetadata: creationTimestamp: &apos;2019-12-14T03:28:23Z&apos; name: net namespace: default resourceVersion: &apos;19318673&apos; selfLink: /api/v1/namespaces/default/services/net uid: c8ba0fb3-1e21-11ea-9a91-9e68548c9b68spec: clusterIP: None ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: 52fx-default sessionAffinity: None type: ClusterIPstatus: loadBalancer: {} Ingress： 123456789101112131415161718192021222324apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/service-weight: &apos;net: 100&apos; creationTimestamp: &apos;2019-12-14T03:33:32Z&apos; generation: 1 name: 52fx namespace: default resourceVersion: &apos;19338000&apos; selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/52fx uid: 80bd156a-1e22-11ea-9a91-9e68548c9b68spec: rules: - http: paths: - backend: serviceName: net servicePort: 80 path: /status: loadBalancer: ingress: - ip: 39.105.240.144 后面，我们再详细了解一下k8s的相关配置参考容器服务Kubernetes版","link":"/posts/266680198.html"},{"title":"Nginx为同主机配置Https多域名","text":"前面，我有提到nginx多网站配置，以及nginx配置https.但是在windows下面同主机配置多Https域名暂未提及，那么这次正好遇到这个场景。我就把他记录下来 前言最近在做一个私活，因为老板考虑到节约成本，就只有一台2核4G的windows主机，可是现在是要独立出来多个微信小程序，故会涉及到多个应用。当然我也以才用nginx强大的location配置功能转发到不同的目录这也是可以实现的。但是感觉不是很优雅，所以还是使用强大的Server模块吧。我们来看看官网介绍http://nginx.org/en/docs/http/configuring_https_servers.html,官方文档也是很详细的 下载Nginxwindows使用nginx很简单，直接下载官方编译好的即可(如果自己来编译的话,操作还是有些繁琐).下载nginx的zip压缩包，如果想使用nginx+lua的话，那么就去下载OpenResty吧。 配置Nginx首先肯定是去域名注册商去下载nginx的SSL证书，然后把我们的证书放在nginx的conf目录下。接下来就是编辑我们的nginx.conf配置文件。配置如下: 123456789101112131415161718192021222324252627282930313233server { listen 443 ssl; server_name xxxxx.baoqipai.com; ssl_certificate cert/xxxxx.baoqipai.com.pem; ssl_certificate_key cert/xxxxx.baoqipai.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { proxy_pass http://127.0.0.1:8002; } } server { listen 443 ssl; server_name admin.xxxxx.nationallab.cn; ssl_certificate cert/admin.xxxxx.nationallab.cn.pem; ssl_certificate_key cert/admin.xxxxx.nationallab.cn.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { proxy_pass http://127.0.0.1:8001; } } server { listen 443 ssl; server_name host.xxxxx.nationallab.cn; ssl_certificate cert/host.xxxxx.nationallab.cn.pem; ssl_certificate_key cert/host.xxxxx.nationallab.cn.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { proxy_pass http://127.0.0.1:8001; } } 启动nginx，解析域名，此时不出意外应该可以正常访问了。 遇到的问题这里有个会有一个小问题,会报could not build server_names_hash, you should increase server_names_hash_bucket_size: 32,这是因为server配置中server_name的值过长导致，http://nginx.org/en/docs/http/server_names.html提到修改server_names_hash_bucket_size的值: http { server_names_hash_bucket_size 64; …那么server_names_hash_bucket_size的默认值是多少呢？在官网文档中有提到: 123Syntax: server_names_hash_bucket_size size;Default: server_names_hash_bucket_size 32|64|128;Context: http Sets the bucket size for the server names hash tables. The default value depends on the size of the processor’s cache line. The details of setting up hash tables are provided in a separate document.","link":"/posts/1168520487.html"},{"title":"Spring Boot 常用注解汇总","text":"Spring Boot是越来越火了也主要是因为其注解给我们带来了莫大的帮助，使我们开发更加的快速便捷，所以，我们有必要简单的整理一下工作中常用的注解命令。 @SpringBootApplication:作用在主类上，标识该应用为Spring Boot 应用,为应用赋能。 12345678@SpringBootApplicationpublic class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }} 在spring-boot-autoconfigure中，我们可以看到@SpringBootApplication包含了@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan三个注解： 1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })public @interface SpringBootApplication {} @Configuration:该注解用于标识该类为一个配置类，作用于类，相当于配置文件如： 1234&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd&quot; default-autowire=&quot;byName&quot;&gt; &lt;bean id=&quot;dmsDataSecurityRuleService&quot; class=&quot;xxxx.xx.datasecurity.service.DmsDataSecurityRuleService&quot;/&gt; &lt;bean id=&quot;dmsDataSecurityRuleAction&quot; class=&quot;xxxx.xx.datasecurity.web.DmsDataSecurityRuleAction&quot;/&gt;&lt;/beans&gt; @Bean:该注解主要作用于方法，作用是将该方法放回的类实例注入到IOC容器中.默认取方法名为对象bean id，可以使用@Bean注解设置属性值来指定bean id。 12345678910111213141516171819202122232425@Configuration //标识为配置类@EnableSwagger2@EnableSwaggerBootstrapUi@Import(BeanValidatorPluginsConfiguration.class)public class SwaggerConfiguration { @Bean(&quot;createRestApi&quot;) //指定bean id public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo( new ApiInfoBuilder() //页面标题 .title(&quot;Demo Web Api文档&quot;) //创建人 .contact(new Contact(&quot;eyiadmin&quot;, &quot;https://springfox.github.io/springfox/&quot;, &quot;eyiadmin@163.com&quot;)) .version(&quot;1.0&quot;) .description(&quot;Demo Web Api文档&quot;) .build()) .select() //API接口所在的包位置 .apis(RequestHandlerSelectors.basePackage(&quot;com.eyiadmin.demo.controller&quot;)) .paths(PathSelectors.any()) .build(); }} @Scope:该注解作用于类方法，用于指定bean的作用域，默认为单例： prototype：每次从IOC容器中取出对象都是重新创建Bean实例 singleton：整个应用IOC容器中只有一个Bean实例 request：同一个http请求创建一个Bean实例 session：同一个Session会话创建一个Bean实例 @EnableAutoConfiguration:允许 Spring Boot 自动配置注解，开启这个注解之后，Spring Boot 就能根据当前类路径&gt;下的包或者类来配置 Spring Bean。如：当前类路径下有 Mybatis 这个 JAR 包，MybatisAutoConfiguration 注解就能根据相&gt;关参数来配置 Mybatis 的各个 Spring Bean。 @SpringBootConfiguration:这个注解就是 @Configuration 注解的变体，只是用来修饰是 Spring Boot 配置而已，或&gt;者可利于 Spring Boot 后续的扩展。 @ComponentScan:该注解用来代替配置文件中的 component-scan 配置，开启&gt;组件扫描，相当于context:component-scan，如果扫描到有@Component @Controller @Service等这些注解的类，则把这些类注册为Bean @Component:该注解是将普通pojo实例化到spring容器中，相当于XML中的，一般加在主类上。 @AutoWired:该注解是将注入到的bean实例取出来，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。当加上（required=false）时，就算找不到bean也不报错。 @RestController:该注解相当于@Controller、@ResponseBody两个注解的结合,我们可以再github查看源码https://github.com/spring-projects/spring-framework/blob/master/spring-web/src/main/java/org/springframework/web/bind/annotation/RestController.java: 1234567891011121314151617@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Controller@ResponseBodypublic @interface RestController { /** * The value may indicate a suggestion for a logical component name, * to be turned into a Spring bean in case of an autodetected component. * @return the suggested component name, if any (or empty String otherwise) * @since 4.0.1 */ @AliasFor(annotation = Controller.class) String value() default &quot;&quot;;} controller相关注解在https://github.com/spring-projects/spring-framework/tree/master/spring-web/src/main/java/org/springframework/web/bind/annotation能找到。 @Controller:该注解主要作用于类上，标识该类为一个控制器，在对应的类上加上该注解，视图解析器可以解析return 的jsp,html页面，并且跳转到相应页面。 @ResponseBody:表示该方法的返回结果直接写入HTTP response body中一般在异步获取数据时使用，在使用@RequestMapping后，返回值通常解析为跳转路径，加上@responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。比如异步获取json数据，加上@responsebody后，会直接返回json数据。 @RequestMapping:该注解会将 HTTP 请求映射到 MVC 和 REST 控制器的处理方法上,看源码: 1234567891011121314151617181920212223242526272829303132@Target({ElementType.TYPE, ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping { String name() default &quot;&quot;; @AliasFor(&quot;path&quot;) String[] value() default {}; @AliasFor(&quot;value&quot;) String[] path() default {}; RequestMethod[] method() default {}; String[] params() default {}; String[] headers() default {}; String[] consumes() default {}; String[] produces() default {};}RequestMethod是一个枚举类型:public enum RequestMethod { GET, HEAD, POST, PUT, PATCH, DELETE, OPTIONS, TRACE} 使用时需要执行value和method参数,如:@RequestMapping(value = &quot;hello/{name}&quot;, method= RequestMethod.GET). @GetMapping:该注解将HTTP GET请求映射到指定处理程序: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documented@RequestMapping(method = RequestMethod.GET)public @interface GetMapping { /** * Alias for {@link RequestMapping#name}. */ @AliasFor(annotation = RequestMapping.class) String name() default &quot;&quot;; /** * Alias for {@link RequestMapping#value}. */ @AliasFor(annotation = RequestMapping.class) String[] value() default {}; /** * Alias for {@link RequestMapping#path}. */ @AliasFor(annotation = RequestMapping.class) String[] path() default {}; /** * Alias for {@link RequestMapping#params}. */ @AliasFor(annotation = RequestMapping.class) String[] params() default {}; /** * Alias for {@link RequestMapping#headers}. */ @AliasFor(annotation = RequestMapping.class) String[] headers() default {}; /** * Alias for {@link RequestMapping#consumes}. * @since 4.3.5 */ @AliasFor(annotation = RequestMapping.class) String[] consumes() default {}; /** * Alias for {@link RequestMapping#produces}. */ @AliasFor(annotation = RequestMapping.class) String[] produces() default {};} 我在源码中可以看到@RequestMapping(method = RequestMethod.GET)这句代码,组合了@RequestMapping注解并指定了GET请求方式,一般使用@GetMapping注解时，指定其value属性即可,如:@GetMapping(value = &quot;/name&quot;),等价于上面的@RequestMapping(value = &quot;hello/{name}&quot;, method= RequestMethod.GET)。除了@GetMapping,还有@PostMapping、@PutMapping、@DeleteMapping等，也是才用类似的方式，就不一一说明了。 @PathVariable:该注解用于获取url中占位符的数据，如： 1234567@RequestMapping(value = &quot;hello/{name}&quot;, method= RequestMethod.GET) public ResponseEntity&lt;String&gt; Hello( @PathVariable String name) { return new ResponseEntity&lt;&gt;(String.format(&quot;Hello %s!&quot;,name), HttpStatus.OK); }请求格式:http://localhost:8080/demo/hello/jane @RequestParam:该注解主要用于获取url中的请求参数,如: 1234567@RequestMapping(value = &quot;baby&quot;, method= RequestMethod.GET) public ResponseEntity&lt;String&gt; Baby( @RequestParam String name) { return new ResponseEntity&lt;&gt;(String.format(&quot;Hello, %s Baby!&quot;,name), HttpStatus.OK); }请求格式:http://localhost:8080/demo/baby?name=Jane @RequestBody:该注解用于接收HTTP的Body的内容并序列化为接受类型的对象，可接受复杂嵌套的内容，默认是使用JSON的格式。 还有@RequestHeader、@CookieValue等，大家自己去查阅相关文档吧 @Value:该注解主要作用于字段属性，用于属性取值: @value(“值”),如:@value(&quot;农民工&quot;); @Value(“#{}”) 表示SpEl表达式通常用来获取bean的属性，或者调用bean的某个方法,如:@Value(&quot;#{12*2}&quot;); @Value(“${xxxx}”)注解从配置文件读取值,如:@Value(&quot;${datasource.url}&quot;); 一般我们要取配置文件内容，还会配合@ConfigurationProperties一起使用,主要是用于指定配置文件中的指定 属性与该Bean绑定,如:@ConfigurationProperties(&quot;datasource&quot;) @Repository:该注解用于标注数据访问组件(DAO),作用于类,https://github.com/spring-projects/spring-framework/blob/master/spring-context/src/main/java/org/springframework/stereotype/Repository.java: 123456789101112131415@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface Repository { /** * The value may indicate a suggestion for a logical component name, * to be turned into a Spring bean in case of an autodetected component. * @return the suggested component name, if any (or empty String otherwise) */ @AliasFor(annotation = Component.class) String value() default &quot;&quot;;} @Service:该注解用于标识用业务层组件,作用于类,源码如下: 123456789101112131415@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface Service { /** * The value may indicate a suggestion for a logical component name, * to be turned into a Spring bean in case of an autodetected component. * @return the suggested component name, if any (or empty String otherwise) */ @AliasFor(annotation = Component.class) String value() default &quot;&quot;;} value值作为Bean id,我们可以为Service指定Bean id,如:@Service(&quot;serviceBeanId&quot;) @PropertySource:该注解用于导入properties配置文件,如:@PropertySource(value = {&quot;classpath : path/application.properties&quot;})，多配置文件在{}中意逗号(,)隔开。 @ImportResource:该注解用于导入xml配置文件,支持相对路径和绝对路径,代码如下： 1234567891011121314@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documentedpublic @interface ImportResource { @AliasFor(&quot;locations&quot;) String[] value() default {}; @AliasFor(&quot;value&quot;) String[] locations() default {}; Class&lt;? extends BeanDefinitionReader&gt; reader() default BeanDefinitionReader.class;} @ControllerAdvice:该注解主要用于统一处理异常，作用于类，组合了@Component注解,源码如下： 12345678910111213141516171819@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface ControllerAdvice { @AliasFor(&quot;basePackages&quot;) String[] value() default {}; @AliasFor(&quot;value&quot;) String[] basePackages() default {}; Class&lt;?&gt;[] basePackageClasses() default {}; Class&lt;?&gt;[] assignableTypes() default {}; Class&lt;? extends Annotation&gt;[] annotations() default {};} 用法如下: 12345@ControllerAdvicepublic class GlobalExceptionHandler { } @ExceptionHandler:该注解声明异常处理方法，作用于方法上,例如: 1234@ExceptionHandler(Exception.class)String catchException(){ return &quot;&quot;;} 当触发Exception会执行catchException方法。 目前能想到的大概就这些，后面有新发现再补充吧。 [参考]终于有人总结Spring Boot最常用的25个注解，干货了解一下！","link":"/posts/4032611999.html"},{"title":"Spring Boot集成Swagger","text":"大部分人应该都知道Swagger是帮我们的Web API快速生成接口文档，前面我们也有提到.Net Core3.x集成Swagger，这里，我们再来归纳一下Spring Boot集成Swagger的常规操作 创建一个Spring Boot项目创建Spring Boot应用的方式有很多，如：直接访问spring提供的项目生成工具https://start.spring.io/、或者Idea里面的Spring Initializr模块创建等等。这里我们就直接通过Idea工具来创建，创建完成后，就是maven自动安装包了。 集成Swagger首先，我们需要引入引入jar包: 123456789101112&lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger2 --&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger-ui --&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt; 配置Swagger 新建一个SwaggerConfiguration类，添加注解@Configuration、@EnableSwagger21234567891011121314151617181920212223@Configuration@EnableSwagger2public class SwaggerConfiguration { @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo( new ApiInfoBuilder() //页面标题 .title(&quot;Demo Web Api文档&quot;) //创建人 .contact(new Contact(&quot;eyiadmin&quot;, &quot;https://springfox.github.io/springfox/&quot;, &quot;eyiadmin@163.com&quot;)) .version(&quot;1.0&quot;) .description(&quot;Demo Web Api文档&quot;) .build()) .select() //API接口所在的包位置 .apis(RequestHandlerSelectors.basePackage(&quot;com.eyiadmin.demo.controller&quot;)) .paths(PathSelectors.any()) .build(); }} 配置Controller:123456789101112@Api(tags = {&quot;Swagger Demo API展示&quot;})@RequestMapping(&quot;/demo&quot;)@RestControllerpublic class DemoController { @ApiOperation(value = &quot;示例&quot;, notes = &quot;通过名字打个招呼&quot;) @RequestMapping(value = &quot;hello/{name}&quot;, method= RequestMethod.GET) public ResponseEntity&lt;String&gt; Hello(@ApiParam(value = &quot;用户姓名&quot;,required = true) @PathVariable String name) { return new ResponseEntity&lt;&gt;(String.format(&quot;Hello %s!&quot;,name), HttpStatus.OK); }} 启动起来，访问localhost:8080/swagger-ui.html，还是那个熟悉的界面: 使用Swagger增强版knife4j-spring-ui在https://gitee.com/xiaoym/knife4j提供了swagger-bootstrap-ui,界面相对来说更加美观,也可以导出md文档，同时可以借助其他工具转成pdf等文档我们把修改一下pom.xml文件，将swagger替换成knife4j: 12345&lt;dependency&gt; &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt; &lt;artifactId&gt;knife4j-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.9.6&lt;/version&gt; &lt;/dependency&gt; 修改配置类: 1234@Configuration@EnableSwagger2@EnableSwaggerBootstrapUi@Import(BeanValidatorPluginsConfiguration.class) 启动Spring Boot，访问localhost:8080/doc.html：至此，Spring Boot集成Swagger暂时告一段落,下面，我会整理一些Swagger 常用注解. @Api一般作用于Controller类,标识类作用说明 1234参数说明:* tags=&quot;说明该类的作用&quot;* value=&quot;可不配置&quot;@Api(tags = {&quot;Swagger Demo API展示&quot;}) @ApiOperation一般作用于方法，标识说明该方法的作用 1234参数说明: * value=&quot;方法描述&quot; * notes=&quot;备注说明&quot;@ApiOperation(value = &quot;方法描述&quot;, notes = &quot;备注说明&quot;) @ApiParam作用于方法，解释方法中的参数说明 1public ResponseEntity&lt;String&gt; Hello(@ApiParam(value = &quot;参数名称&quot;,required = true) @PathVariable String name) @ApiModel作用于VO类,解释说明类 1@ApiModel(&quot;例如说明&quot;) @ApiModelProperty作用于字段属性,为VO类属性解释说明 12@ApiModelProperty(&quot;用户名&quot;) private String userName;","link":"/posts/2812958799.html"},{"title":"Spring Boot整合Mybatis Plus和Druid","text":"在Java中，我比较ORM熟悉就只有Hibernate和Mybatis,其他的并未实践使用过，在这二者之间我更喜欢Mybatis,因为它精简、灵活(毕竟我是上年纪的程序员，喜欢自己写SQL)。 刚才有提到Mybatis，但是这里的重点是介绍Mybatis-Plus，它是Mybatis的增强版，如果要了解Mybatis的细节的话请点击这里。 简介据MyBatis-Plus官网介绍，MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。这看上去挺香的，所以必须得尝尝。这里涉及到的环境、组件如下: MariaDB 10.3.10 Windows 10 IntelliJ IDEA 2019.3.1 64 bit JDK 1.8.0_231 Spring Boot 2.2.3.RELEASE Lombok 1.18.10 Knife4j 2.0.1 Mybatis-plus 3.3.0 Druid Spring Boot 整合Mybatis-Plus我之前一直是直接用的Mybatis,但是作为喜欢偷懒的人，当然得想办法来提高我们的效率，所以就想着用Mybatis-Plus来省去一些单表的CRUD操作再结合MyBatis-Plus配套的AutoGenerator代码生成器，就能为我们节省不少时间。 引入依赖包首先，我们得引入Mybatis-Plus和mariadb-java-client等几个包: 12345678910111213141516171819&lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger2 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt; &lt;artifactId&gt;knife4j-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.baomidou/mybatis-plus-boot-starter --&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.mariadb.jdbc/mariadb-java-client --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mariadb.jdbc&lt;/groupId&gt; &lt;artifactId&gt;mariadb-java-client&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;/dependency&gt; 在application.yml配置我们的数据库连接信息: 12345678910111213spring: datasource: driver-class-name: org.mariadb.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/nacos_config?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true username: root password: root minimum-idle: 5 maximum-pool-size: 50 auto-commit: true idle-timeout: 30000 max-lifetime: 1800000 connection-timeout: 30000 connection-test-query: SELECT 1 这里我新建一个名为User的实体,具体属性如下: 12345678@Data@Builder@TableName(&quot;users&quot;)public class User { private String username; private String password; private int enabled;} 新建一个UserMapper接口: 12public interface UserMapper extends BaseMapper&lt;User&gt; {} 在我们的启动类加上@MapperScan来指定我们的Mapper扫描目录: 1@MapperScan(&quot;com.eyiadmin.demo.mapper&quot;) 我新建一个单元测试，来试试我们的UserMapper的selectList: 123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class UserTests { @Autowired private UserMapper userMapper; @Test public void testUser() { List&lt;User&gt; userList = userMapper.selectList(null); userList.forEach(System.out::println); }} 会看到Mybatis-Plus为我取出的数据: 12342020-01-20 09:44:14.125 TRACE org.apache.ibatis.logging.jdbc.BaseJdbcLogger.trace(BaseJdbcLogger.java:149) 2CNU7X5OLAUE004 --- [ main] c.e.d.m.U.selectList : &lt;== Columns: username, password, enabled2020-01-20 09:44:14.125 TRACE org.apache.ibatis.logging.jdbc.BaseJdbcLogger.trace(BaseJdbcLogger.java:149) 2CNU7X5OLAUE004 --- [ main] c.e.d.m.U.selectList : &lt;== Row: nacos, $2a$10$EuWPZHzz32dJN7jexM34MOeYirDdFAZm2kuWj7VEOJhhZkDrxfvUu, 12020-01-20 09:44:14.131 DEBUG org.apache.ibatis.logging.jdbc.BaseJdbcLogger.debug(BaseJdbcLogger.java:143) 2CNU7X5OLAUE004 --- [ main] c.e.d.m.U.selectList : &lt;== Total: 1User(username=nacos, password=$2a$10$EuWPZHzz32dJN7jexM34MOeYirDdFAZm2kuWj7VEOJhhZkDrxfvUu, enabled=1) 我们再试试Mybatis-Plus为什么封装的Insert: 12345@Test public void TestUserInsert() { int row = userMapper.insert(User.builder().password(&quot;aaaa&quot;).username(&quot;bbbb&quot;).enabled(1).build()); Assert.assertEquals(row, 1); } 可以看到日志: 122020-01-20 09:51:32.021 DEBUG org.apache.ibatis.logging.jdbc.BaseJdbcLogger.debug(BaseJdbcLogger.java:143) 2CNU7X5OLAUE004 --- [ main] c.e.d.m.U.insert : ==&gt; Preparing: INSERT INTO users ( username, password, enabled ) VALUES ( ?, ?, ? ) 2020-01-20 09:51:32.036 DEBUG org.apache.ibatis.logging.jdbc.BaseJdbcLogger.debug(BaseJdbcLogger.java:143) 2CNU7X5OLAUE004 --- [ main] c.e.d.m.U.insert : ==&gt; Parameters: bbbb(String), aaaa(String), 1(Integer) 其他高端操作请阅读相关文档https://mp.baomidou.com/guide/quick-start.html Spring Boot整合Druid数据库连接池在Spring Boot 2.X默认使用了HikariCP作为数据库连接池,据说hikariCP性能最高(hikariCP&gt;druid&gt;dbcp&gt;c3p0),但是我更喜欢Druid全面的功能和毫不逊色的性能。开撸吧，首先当然还是引入我们的需要的Druid依赖包: 123456&lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid-spring-boot-starter --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.21&lt;/version&gt;&lt;/dependency&gt; 接下来就是修改我们的的application.yml配置: 12345678910111213spring: datasource: driver-class-name: org.mariadb.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/nacos_config?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true username: root password: root ### 连接池配置 druid: initial-size: 50 max-active: 200 min-idle: 50 max-wait: 50 validation-query: SELECT 1 可以看到我们的日志信息为: 122020-01-20 10:14:54.581 INFO com.alibaba.druid.pool.DruidDataSource.close(DruidDataSource.java:2003) 2CNU7X5OLAUE004 --- [extShutdownHook] c.a.d.p.DruidDataSource : {dataSource-1} closing ...2020-01-20 10:14:54.691 INFO com.alibaba.druid.pool.DruidDataSource.close(DruidDataSource.java:2075) 2CNU7X5OLAUE004 --- [extShutdownHook] c.a.d.p.DruidDataSource : {dataSource-1} closed 现在我们增加一个Controller来获取Druid的监控数据: 12345678@RestControllerpublic class DruidStatController { @GetMapping(&quot;/druid/status&quot;) public Object druidStat(){ // DruidStatManagerFacade#getDataSourceStatDataList 该方法可以获取所有数据源的监控数据，除此之外 DruidStatManagerFacade 还提供了一些其他方法，你可以按需选择使用。 return DruidStatManagerFacade.getInstance().getDataSourceStatDataList(); }} 访问http://localhost:8080/druid/status可以得到一个json: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[{ &quot;Identity&quot;: 1914285129, &quot;Name&quot;: &quot;DataSource-1914285129&quot;, &quot;DbType&quot;: &quot;mysql&quot;, &quot;DriverClassName&quot;: &quot;org.mariadb.jdbc.Driver&quot;, &quot;URL&quot;: &quot;jdbc:mysql://127.0.0.1:3306/nacos_config?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&quot;, &quot;UserName&quot;: &quot;root&quot;, &quot;FilterClassNames&quot;: [], &quot;WaitThreadCount&quot;: 0, &quot;NotEmptyWaitCount&quot;: 0, &quot;NotEmptyWaitMillis&quot;: 0, &quot;PoolingCount&quot;: 50, &quot;PoolingPeak&quot;: 50, &quot;PoolingPeakTime&quot;: &quot;2020-01-20T02:26:22.466+0000&quot;, &quot;ActiveCount&quot;: 0, &quot;ActivePeak&quot;: 0, &quot;ActivePeakTime&quot;: null, &quot;InitialSize&quot;: 50, &quot;MinIdle&quot;: 50, &quot;MaxActive&quot;: 200, &quot;QueryTimeout&quot;: 0, &quot;TransactionQueryTimeout&quot;: 0, &quot;LoginTimeout&quot;: 0, &quot;ValidConnectionCheckerClassName&quot;: null, &quot;ExceptionSorterClassName&quot;: null, &quot;TestOnBorrow&quot;: false, &quot;TestOnReturn&quot;: false, &quot;TestWhileIdle&quot;: true, &quot;DefaultAutoCommit&quot;: true, &quot;DefaultReadOnly&quot;: null, &quot;DefaultTransactionIsolation&quot;: null, &quot;LogicConnectCount&quot;: 0, &quot;LogicCloseCount&quot;: 0, &quot;LogicConnectErrorCount&quot;: 0, &quot;PhysicalConnectCount&quot;: 50, &quot;PhysicalCloseCount&quot;: 0, &quot;PhysicalConnectErrorCount&quot;: 0, &quot;DiscardCount&quot;: 0, &quot;ExecuteCount&quot;: 0, &quot;ExecuteUpdateCount&quot;: 0, &quot;ExecuteQueryCount&quot;: 0, &quot;ExecuteBatchCount&quot;: 0, &quot;ErrorCount&quot;: 0, &quot;CommitCount&quot;: 0, &quot;RollbackCount&quot;: 0, &quot;PSCacheAccessCount&quot;: 0, &quot;PSCacheHitCount&quot;: 0, &quot;PSCacheMissCount&quot;: 0, &quot;StartTransactionCount&quot;: 0, &quot;TransactionHistogram&quot;: [0, 0, 0, 0, 0, 0, 0], &quot;ConnectionHoldTimeHistogram&quot;: [0, 0, 0, 0, 0, 0, 0, 0], &quot;RemoveAbandoned&quot;: false, &quot;ClobOpenCount&quot;: 0, &quot;BlobOpenCount&quot;: 0, &quot;KeepAliveCheckCount&quot;: 0, &quot;KeepAlive&quot;: false, &quot;FailFast&quot;: false, &quot;MaxWait&quot;: 50, &quot;MaxWaitThreadCount&quot;: -1, &quot;PoolPreparedStatements&quot;: false, &quot;MaxPoolPreparedStatementPerConnectionSize&quot;: 10, &quot;MinEvictableIdleTimeMillis&quot;: 1800000, &quot;MaxEvictableIdleTimeMillis&quot;: 25200000, &quot;LogDifferentThread&quot;: true, &quot;RecycleErrorCount&quot;: 0, &quot;PreparedStatementOpenCount&quot;: 0, &quot;PreparedStatementClosedCount&quot;: 0, &quot;UseUnfairLock&quot;: true, &quot;InitGlobalVariants&quot;: false, &quot;InitVariants&quot;: false}] 我们还可以打开stat-view-servlet,需要加入如下配置: 1234567spring: datasource: druid: stat-view-servlet: enabled: true login-username: admin login-password: admin 这是启动后，访问http://localhost:8080/druid/index.html页面就会跳转到登录页面,输入我们配置的用户名和密码admin/admin:可以看到Druid提供的功能是比较全面的，另外在第三张图可以看到我们的相关参数，大家也可以参照这个来配置连接池。 MyBatis-Plus的代码生成器Mybatis有Generator工具为我们提高编码效率，MyBatis-Plus也不示弱，它也提供有MyBatis-Plus AutoGenerator 。在上面的简单实体只有3个属性，加入有几十个属性怎么办呢？这时候AutoGenerator就可以帮我们一个大忙。首先引入所需包: 123456789101112&lt;!-- https://mvnrepository.com/artifact/com.baomidou/mybatis-plus-generator --&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.velocity/velocity-engine-core --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.velocity&lt;/groupId&gt; &lt;artifactId&gt;velocity-engine-core&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt;&lt;/dependency&gt; 新建CodeGenerator类: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 演示例子，执行 main 方法控制台输入模块表名回车自动生成对应项目目录中public class CodeGenerator { /** * &lt;p&gt; * 读取控制台内容 * &lt;/p&gt; */ public static String scanner(String tip) { Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(&quot;请输入&quot; + tip + &quot;：&quot;); System.out.println(help.toString()); if (scanner.hasNext()) { String ipt = scanner.next(); if (StringUtils.isNotBlank(ipt)) { return ipt; } } throw new MybatisPlusException(&quot;请输入正确的&quot; + tip + &quot;！&quot;); } public static void main(String[] args) { // 代码生成器 AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(&quot;user.dir&quot;); gc.setOutputDir(projectPath + &quot;/src/main/java&quot;); gc.setAuthor(&quot;eyiadmin&quot;); gc.setOpen(false); // gc.setSwagger2(true); 实体属性 Swagger2 注解 mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(&quot;jdbc:mysql://localhost:3306/nacos_config?useUnicode=true&amp;useSSL=false&amp;characterEncoding=utf8&quot;); // dsc.setSchemaName(&quot;public&quot;); dsc.setDriverName(&quot;org.mariadb.jdbc.Driver&quot;); dsc.setUsername(&quot;root&quot;); dsc.setPassword(&quot;root&quot;); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(&quot;模块名&quot;)); pc.setParent(&quot;com.eyiadmin.demo&quot;); mpg.setPackageInfo(pc); String templatePath = &quot;/templates/mapper.xml.vm&quot;; // 自定义输出配置 List&lt;FileOutConfig&gt; focList = new ArrayList&lt;&gt;(); // 自定义配置会被优先输出 focList.add(new FileOutConfig(templatePath) { @Override public String outputFile(TableInfo tableInfo) { // 自定义输出文件名 ， 如果你 Entity 设置了前后缀、此处注意 xml 的名称会跟着发生变化！！ return projectPath + &quot;/src/main/resources/mapper/&quot; + pc.getModuleName() + &quot;/&quot; + tableInfo.getEntityName() + &quot;Mapper&quot; + StringPool.DOT_XML; } }); mpg.execute(); }} 我用的官网提供的默认方式,运行后会生成对应的controller、entity、mapper、service代码。也支持自定义模版https://mp.baomidou.com/guide/generator.html Knife4j的使用这里我引入了Knife4j包: 123456&lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger2 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt; &lt;artifactId&gt;knife4j-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;/dependency&gt; 新建一个SwaggerConfiguration类配置我们的Swagger: 12345678910111213141516171819202122232425@Configuration@EnableSwagger2@EnableKnife4j@Import(BeanValidatorPluginsConfiguration.class)public class SwaggerConfiguration { @Bean(&quot;createRestApi&quot;) public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo( new ApiInfoBuilder() //页面标题 .title(&quot;Demo Web Api文档&quot;) //创建人 .contact(new Contact(&quot;eyiadmin&quot;, &quot;https://springfox.github.io/springfox/&quot;, &quot;eyiadmin@163.com&quot;)) .version(&quot;1.0&quot;) .description(&quot;Demo Web Api文档&quot;) .build()) .select() //API接口所在的包位置 .apis(RequestHandlerSelectors.basePackage(&quot;com.eyiadmin.demo.controller&quot;)) .paths(PathSelectors.any()) .build(); }} 新建一个名为UserController的Controller: 1234567891011121314@RequestMapping(&quot;/v1/user&quot;)@RestController@Api(tags = &quot;User API展示&quot;)public class UserController { @Autowired UserMapper userMapper; @GetMapping(&quot;/list&quot;) public ResponseResult&lt;?&gt; getUserList() { return ResponseResult.success(userMapper.selectList(null)); }} 启动起来，访问localhost:8080/doc.html:调用我们的接口，可以看到Druid监控到了我们SQL语句的执行情况过于Swagger也可以看看我之前的一篇文章Spring Boot集成Swagger 若有不足之处还望指正，多谢。欢迎感兴趣的朋友与我多多交流 参考MyBatis-Plushttps://github.com/alibaba/druid/tree/master/druid-spring-boot-starterknife4j","link":"/posts/1287150620.html"},{"title":"Spring Boot基于javax.validation进行参数校验","text":"在我们使用Spring Boot开发Web Api的时候，肯定会少不了参数校验，一般情况下，大部分是使用if来判断参数是否合法，在参数较少的情况下，这样做除了少量的重复工作外，也没有较大的工作量。但是当遇到一个复杂的参数的时候，可能整个方法大部分的代码都是if。工作量大不说，代码也精简。那么有没有更好的方式来处理呢？这就是我们这里要说到的参数校验技巧。 为了体现说明，我们事先创建好一个StudentVO类和一个StudentController.在StudentVO中加入几个简单的属性，仅做展示： 1234567891011121314151617import io.swagger.annotations.ApiModel;import io.swagger.annotations.ApiModelProperty;import lombok.Data;@ApiModel(&quot;StudentVO&quot;)@Datapublic class StudentVO { @ApiModelProperty(&quot;学生名字&quot;) private String name; @ApiModelProperty(&quot;学生年龄&quot;) private Integer age; @ApiModelProperty(&quot;学生住址&quot;) private String address;} 这里@ApiModelProperty和@ApiModel注解是我用了Swagger,你们可以直接写注释即可。为了减少Getter、Setter的工作量，我们引入了lombok工具包: 1234567&lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 这里需要注意的是，需要给IDE安装插件，因为我是用的IDEA所以插件安装很方便：Settings-&gt;Plugins,搜索lombok插件点击Install即可，安装完成后有个Restart IDE的按钮,点击重启即可。接下来，我们来进入示例环节,开始编码: 12345678910111213141516@RequestMapping(&quot;/v1/student&quot;)@RestController@Api(value = &quot;Student API展示&quot;)public class StudentController { @PostMapping(&quot;/CreateStudent&quot;) public ResponseEntity&lt;String&gt; CreateStudent(StudentVO studentVO) { if(StringUtils.isEmpty(studentVO.getName())) return new ResponseEntity&lt;&gt;(&quot;学生名字不能为空&quot;, HttpStatus.OK); if(studentVO.getAge()&gt;=5&amp;&amp;studentVO.getAge()&lt;=30) return new ResponseEntity&lt;&gt;(&quot;学生年龄不能大于30岁或者小于5岁&quot;, HttpStatus.OK); if(StringUtils.isEmpty(studentVO.getName())) return new ResponseEntity&lt;&gt;(&quot;学生住址不能为空&quot;, HttpStatus.OK); return new ResponseEntity&lt;&gt;(&quot;Success&quot;, HttpStatus.OK); }} 可以看到，目前这个vo类只有三个属性，在我们的action中也写了许多的用于校验的if,我们来看一下效果:当我们的属性有几十个的时候，还继续使用if来检验参数，确实是一件比较痛苦的事情，成了完全的手工活，而且还有可能会出错，为了提高我们的开发效率，自然得让一个工具来解决这个参数校验的问题，在我使用的Spring Boot2.2.1中已经为我们引入了javax.validation: 1234567&lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt; &lt;version&gt;${javax-validation.version}&lt;/version&gt; &lt;/dependency&gt;&lt;javax-validation.version&gt;2.0.1.Final&lt;/javax-validation.version&gt; 那么我们就可以直接使用validation-api,在使用之前，我们需要先熟悉几个注解: @AssertFalse 用于boolean字段，只能为true @AssertFalse 用于boolean字段，只能为false @DecimalMax（value=x）验证注解的元素值小于等于指定的value值 @DecimalMin（value=x）验证注解的元素值大于等于指定的value值 @Digits(integer=整数位数, fraction=小数位数) 验证注解的元素值必须是数值 @Email 验证注解的元素值是电子邮件地址 @Future(integer=整数位数, fraction=小数位数) 验证注解的元素值（日期类型）比当前时间晚 @FutureOrPresent(integer=整数位数, fraction=小数位数)验证注解的元素值（日期类型）比当前时间晚或者等于当前时间 @Past 验证注解的元素值（日期类型）比当前时间早 @ScriptAssert(lang= ,script=, alias=) @URL(protocol=,host=, port=,regexp=, flags=) URL验证 @PastOrPresent验证注解的元素值（日期类型）比当前时间早或等于现在 @Max（value=x）验证注解的元素值小于等于指定的value值 @Mix（value=x）验证注解的元素值大于等于指定的value值 @NotBlank 验证注解的元素值不为空（不为null、去除首位空格后长度为0），不同于@NotEmpty，@NotBlank只应用于字符串且在比较时会去除字符串的空格 @NotEmpty 验证注解的元素值不为null且不为空（字符串长度不为0、集合大小不为0） @NotNull 验证注解的元素值不是null @CreditCardNumber 信用卡验证 @Null 验证注解的元素值是null @Pattern(regex=正则表达式, flag=) 验证注解的元素值与指定的正则表达式匹配 @Size(min=最小值, max=最大值) 验证注解的元素值的在min和max（包含）指定区间之内，如字符长度、集合大小我们就可以参照这些注解来校验参数:12345678910111213141516@ApiModel(&quot;StudentVO&quot;)@Datapublic class StudentVO { @NotEmpty(message =&quot;学生名字不能为空&quot;) @ApiModelProperty(&quot;学生名字&quot;) private String name; @Range(min = 5, max = 30, message = &quot;学生年龄不能大于30岁或者小于5岁&quot;) @ApiModelProperty(&quot;学生年龄&quot;) private Integer age; @NotEmpty(message =&quot;学生住址不能为空&quot;) @ApiModelProperty(&quot;学生住址&quot;) private String address;} 修改StudentController:12345678910111213141516171819@RequestMapping(&quot;/v1/student&quot;)@RestController@Api(value = &quot;Student API展示&quot;)public class StudentController { @PostMapping(&quot;/CreateStudent&quot;) public ResponseEntity&lt;String&gt; CreateStudent(@Valid StudentVO studentVO, BindingResult validResult) { if(validResult.hasErrors()) return new ResponseEntity&lt;&gt;(validResult.getAllErrors().get(0).getDefaultMessage(), HttpStatus.OK);// if(StringUtils.isEmpty(studentVO.getName()))// return new ResponseEntity&lt;&gt;(&quot;学生名字不能为空&quot;, HttpStatus.OK);// if(studentVO.getAge()&gt;=5&amp;&amp;studentVO.getAge()&lt;=30)// return new ResponseEntity&lt;&gt;(&quot;学生年龄不能大于30岁或者小于5岁&quot;, HttpStatus.OK);// if(StringUtils.isEmpty(studentVO.getName()))// return new ResponseEntity&lt;&gt;(&quot;学生住址不能为空&quot;, HttpStatus.OK); return new ResponseEntity&lt;&gt;(&quot;Success&quot;, HttpStatus.OK); }} 最后效果","link":"/posts/605205503.html"},{"title":"Vue制作静态页面简历并发布到七牛云上","text":"一般情况下，我们使用word可以写出漂亮的简历。但是我们是程序员，就应该用程序员的方式来做–使用网页写简历。当然，网页简历的方式就很多了，这里我们就用vue来搞定这个事，因为它灵活、高效、方便。 Vue插件和UI框架比较丰富，我们这里就借助elementui来帮助我们快速完成网页.大家可以先去Element官方查看相应的文档。 ElementUI快速上手安装ElementUI和相应的插件 12cnpm i element-ui -Scnpm install babel-plugin-component -D main.js文件引入ElementUI: 123456789101112131415import Vue from &apos;vue&apos;import App from &apos;./App&apos;import router from &apos;./router&apos;import ElementUI from &apos;element-ui&apos;;import &apos;element-ui/lib/theme-chalk/index.css&apos;;Vue.use(ElementUI); # 完整引入Vue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: &apos;#app&apos;, router, components: { App }, template: &apos;&lt;App/&gt;&apos;}) 在.babelrc配置babel-plugin-component，我们借助 babel-plugin-component来实现按需引入，以达到减小项目体积的目的 12345678910111213141516171819{ &quot;presets&quot;: [ [&quot;env&quot;, { &quot;modules&quot;: false, &quot;targets&quot;: { &quot;browsers&quot;: [&quot;&gt; 1%&quot;, &quot;last 2 versions&quot;, &quot;not ie &lt;= 8&quot;] } }], &quot;stage-2&quot; ], &quot;plugins&quot;: [&quot;transform-vue-jsx&quot;, &quot;transform-runtime&quot;, [ &quot;component&quot;, { &quot;libraryName&quot;: &quot;element-ui&quot;, &quot;styleLibraryName&quot;: &quot;theme-chalk&quot; } ]]} 現在我們就可以註冊我們需要的組件， 123456789101112131415import Vue from &apos;vue&apos;;import { Button, Select } from &apos;element-ui&apos;;import App from &apos;./App.vue&apos;;Vue.component(Button.name, Button);Vue.component(Select.name, Select);/* 或写为 * Vue.use(Button) * Vue.use(Select) */new Vue({ el: &apos;#app&apos;, render: h =&gt; h(App)}); 我新建了几个组件， 结构如下：将我们的组件注册到router中: 1234567891011121314151617181920212223242526272829import About from &apos;@/components/about/About&apos;import Skill from &apos;@/components/skill/Index&apos;import Index from &apos;@/components/index/Index&apos;import His from &apos;@/components/history/Index&apos;Vue.use(Router)export default new Router({ routes: [ { path: &apos;/&apos;, name: &apos;Index&apos;, component: Index }, { path:&apos;/about&apos;, name:&apos;About&apos;, component:About },{ path:&apos;/skill&apos;, name:&apos;skill&apos;, component:Skill },{ path:&apos;/his&apos;, name:&apos;his&apos;, component:His } ]}) 在首页用到了Element 的layout布局，App.vue做了如下修改: 123456789101112131415161718192021222324252627&lt;template&gt; &lt;div id=&quot;app&quot;&gt; &lt;el-container&gt; &lt;el-header&gt; &lt;Header&gt;&lt;/Header&gt; &lt;/el-header&gt; &lt;el-main&gt; &lt;router-view /&gt; &lt;/el-main&gt; &lt;el-footer&gt; &lt;Footer&gt;&lt;/Footer&gt; &lt;/el-footer&gt; &lt;/el-container&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;import Footer from &quot;@/components/footer/Footer&quot;;import Header from &quot;@/components/header/Header&quot;;export default { components: { Footer, Header }, name: &quot;App&quot;};&lt;/script&gt; 在Header.vue中用到了el-menu： 1234567891011121314151617181920212223&lt;template&gt; &lt;el-menu :default-active=&quot;activeIndex&quot; class=&quot;el-menu-demo&quot; :router=&quot;true&quot; mode=&quot;horizontal&quot; &gt; &lt;el-menu-item index=&quot;/&quot;&gt;首页&lt;/el-menu-item&gt; &lt;el-menu-item index=&quot;/about&quot;&gt;个人简介&lt;/el-menu-item&gt; &lt;el-menu-item index=&quot;/skill&quot;&gt;个人技能&lt;/el-menu-item&gt; &lt;el-menu-item index=&quot;/his&quot;&gt;工作经历&lt;/el-menu-item&gt; &lt;/el-menu&gt;&lt;/template&gt;&lt;script&gt;export default { name: &apos;Header&apos;, data () { return { activeIndex: &apos;/&apos; } }}&lt;/script&gt; 这里的组件代码来源于Vue+Element实现网页版个人简历系统.接下来编译成静态页面: 1npm run build 我之前有把博客上传到七牛云的文章，大家可以去看看。这里我只简单的走个流程: 1qshell qupload2 --overwrite=true --rescan-local=true --src-dir=E:\\vue\\myresume\\dist --bucket=eyiadminresume 之前也有说到将命令写入到package.json中: 1&quot;build&quot;: &quot;node build/build.js &amp; qshell qupload2 --overwrite=true --rescan-local=true --src-dir=E:/vue/myresume/dist --bucket=eyiadminresume&quot; 这样在执行npm run build的时候，会先执行node build/build.js然后会执行qshell qupload2上传到七牛云.关于qshell的详细教程请访问https://github.com/qiniu/qshell。上传完成后，我们就需要解析到我们的域名上resume.52fx.biz个人简历我会不定期更新，如有疑问可以加我QQ详聊188781475","link":"/posts/3897799686.html"},{"title":"Spring Boot整合Dubbo和Nacos","text":"目前适用于Spring Cloud服务注册中心的解决方案比较多,诸如：Consul、Spring Cloud Eureka、Zookeeper、Etcd等，配置中心有：Spring Cloud Config、Apollo、Disconf等。总感觉这些搭建起来较为繁琐，所以考虑使用Nacos来做配置和服务注册中心。 简介Apache Dubbo 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。Dubbo的注册中心有多种:Multicast、zookeeper、Nacos、Redis、Simple，官方推荐使用Zookeeper作为注册中心，不过在我之前搭建大数据的时候，总感觉Zookeeper太重了，觉得Nacos才是我想要的。 Nacos是一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。它具有:动态配置服务、服务发现及管理、动态DNS服务等功能。这里我用到的环境为: Dubbo 2.7.5 Nacos 1.1.4 MariaDB 10.3.10 Windows 10 IntelliJ IDEA 2019.3.1 64 bit JDK 1.8.0_231 Spring Boot 2.2.3.RELEASE 部署Nacos开发环境数据库安装在这里就不展开细说，大家自行探索。首先需要去https://github.com/alibaba/nacos/releases下载Nacos,解压之后目录结构如下： 12345678910111213141516171819│ LICENSE│ NOTICE│├─bin│ shutdown.cmd│ shutdown.sh│ startup.cmd│ startup.sh│├─conf│ application.properties│ application.properties.example│ cluster.conf.example│ nacos-logback.xml│ nacos-mysql.sql│ schema.sql│└─target nacos-server.jar 先初始化Nacos数据库,这里我创建了一个名为nacos-config的数据库并导入nacos-mysql.sql的数据库脚本. 修改conf/application.properties文件,添加mysql数据源的url、用户名和密码。1234db.num=1db.url.0=jdbc:mysql://localhost:3306/nacos_config?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=truedb.user=rootdb.password=root 我们进入到bin目录,双击startup.cmd文件启动Nacos服务：12345678910111213141516171819202122232425262728293031323334 ,--. ,--.&apos;| ,--,: : | Nacos 1.1.4,`--.&apos;`| &apos; : ,---. Running in stand alone mode, All function modules| : : | | &apos; ,&apos;\\ .--.--. Port: 8848: | \\ | : ,--.--. ,---. / / | / / &apos; Pid: 24724| : &apos; &apos;; | / \\ / \\. ; ,. :| : /`./ Console: http://192.168.3.27:8848/nacos/index.html&apos; &apos; ;. ;.--. .-. | / / &apos;&apos; | |: :| : ;_| | | \\ | \\__\\/: . .. &apos; / &apos; | .; : \\ \\ `. https://nacos.io&apos; : | ; .&apos; ,&quot; .--.; |&apos; ; :__| : | `----. \\| | &apos;`--&apos; / / ,. |&apos; | &apos;.&apos;|\\ \\ / / /`--&apos; /&apos; : | ; : .&apos; \\ : : `----&apos; &apos;--&apos;. /; |.&apos; | , .-./\\ \\ / `--&apos;---&apos;&apos;---&apos; `--`---&apos; `----&apos;此处省略xxx行2020-01-18 11:17:19,063 INFO Exposing 2 endpoint(s) beneath base path &apos;/actuator&apos;2020-01-18 11:17:19,104 INFO Initializing ExecutorService &apos;taskScheduler&apos;2020-01-18 11:17:19,297 INFO Tomcat started on port(s): 8848 (http) with context path &apos;/nacos&apos;2020-01-18 11:17:19,304 INFO Nacos Log files: E:\\tool\\nacos-server-1.1.4\\nacos/logs/2020-01-18 11:17:19,304 INFO Nacos Conf files: E:\\tool\\nacos-server-1.1.4\\nacos/conf/2020-01-18 11:17:19,305 INFO Nacos Data files: E:\\tool\\nacos-server-1.1.4\\nacos/data/2020-01-18 11:17:19,306 INFO Nacos started successfully in stand alone mode.2020-01-18 11:17:19,812 INFO Initializing Servlet &apos;dispatcherServlet&apos;2020-01-18 11:17:19,823 INFO Completed initialization in 11 ms 由于只是开发环境，所以就先搭建这个单机模式版本，现在我们访问Nacos服务:http://192.168.3.27:8848/nacos/index.html默认用户名和密码为:nacos/nacos IDEA创建多模块项目新建一个Maven项目步骤:Create New Project-&gt;选择Maven点击Next-&gt;输入项目名称、选择目录存放地址等,点击Finish,创建完成后,我们需要删除src目录并且在pom添加&lt;packaging&gt;pom&lt;/packaging&gt;,如下： 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.nacos&lt;/groupId&gt; &lt;artifactId&gt;dubbo-nacos&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt;&lt;/project&gt; 创建Provider模块在项目上点击右键，New-&gt;Module输入项目信息，点击Next-&gt;Finish 创建Consumer模块在项目上点击右键，New-&gt;Module输入项目信息，点击Next-&gt;Finish 引入相关jar包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;properties&gt; &lt;spring-boot.version&gt;2.2.3.RELEASE&lt;/spring-boot.version&gt; &lt;dubbo.version&gt;2.7.5&lt;/dubbo.version&gt; &lt;nacos.version&gt;1.1.4&lt;/nacos.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Spring Boot --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-boot.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Apache Dubbo --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-dependencies-bom&lt;/artifactId&gt; &lt;version&gt;${dubbo.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Dubbo Spring Boot Starter --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Dubbo Registry Nacos --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-registry-nacos&lt;/artifactId&gt; &lt;version&gt;${dubbo.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt; &lt;artifactId&gt;nacos-client&lt;/artifactId&gt; &lt;version&gt;${nacos.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Dubbo&amp;Nacos注册中心我们首先在Provider创建一个服务并注册到nacos中. 配置application:12345678910111213spring.application.name=dubbo-nacos-provider# Base packages to scan Dubbo Component: @org.apache.dubbo.config.annotation.Servicedubbo.scan.base-packages=com.nacos.provider.service# Dubbo Protocoldubbo.protocol.name=dubbo## Random portdubbo.protocol.port=8888## Dubbo Registrynacos.server-address = 127.0.0.1nacos.port = 8848dubbo.registry.address=nacos://${nacos.server-address}:${nacos.port} 创建一个DemoService接口12345public interface DemoService { String sayHello(String name);} 创建实现类并引入dubbo提供的@Service注解：12345678910@Service(version = &quot;1.0.0&quot;)public class DemoServiceImpl implements DemoService { @Value(&quot;${dubbo.application.name}&quot;) private String serviceName; public String sayHello(String name) { return String.format(&quot;[%s] : Hello, %s&quot;, serviceName, name); }} 把我们的Provider运行起来(请勿关闭，供下面consumer程序调用),可以看到服务已经注册到Nacos中 现在我们来实现对服务的消费.首先当然还是我们的application的配置: 1234567spring.application.name=dubbo-nacos-consumer## Dubbo Registrynacos.server-address = 127.0.0.1nacos.port = 8848dubbo.registry.address=nacos://${nacos.server-address}:${nacos.port} 新建了一个Controller： 12345678910111213@RequestMapping(&quot;/demo&quot;)@RestControllerpublic class DemoController { @Reference(version = &quot;1.0.0&quot;) private DemoService demoService; @GetMapping(&quot;hello/{name}&quot;) public ResponseEntity&lt;String&gt; sayHello(@PathVariable String name) { return new ResponseEntity&lt;String&gt;(demoService.sayHello(name),HttpStatus.OK); }} 并引入Provier用到的接口DemoService。启动起来，调用接口: Nacos作为配置中心如果要使用Nacos作为我们的配置中心的话，这里还需要引入一个包: 123456&lt;!-- https://mvnrepository.com/artifact/com.alibaba.boot/nacos-config-spring-boot-starter --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;nacos-config-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.4&lt;/version&gt;&lt;/dependency&gt; 并在配置文件中加入nacos.config.server-addr的配置: 1nacos.config.server-addr=127.0.0.1:8848 # 根据实际部署情况修改 然后，我们进入到Nacos控制台,新建一个配置名为dubbo-nacos-config,如下:配置的相关说明, 1234Data ID：com.alibaba.nacos.example.propertiesGroup：不填写，即使用默认的 DEFAULT_GROUP。配置格式：Properties配置内容：connectTimeoutInMills=3000 在Spring Boot启动类加上@NacosPropertySource(dataId = &quot;dubbo-nacos-config&quot;, autoRefreshed = true),autoRefreshed表示自动刷新这里我们在需要用到配置的类中加入如下代码: 12@NacosValue(value = &quot;${service.name:default}&quot;, autoRefreshed = true)private String serviceName @NacosValue和@Value的用法差不多。然后在controller添加了一个方法: 12345@GetMapping(&quot;version&quot;) public ResponseEntity&lt;String&gt; getNacosConfig() { return new ResponseEntity&lt;String&gt;(nacosConfig.getServiceName(),HttpStatus.OK); } 启动程序，访问http://localhost:8080/demo/version,可以看到我们获取到了nacos中的配置数据:我在nacos中修改配置文件的值: 1service.name=demo 直接刷新页面可以发现我们的值自动改变了, [参考]https://github.com/apache/dubbo-spring-boot-project/tree/master/dubbo-spring-boot-sampleshttps://nacos.io/en-us/docs/quick-start-spring-boot.htmlhttp://dubbo.apache.org/en-us/docs/user/quick-start.html 有问题一定要多看官方文档和Github","link":"/posts/619628292.html"},{"title":"Spring Boot 自定义Validation注解","text":"在项目开发过程中，注解给我们带来方便、快捷的极简编程体验，前面也有记录了一些常用的注解命令，可见注解是比较常用且重要的功能，有时候validation自带的注解可能不能满足我们的个性化需要时，我们能不能根据自己的情况自定义注解命令呢? 源码浅析我们先来看看常用的@Min注解，学习一下它是如何实现的，先看源码: 12345678910111213141516171819202122232425262728293031@Target({ METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE })@Retention(RUNTIME)@Repeatable(List.class)@Documented@Constraint(validatedBy = { })public @interface Min { String message() default &quot;{javax.validation.constraints.Min.message}&quot;; Class&lt;?&gt;[] groups() default { }; Class&lt;? extends Payload&gt;[] payload() default { }; /** * @return value the element must be higher or equal to */ long value(); /** * Defines several {@link Min} annotations on the same element. * * @see Min */ @Target({ METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE }) @Retention(RUNTIME) @Documented @interface List { Min[] value(); }} @Target 注解的作用范围，参数是一个枚举类型: 123456789101112131415161718192021222324252627282930313233343536373839 public enum ElementType { /** 类、接口或者是枚举 */ TYPE, /** 字段属性 */ FIELD, /** 方法 */ METHOD, /** 参数 */ PARAMETER, /** 构造方法 */ CONSTRUCTOR, /** 局部变量 */ LOCAL_VARIABLE, /** 注解类型 如：@interface修饰的类型 */ ANNOTATION_TYPE, /** 包 */ PACKAGE, /** * Type parameter declaration * * @since 1.8 */ TYPE_PARAMETER, /** * Use of a type * * @since 1.8 */ TYPE_USE} @Retention用于声明注解的声明周期，也是一个枚举类型： 123456789101112131415161718192021 public enum RetentionPolicy { /** * Annotations are to be discarded by the compiler. 注解是保留在源代码中,编译时丢弃 */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior.代码保留在class文件中，加载到JVM虚拟机时丢弃 */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * 保留在程序运行期间，此时可以通过反射获得定义在某个类上的所有注解 * @see java.lang.reflect.AnnotatedElement */ RUNTIME} @Repeatable注解表明标记的注解可以多次应用于相同的声明或类型 @Documented注解会被javadoc之类的工具处理, 所以注解类型信息也会被包括在生成的文档中.默认情况下javadoc是不包括注解的 @Constraint注解限定自定义注解的方法，指定自定义的处理类。 自定义注解刚才简单介绍了一下Spring给我们带来的注解的定义方式，接下来就依样画葫芦来实现自定义Validation注解。首先新建一个Phone注解: 123456789101112@Target({FIELD })@Retention(RUNTIME)@Documented@Constraint(validatedBy = {PhoneConstraintValidator.class})public @interface Phone { String message() default &quot;手机号错误&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {};} 再创建一个Validator校验类,用于Phone @Constraint指定的处理校验类: 123456789101112131415161718public class PhoneConstraintValidator implements ConstraintValidator&lt;Phone,String&gt; { @Override public boolean isValid(String value, ConstraintValidatorContext context) { return isMobile(value); } private boolean isMobile(String source) { Pattern pattern = null; Matcher matcher = null; String reg=&quot;^[1](([3|5|8][\\\\d])|([4][4,5,6,7,8,9])|([6][2,5,6,7])|([7][^9])|([9][1,8,9]))[\\\\d]{8}$&quot;;// 验证手机号 if(!StringUtils.isEmpty(source)){ pattern = Pattern.compile(reg); matcher = pattern.matcher(source); return matcher.matches(); } return false; }} 现在就可以在使用我们自定义的注解: 12345678910111213141516171819202122@ApiModel(&quot;StudentVO&quot;)@Datapublic class StudentVO { @NotEmpty(message =&quot;学生名字不能为空&quot;) @ApiModelProperty(&quot;学生名字&quot;) private String name; @Range(min = 5, max = 30, message = &quot;学生年龄不能大于30岁或者小于5岁&quot;) @ApiModelProperty(&quot;学生年龄&quot;) private Integer age; @NotEmpty(message =&quot;学生住址不能为空&quot;) @ApiModelProperty(&quot;学生住址&quot;) private String address; @NotBlank(message = &quot;手机号必填&quot;) @Phone(message = &quot;号码不正确&quot;) @ApiModelProperty(&quot;手机号&quot;) private String PhoneNo;} 自定义Validation就暂时告一段落，后面，我们再来总结一下其他实用注解的定义方法。","link":"/posts/666989412.html"},{"title":"Vue入门","text":"现在前端比较火的三大家就Vue、React、Angular，当然现在逐渐兴趣和完善的Fullter,现在也有一些大厂开始用Fullter来开发应用了。但是我还是希望等它再成熟一点之后再开始入手，这样可以少采一些坑嘛。这次，我们先来总结一下Vue的一些基础知识，为什么要学Vue呢，因为它易用、灵活、高效. 环境准备我们首先需要安装Node.js,去Nodejs官网下载,安装很方便直接下一步即可:验证是否安装成功: 1234C:\\Users\\lenovo&gt;node -vv13.5.0C:\\Users\\lenovo&gt;npm -v6.13.4 使用npm安装vue-cli脚手架构建工具: 12345678C:\\Users\\lenovo&gt;npm install -g vue-clinpm WARN deprecated vue-cli@2.9.6: This package has been deprecated in favour of @vue/clinpm WARN deprecated coffee-script@1.12.7: CoffeeScript on NPM has moved to &quot;coffeescript&quot; (no hyphen)C:\\Users\\lenovo\\AppData\\Roaming\\npm\\vue -&gt; C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\vue-cli\\bin\\vueC:\\Users\\lenovo\\AppData\\Roaming\\npm\\vue-init -&gt; C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\vue-cli\\bin\\vue-initC:\\Users\\lenovo\\AppData\\Roaming\\npm\\vue-list -&gt; C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\vue-cli\\bin\\vue-list+ vue-cli@2.9.6added 4 packages from 1 contributor, removed 1022 packages and updated 237 packages in 162.365s 当然，npm会因为网络问题，会下载很慢甚至导致失败。这里可以使用cnpm或者nvm来更改镜像地址。为了方便，我们还是选择cnpm吧,在cmd中安装cnpm: 1234 C:\\Users\\lenovo&gt; npm install -g cnpm --registry=http://registry.npm.taobao.orgC:\\Users\\lenovo\\AppData\\Roaming\\npm\\cnpm -&gt; C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\cnpm\\bin\\cnpm+ cnpm@6.1.1added 31 packages from 27 contributors, removed 29 packages, updated 119 packages and moved 1 package in 130.257s 12345678910C:\\Users\\lenovo&gt;cnpm -vcnpm@6.1.1 (C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\cnpm\\lib\\parse_argv.js)npm@6.13.4 (C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\cnpm\\node_modules\\npm\\lib\\npm.js)node@13.5.0 (D:\\Program Files\\nodejs\\node.exe)npminstall@3.25.2 (C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\cnpm\\node_modules\\npminstall\\lib\\index.js)prefix=C:\\Users\\lenovo\\AppData\\Roaming\\npmwin32 x64 10.0.18362registry=https://r.npm.taobao.orgC:\\Users\\lenovo&gt;vue -V2.9.6 创建项目我们先来看一下vue命令工具: 12345678910111213C:\\Users\\lenovo&gt;vue -helpUsage: vue &lt;command&gt; [options]Options: -V, --version output the version number -h, --help output usage informationCommands: init 从模板生成新项目 list 列出可用的模板 build prototype a new project create (for v3 warning only) help [cmd] display help for [cmd] 我这里创建一个名为myresume: 1234567891011121314151617181920212223242526E:\\vue&gt;vue init webpack myresume? Project name myresume? Project description A Vue.js project? Author eyiadmin &lt;326076105@qq.com&gt;? Vue build standalone? Install vue-router? Yes? Use ESLint to lint your code? Yes? Pick an ESLint preset Standard? Set up unit tests No? Setup e2e tests with Nightwatch? No? Should we run `npm install` for you after the project has been created? (recommended) no vue-cli · Generated &quot;myresume&quot;.# Project initialization finished!# ========================To get started: cd myresume npm install (or if using yarn: yarn) npm run lint -- --fix (or for yarn: yarn run lint --fix) npm run devDocumentation can be found at https://vuejs-templates.github.io/webpack 一般默认既可以，在最后的Should we runnpm installfor you after the project has been created，我选择No,在模版创建好后，自己手动安装。 123456789101112131415161718192021222324252627282930313233E:\\vue&gt;cd myresumeE:\\vue\\myresume&gt;cnpm install| [30/47] Installing url-loader@^0.5.8platform unsupported babel-loader@7.1.5 › webpack@3.12.0 › watchpack@1.6.0 › chokidar@2.1.8 › fsevents@^1.2.7 Package require os(darwin) not compatible with your platform(win32)| [30/47] Installing postcss-normalize-unicode@^4.0.1[fsevents@^1.2.7] optional install error: Package require os(darwin) not compatible with your platform(win32)√ Installed 47 packages√ Linked 805 latest versions[1/3] scripts.postinstall babel-loader@7.1.5 › webpack@3.12.0 › uglifyjs-webpack-plugin@^0.4.6 run &quot;node lib/post_install.js&quot;, root: &quot;E:\\\\vue\\\\myresume\\\\node_modules\\\\_uglifyjs-webpack-plugin@0.4.6@uglifyjs-webpack-plugin&quot;[1/3] scripts.postinstall babel-loader@7.1.5 › webpack@3.12.0 › uglifyjs-webpack-plugin@^0.4.6 finished in 556ms[2/3] scripts.postinstall babel-plugin-transform-runtime@6.23.0 › babel-runtime@6.26.0 › core-js@^2.4.0 run &quot;node -e \\&quot;try{require(&apos;./postinstall&apos;)}catch(e){}\\&quot;&quot;, root: &quot;E:\\\\vue\\\\myresume\\\\node_modules\\\\_core-js@2.6.11@core-js&quot;Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!The project needs your help! Please consider supporting of core-js on Open Collective or Patreon:&gt; https://opencollective.com/core-js&gt; https://www.patreon.com/zloirockAlso, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)[2/3] scripts.postinstall babel-plugin-transform-runtime@6.23.0 › babel-runtime@6.26.0 › core-js@^2.4.0 finished in 216ms[3/3] scripts.postinstall webpack-bundle-analyzer@2.13.1 › ejs@^2.5.7 run &quot;node ./postinstall.js&quot;, root: &quot;E:\\\\vue\\\\myresume\\\\node_modules\\\\_ejs@2.7.4@ejs&quot;Thank you for installing EJS: built with the Jake JavaScript build tool (https://jakejs.com/)[3/3] scripts.postinstall webpack-bundle-analyzer@2.13.1 › ejs@^2.5.7 finished in 136ms√ Run 3 scriptspeerDependencies link ajv@5.5.2 in E:\\vue\\myresume\\node_modules\\_ajv-keywords@2.1.1@ajv-keywords unmet with E:\\vue\\myresume\\node_modules\\ajv(6.10.2)deprecate autoprefixer@7.2.6 › browserslist@^2.11.3 Browserslist 2 could fail on reading Browserslist &gt;3.0 config used in other tools.deprecate css-loader@0.28.11 › cssnano@3.10.0 › autoprefixer@6.7.7 › browserslist@^1.7.6 Browserslist 2 could fail on reading Browserslist &gt;3.0 config used in other tools.deprecate eslint@4.19.1 › file-entry-cache@2.0.0 › flat-cache@1.3.4 › circular-json@^0.3.1 CircularJSON is in maintenance only, flatted is its successor.deprecate babel-plugin-transform-runtime@6.23.0 › babel-runtime@6.26.0 › core-js@^2.4.0 core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.deprecate webpack-bundle-analyzer@2.13.1 › bfj-node4@^5.2.0 Switch to the `bfj` package for fixes and new features!Recently updated (since 2019-12-24): 5 packages (detail see file E:\\vue\\myresume\\node_modules\\.recently_updates.txt) Today: → optimize-css-assets-webpack-plugin@3.2.1 › cssnano@4.1.10 › postcss@^7.0.0(7.0.26) (08:11:16)√ All packages installed (1005 packages installed from npm registry, used 56s(network 52s), speed 66.01kB/s, json 852(1.95MB), tarball 1.4MB) 安装完成后就可以初步看看我们的项目模版: 1234567891011E:\\vue\\myresume&gt;npm run dev&gt; myresume@1.0.0 dev E:\\vue\\myresume&gt; webpack-dev-server --inline --progress --config build/webpack.dev.conf.js 13% building modules 33/37 modules 4 active ...yresume\\src\\components\\HelloWorld.vue{ parser: &quot;babylon&quot; } is deprecated; we now treat it as { parser: &quot;babel&quot; }. 95% emitting DONE Compiled successfully in 3332ms 下午6:20:31 I Your application is running here: http://localhost:8080 在浏览器访问http://localhost:8080/#/ 基础介绍模板语法Vue.js 使用了基于 HTML 的模板语法，允许开发者声明式地将 DOM 绑定至底层 Vue 实例的数据。所有 Vue.js 的模板都是合法的 HTML ，所以能被遵循规范的浏览器和 HTML 解析器解析。在底层的实现上，Vue 将模板编译成虚拟 DOM 渲染函数。结合响应系统，Vue 能够智能地计算出最少需要重新渲染多少组件，并把 DOM 操作次数减到最少。 文本数据绑定最常见的形式就是使用Mustache语法 (双大括号) 的文本插值： 1&lt;span&gt;我会变: {{ msg }}&lt;/span&gt; Mustache标签将会被替代为对应数据对象上 msg 属性的值。无论何时，绑定的数据对象上 msg 属性发生了改变，插值处的内容都会更新。通过使用 v-once 指令，你也能执行一次性地插值，当数据改变时，插值处的内容不会更新。但请留心这会影响到该节点上的其它数据绑定： 1&lt;span v-once&gt;我不会改变: {{ msg }}&lt;/span&gt; 我们来看看这二者的效果: 原始 HTML双大括号会将数据解释为普通文本，而非 HTML 代码。为了输出真正的 HTML，你需要使用 v-html 指令： 123456789101112&lt;p&gt;Using mustaches: {{ rawHtml }}&lt;/p&gt;&lt;p&gt;Using v-html directive: &lt;span v-html=&quot;rawHtml&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;script&gt;export default { name: &apos;HelloWorld&apos;, data () { return { rawHtml:&apos;&lt;span&gt;html代码&lt;/span&gt;&apos; } }}&lt;/script&gt; 这个 span 的内容将会被替换成为属性值 rawHtml，直接作为 HTML——会忽略解析属性值中的数据绑定。注意，你不能使用 v-html 来复合局部模板，因为 Vue 不是基于字符串的模板引擎。反之，对于用户界面 (UI)，组件更适合作为可重用和可组合的基本单位。你的站点上动态渲染的任意 HTML 可能会非常危险，因为它很容易导致 XSS 攻击。请只对可信内容使用 HTML 插值，绝不要对用户提供的内容使用插值。 特性Mustache 语法不能作用在 HTML 特性上，遇到这种情况应该使用 v-bind 指令： 1&lt;div v-bind:id=&quot;dynamicId&quot;&gt;&lt;/div&gt; 对于布尔特性 (它们只要存在就意味着值为 true)，v-bind 工作起来略有不同，在这个例子中： 1&lt;button v-bind:disabled=&quot;isButtonDisabled&quot;&gt;Button&lt;/button&gt; 如果 isButtonDisabled 的值是 null、undefined 或 false，则 disabled 特性甚至不会被包含在渲染出来的 &lt;button&gt; 元素中。 使用 JavaScript 表达式迄今为止，在我们的模板中，我们一直都只绑定简单的属性键值。但实际上，对于所有的数据绑定，Vue.js 都提供了完全的 JavaScript 表达式支持。 1234567891011121314151617181920{{ number + 1 }}{{ ok ? &apos;YES&apos; : &apos;NO&apos; }}{{ message.split(&apos;&apos;).reverse().join(&apos;&apos;) }}&lt;div v-bind:id=&quot;&apos;list-&apos; + id&quot;&gt;&lt;/div&gt;&lt;script&gt;export default { name: &apos;HelloWorld&apos;, data () { return { id:1, ok:1, message:&apos;reverse&apos;, number:1, } }}&lt;/script&gt; 这些表达式会在所属 Vue 实例的数据作用域下作为 JavaScript 被解析。有个限制就是，每个绑定都只能包含单个表达式，所以下面的例子都不会生效。 12345&lt;!-- 这是语句，不是表达式 --&gt;{{ var a = 1 }}&lt;!-- 流控制也不会生效，请使用三元表达式 --&gt;{{ if (ok) { return message } }} 模板表达式都被放在沙盒中，只能访问全局变量的一个白名单，如 Math 和 Date 。你不应该在模板表达式中试图访问用户定义的全局变量。 指令指令 (Directives) 是带有 v- 前缀的特殊特性。指令特性的值预期是单个 JavaScript 表达式 (v-for 是例外情况，稍后我们再讨论)。指令的职责是，当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM。回顾我们在介绍中看到的例子： 1234567891011&lt;p v-if=&quot;seen&quot;&gt;现在你看到我了&lt;/p&gt;&lt;script&gt;export default { name: &apos;HelloWorld&apos;, data () { return { seen:true } }}&lt;/script&gt; 这里，v-if 指令将根据表达式 seen 的值的真假来插入/移除 &lt;p&gt; 元素。 参数一些指令能够接收一个“参数”，在指令名称之后以冒号表示。例如，v-bind 指令可以用于响应式地更新 HTML 特性： 1&lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt; 在这里 href 是参数，告知 v-bind 指令将该元素的 href 特性与表达式 url 的值绑定。 另一个例子是 v-on 指令，它用于监听 DOM 事件： 12345678910111213141516 &lt;a v-on:click=&quot;doSomething&quot;&gt;hello vue&lt;/a&gt; &lt;script&gt;export default { name: &apos;HelloWorld&apos;, data () { return { } }, methods:{ doSomething:function(){ alert(&quot;hello vue&quot;); } }}&lt;/script&gt; 在这里参数是监听的事件名。我们也会更详细地讨论事件处理。 动态参数2.6.0 新增 从 2.6.0 开始，可以用方括号括起来的 JavaScript 表达式作为一个指令的参数： 1234&lt;!--注意，参数表达式的写法存在一些约束，如之后的“对动态参数表达式的约束”章节所述。--&gt;&lt;a v-bind:属性名=&quot;url&quot;&gt; ... &lt;/a&gt; 这里的属性名 会被作为一个 JavaScript 表达式进行动态求值，求得的值将会作为最终的参数来使用。例如，如果你的 Vue 实例有一个 data 属性 属性名，其值为 “href”，那么这个绑定将等价于 v-bind:href。示例: 1234567891011&lt;a v-bind:href=&quot;url&quot;&gt; baidu &lt;/a&gt;export default { name: &apos;HelloWorld&apos;, data () { return { url:&quot;https://www.baidu.com&quot; } }, }&lt;/script&gt; 同样地，你可以使用动态参数为一个动态的事件名绑定处理函数： 1&lt;a v-on:事件名=&quot;doSomething&quot;&gt; ... &lt;/a&gt; 在这个示例中，当 事件名 的值为 “focus” 时，v-on:事件名 将等价于 v-on:focus。 对动态参数的值的约束动态参数预期会求出一个字符串，异常情况下值为 null。这个特殊的 null 值可以被显性地用于移除绑定。任何其它非字符串类型的值都将会触发一个警告。 对动态参数表达式的约束动态参数表达式有一些语法约束，因为某些字符，如空格和引号，放在 HTML attribute 名里是无效的。例如： 12&lt;!-- 这会触发一个编译警告 --&gt;&lt;a v-bind:[&apos;foo&apos; + bar]=&quot;value&quot;&gt; ... &lt;/a&gt; 变通的办法是使用没有空格或引号的表达式，或用计算属性替代这种复杂表达式。 在 DOM 中使用模板时 (直接在一个 HTML 文件里撰写模板)，还需要避免使用大写字符来命名键名，因为浏览器会把 attribute 名全部强制转为小写： 12345&lt;!--在 DOM 中使用模板时这段代码会被转换为 `v-bind:[someattr]`。除非在实例中有一个名为“someattr”的 property，否则代码不会工作。--&gt;&lt;a v-bind:[someAttr]=&quot;value&quot;&gt; ... &lt;/a&gt; 修饰符修饰符 (modifier) 是以半角句号 . 指明的特殊后缀，用于指出一个指令应该以特殊方式绑定。例如，.prevent 修饰符告诉 v-on 指令对于触发的事件调用 event.preventDefault()： 1&lt;form v-on:submit.prevent=&quot;onSubmit&quot;&gt;...&lt;/form&gt; 在接下来对 v-on 和 v-for 等功能的探索中，你会看到修饰符的其它例子。 缩写v- 前缀作为一种视觉提示，用来识别模板中 Vue 特定的特性。当你在使用 Vue.js 为现有标签添加动态行为 (dynamic behavior) 时，v- 前缀很有帮助，然而，对于一些频繁用到的指令来说，就会感到使用繁琐。同时，在构建由 Vue 管理所有模板的单页面应用程序 (SPA - single page application) 时，v- 前缀也变得没那么重要了。因此，Vue 为 v-bind 和 v-on 这两个最常用的指令，提供了特定简写： 1234567891011121314v-bind 缩写&lt;!-- 完整语法 --&gt;&lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt;&lt;!-- 缩写 --&gt;&lt;a :href=&quot;url&quot;&gt;...&lt;/a&gt;…v-on 缩写&lt;!-- 完整语法 --&gt;&lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt;&lt;!-- 缩写 --&gt;&lt;a @click=&quot;doSomething&quot;&gt;...&lt;/a&gt; 它们看起来可能与普通的 HTML 略有不同，但 : 与 @ 对于特性名来说都是合法字符，在所有支持 Vue 的浏览器都能被正确地解析。而且，它们不会出现在最终渲染的标记中。缩写语法是完全可选的。 Class 与 Style 绑定我们可以传给 v-bind:class 一个对象，以动态地切换 class： 1&lt;div v-bind:class=&quot;{ active: isActive }&quot;&gt;&lt;/div&gt; 上面的语法表示 active 这个 class 存在与否将取决于数据属性 isActive 的 truthiness. 你可以在对象中传入更多属性来动态切换多个 class。此外，v-bind:class 指令也可以与普通的 class 属性共存。当有如下模板: 1234&lt;div class=&quot;static&quot; v-bind:class=&quot;{ active: isActive, &apos;text-danger&apos;: hasError }&quot;&gt;&lt;/div&gt; 和如下 data： 1234data: { isActive: true, hasError: false} 结果渲染为： 1&lt;div class=&quot;static active&quot;&gt;&lt;/div&gt; 当 isActive 或者 hasError 变化时，class 列表将相应地更新。例如，如果 hasError 的值为 true，class 列表将变为 “static active text-danger”。 绑定的数据对象不必内联定义在模板里： 1234567&lt;div v-bind:class=&quot;classObject&quot;&gt;&lt;/div&gt;data: { classObject: { active: true, &apos;text-danger&apos;: false }} 渲染的结果和上面一样。我们也可以在这里绑定一个返回对象的计算属性。这是一个常用且强大的模式： 12345678910111213&lt;div v-bind:class=&quot;classObject&quot;&gt;&lt;/div&gt;data: { isActive: true, error: null},computed: { classObject: function () { return { active: this.isActive &amp;&amp; !this.error, &apos;text-danger&apos;: this.error &amp;&amp; this.error.type === &apos;fatal&apos; } }} 数组语法我们可以把一个数组传给 v-bind:class，以应用一个 class 列表： 12345&lt;div v-bind:class=&quot;[activeClass, errorClass]&quot;&gt;&lt;/div&gt;data: { activeClass: &apos;active&apos;, errorClass: &apos;text-danger&apos;} 渲染为： 1&lt;div class=&quot;active text-danger&quot;&gt;&lt;/div&gt; 如果你也想根据条件切换列表中的 class，可以用三元表达式： 1&lt;div v-bind:class=&quot;[isActive ? activeClass : &apos;&apos;, errorClass]&quot;&gt;&lt;/div&gt; 这样写将始终添加 errorClass，但是只有在 isActive 是 truthy[1] 时才添加 activeClass。 不过，当有多个条件 class 时这样写有些繁琐。所以在数组语法中也可以使用对象语法： 1&lt;div v-bind:class=&quot;[{ active: isActive }, errorClass]&quot;&gt;&lt;/div&gt; 条件渲染v-if 指令用于条件性地渲染一块内容。这块内容只会在指令的表达式返回 truthy 值的时候被渲染。 1&lt;h1 v-if=&quot;awesome&quot;&gt;Vue is awesome!&lt;/h1&gt; 也可以用 v-else 添加一个else 块： 12&lt;h1 v-if=&quot;awesome&quot;&gt;Vue is awesome!&lt;/h1&gt;&lt;h1 v-else&gt;Oh no 😢&lt;/h1&gt; 你可以使用 v-else 指令来表示v-if 的”else 块”： 123456&lt;div v-if=&quot;Math.random() &gt; 0.5&quot;&gt; Now you see me&lt;/div&gt;&lt;div v-else&gt; Now you don&apos;t&lt;/div&gt; v-else 元素必须紧跟在带 v-if或者 v-else-if(2.1.0 新增) 的元素的后面，否则它将不会被识别。v-else-if，顾名思义，充当 v-if 的”else-if 块”，可以连续使用： 123456789101112&lt;div v-if=&quot;type === &apos;A&apos;&quot;&gt; A&lt;/div&gt;&lt;div v-else-if=&quot;type === &apos;B&apos;&quot;&gt; B&lt;/div&gt;&lt;div v-else-if=&quot;type === &apos;C&apos;&quot;&gt; C&lt;/div&gt;&lt;div v-else&gt; Not A/B/C&lt;/div&gt; 类似于v-else，v-else-if 也必须紧跟在带 v-if 或者 v-else-if 的元素之后。 该问的内容是从https://cn.vuejs.org/v2/guide/拷贝来的，然后自己在验证一下。","link":"/posts/3749400616.html"},{"title":"Spring Boot统一异常处理","text":"异常可以说像大宝一样天天见，一不小心就可能写了一个bug.不过有异常不可怕，可怕是出了问题连异常的都没有，一般情况下，我们也都是try catch的方式在程序里面捕捉，但是这样有时候可能一疏忽，连try catch都忘了写，这时候如果出现问题，就比较尴尬了，所以，我们需要更加方便、优雅的全局异常处理的方式来优化我们的异常捕捉机制。 Spring Boot默认的异常处理方式这里，我们先看看Spring Boot在我们 程序出现异常的时候，是怎么给我们的提示的。首先，我们得制造一个bug，这是我们最擅长的： 1234567891011@RequestMapping(&quot;/exception&quot;)@RestController@Api(tags = &quot;制造bug&quot;)public class ExceptionController { @GetMapping(&quot;err&quot;) public String getErr(){ int calc= 1/0; return &quot;error&quot;; }} 如果通过丝袜哥来调用我们的接口的话，它已经很体贴的帮我们处理了一下:可是Spring Boot可就没有丝袜哥那么贴心了:这样一来，我们得告诉一下Spring Boot，让他也给我们带来贴心的服务，这时候就要提到两个注解:@RestControllerAdvice和@ExceptionHandler。 浅析注解我们先来看看@RestControllerAdvice和@ExceptionHandler的代码: 12345678910111213141516171819202122232425262728293031@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@ControllerAdvice@ResponseBodypublic @interface RestControllerAdvice { @AliasFor( annotation = ControllerAdvice.class ) String[] value() default {}; @AliasFor( annotation = ControllerAdvice.class ) String[] basePackages() default {}; @AliasFor( annotation = ControllerAdvice.class ) Class&lt;?&gt;[] basePackageClasses() default {}; @AliasFor( annotation = ControllerAdvice.class ) Class&lt;?&gt;[] assignableTypes() default {}; @AliasFor( annotation = ControllerAdvice.class ) Class&lt;? extends Annotation&gt;[] annotations() default {};} 可以看到@RestControllerAdvice注解继承了@ResponseBody和@ControllerAdvice注解， 1234567891011121314151617@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface ControllerAdvice { @AliasFor(&quot;basePackages&quot;) String[] value() default {}; @AliasFor(&quot;value&quot;) String[] basePackages() default {}; Class&lt;?&gt;[] basePackageClasses() default {}; Class&lt;?&gt;[] assignableTypes() default {}; Class&lt;? extends Annotation&gt;[] annotations() default {};} @ControllerAdvice注解又继承了@Component注解,那么在类上使用@RestControllerAdvice就会自动被扫描注册到容器中,并且输出json格式. 123456@Target({ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface ExceptionHandler { Class&lt;? extends Throwable&gt;[] value() default {};} @ExceptionHandler的参数是一个继承了Throwable的类,比如:Exception等，也可以自定义Exception类。 统一返回封装开发Web Api的时候，返回的数据格式肯定都是固定的，所以，这里需要一个统一的返回格式类: 123456789101112131415161718192021222324252627@Datapublic class ResponseResult&lt;T&gt; { //描述 private String msg; //状态码 private int code; //数据 private T data; public ResponseResult(int code, String msg, T data) { this.msg = msg; this.code = code; this.data = data; } public static ResponseResult&lt;String&gt; success() { return success(null); } public static &lt;T&gt; ResponseResult&lt;T&gt; success(T data) { return new ResponseResult&lt;T&gt;(200, &quot;请求成功&quot;, data); } public static &lt;T&gt; ResponseResult&lt;T&gt; error(String msg) { return new ResponseResult&lt;T&gt;(500, msg, null); }} 统一异常处理我们新建一个GlobalExceptionHandler类，用于处理全局异常. 123456789@RestControllerAdvicepublic class GlobalExceptionHandler { //捕捉所有Exception异常，这里可以捕捉自定义的业务异常 @ExceptionHandler(Exception.class) ResponseResult&lt;?&gt; handleException(Exception ex){ return ResponseResult.error(ex.getMessage()); }} 可以看到GlobalExceptionHandler已经为我们处理了Exception异常，在之前的参数校验中，我们也可以这里统一处理: 123456789@ExceptionHandler(BindException.class) public ResponseResult&lt;?&gt; handleValidationException(BindException ex) { BindingResult validResult = ex.getBindingResult(); if (validResult.hasErrors()) { return ResponseResult.error(validResult.getAllErrors().get(0).getDefaultMessage()); } else { return ResponseResult.error(&quot;操作失败&quot;); } } 使用统一处理后，我们的Controller就更加简洁了: 12345678910@RequestMapping(&quot;/v1/student&quot;)@RestController@Api(tags = &quot;Student API展示&quot;)public class StudentController { @PostMapping(&quot;/CreateStudent&quot;) public ResponseResult&lt;?&gt; CreateStudent(@Valid StudentVO studentVO) { return ResponseResult.success(); }} 小插曲如果在@ExceptionHandler(BindException.class)捕捉的过程中没有进入到这个异常处理方法中，那么可以debug查看@ExceptionHandler(Exception.class)捕捉到的是哪个Exception，再针对这个Exception进行异常处理。","link":"/posts/987248073.html"},{"title":"Vue自定义组件","text":"现在比较流行的就是组件化、模块化，因为这可以给我们带来快速和方便、提高开发效率。在React、Angular、Vue已经为自定义可复用组件提供了方便快捷的方式，这就是我们今天要说的内容-在Vue中如何自定义可复用组件。 简单示例全局定义注册首先，我们先在main.js定义一个简单的全局组件: 123Vue.component(&apos;MyComponent&apos;, { template: &apos;&lt;span&gt;自定义组件&lt;/span&gt;&apos;}) 局部注册定义我们直接在HelloWorld.vue定义我们的组件: 12345678910111213141516&lt;script&gt;let myComponent={ template: &apos;&lt;span&gt;自定义组件&lt;/span&gt;&apos;}export default { components:{ &quot;MyComponent&quot;:myComponent }, name: &apos;HelloWorld&apos;, data () { return { msg: &apos;Welcome to Your Vue.js App&apos; } }}&lt;/script&gt; 组件使用在HelloWorld.vue引入我们自定义的组件: 12345&lt;template&gt; &lt;div class=&quot;hello&quot;&gt; &lt;my-component&gt;&lt;/my-component&gt; &lt;/div&gt;&lt;/template&gt; 最后我们可以看到：这里组件的名字支持： 短横线,如:my-component,用法:&lt;my-component&gt;&lt;/my-component&gt; 小驼峰,如:myComponent,用法:&lt;my-component&gt;&lt;/my-component&gt; 大驼峰,如:MyComponent,用法:&lt;my-component&gt;&lt;/my-component&gt; 组件Prop属性既然自定组件，那么肯定会有一些属性字段需要动态绑定值，这里我们就要借助Prop来实现我们的组件属性。我們先來熟悉一下Prop屬性的用法: 數組類型,如:props: ['authorName','age']; 對象,對象方式支持指定屬性的值類型:1234props: { authorName: String, age:Number, } 對象方式支持屬性值驗證，如：1234props: { authorName: {type:String,required: true}, age:{type:Number,default: 100}, } 這裡的類型支持一下幾種: String Number Boolean Array Object Date Function Symbol HTML 中的特性名是大小写不敏感的，所以浏览器会把所有大写字符解释为小写字符。这意味着当你使用 DOM 中的模板时，camelCase (驼峰命名法) 的 prop 名需要使用其等价的 kebab-case (短横线分隔命名) 命名。 Prop示例我們將上面的組件稍作修改: 123456789let myComponent = { template: &quot;&lt;span&gt;自定义组件的作者:{{authorName}},年齡:{{age}}歲&lt;/span&gt;&quot;, props: { authorName: String, age:Number, }}; &lt;my-component author-name=&quot;eyiadmin&quot; age=&quot;1024&quot;&gt;&lt;/my-component&gt; 最終呈現:.我們剛才的值的靜態屬性值,如果我們想動態綁定屬性值怎麼做呢？直接上代碼: 1234567891011121314151617export default { components: { myComponent: myComponent }, name: &quot;HelloWorld&quot;, data() { return { msg: &quot;Welcome to Your Vue.js App&quot;, name:&apos;eyiadmin&apos;, age:1024 }; }};&lt;my-component v-bind:author-name=&quot;name&quot; v-bind:age=&quot;age&quot;&gt;&lt;/my-component&gt;或者這樣&lt;my-component :author-name=&quot;name&quot; :age=&quot;age&quot;&gt;&lt;/my-component&gt; 傳入對象我們把屬性值當做對象傳遞，這樣我們就需要做一些調整: 12345678910111213141516171819export default { components: { myComponent: myComponent }, name: &quot;HelloWorld&quot;, data() { return { msg: &quot;Welcome to Your Vue.js App&quot;, authorInfo: { authorName: &quot;eyiadmin&quot;, age: 1024 } }; }};# 指定對象屬性值&lt;my-component :author-name=&quot;authorInfo.authorName&quot; :age=&quot;authorInfo.age&quot;&gt;&lt;/my-component&gt;# 傳遞整個對象&lt;my-component v-bind=&quot;authorInfo&quot;&gt;&lt;/my-component&gt; 最終效果和上面一樣，就不再附效果圖。在Vue的Prop類型只支持单向数据流，即父级 prop 的更新会向下流动到子组件中，但是反过来则不行。但是我們會有場景用到子組件改變父組件的值，這個我們後面再說。 組件插槽Vue的slot有什麼用呢？剛才我們在自定義組件的時候，我們的使用方式是這樣的： 1&lt;my-component :author-name=&quot;authorInfo.authorName&quot; :age=&quot;authorInfo.age&quot;&gt;&lt;/my-component&gt; 但是我們不能這樣: 123&lt;my-component :author-name=&quot;authorInfo.authorName&quot; :age=&quot;authorInfo.age&quot;&gt;hello world&lt;/my-component&gt;或者&lt;my-component :author-name=&quot;authorInfo.authorName&quot; :age=&quot;authorInfo.age&quot;&gt;&lt;my-component :author-name=&quot;authorInfo.authorName&quot; :age=&quot;authorInfo.age&quot;&gt;&lt;/my-component&gt;&lt;/my-component&gt; 顯然，有時候我們是希望能不能不能這樣的變為可以這樣。這時候slot就可以發揮作用了。我們將我們的組件模版稍作調整: 1template: &quot;&lt;span&gt;自定义组件的作者:{{authorName}},年齡:{{age}}歲&lt;br/&gt;&lt;slot&gt;&lt;/slot&gt;&lt;/span&gt;&quot; 這下我們就可以這樣用： 1234567891011121314151617181920212223export default { components: { myComponent: myComponent }, name: &quot;HelloWorld&quot;, data() { return { msg: &quot;Welcome to Your Vue.js App&quot;, authorInfo: { authorName: &quot;eyiadmin&quot;, age:1024 }, childInfo: { authorName: &quot;eyiadmin_child&quot;, age:100 } }; }}; &lt;my-component v-bind=&quot;authorInfo&quot;&gt; &lt;my-component v-bind=&quot;childInfo&quot;&gt;&lt;/my-component&gt; &lt;/my-component&gt; 我就可以看到,我們再來簡單看看element的使用方式，就拿我們常用的header為例: 123456789101112131415161718&lt;template&gt; &lt;header class=&quot;el-header&quot; :style=&quot;{ height }&quot;&gt; &lt;slot&gt;&lt;/slot&gt; &lt;/header&gt;&lt;/template&gt;&lt;script&gt; export default { name: &apos;ElHeader&apos;, componentName: &apos;ElHeader&apos;, props: { height: { type: String, default: &apos;60px&apos; } } };&lt;/script&gt; 在Header的index.js中來註冊,這個註冊是按需引入的時候註冊。 12345678import Header from &apos;./src/main&apos;;/* istanbul ignore next */Header.install = function(Vue) { Vue.component(Header.name, Header);};export default Header; 我們可以在src/index.js看到全局註冊的方式: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288/* Automatically generated by &apos;./build/bin/build-entry.js&apos; */import Pagination from &apos;../packages/pagination/index.js&apos;;import Dialog from &apos;../packages/dialog/index.js&apos;;import Autocomplete from &apos;../packages/autocomplete/index.js&apos;;import Dropdown from &apos;../packages/dropdown/index.js&apos;;import DropdownMenu from &apos;../packages/dropdown-menu/index.js&apos;;import DropdownItem from &apos;../packages/dropdown-item/index.js&apos;;import Menu from &apos;../packages/menu/index.js&apos;;import Submenu from &apos;../packages/submenu/index.js&apos;;import MenuItem from &apos;../packages/menu-item/index.js&apos;;import MenuItemGroup from &apos;../packages/menu-item-group/index.js&apos;;import Input from &apos;../packages/input/index.js&apos;;import InputNumber from &apos;../packages/input-number/index.js&apos;;import Radio from &apos;../packages/radio/index.js&apos;;import RadioGroup from &apos;../packages/radio-group/index.js&apos;;import RadioButton from &apos;../packages/radio-button/index.js&apos;;import Checkbox from &apos;../packages/checkbox/index.js&apos;;import CheckboxButton from &apos;../packages/checkbox-button/index.js&apos;;import CheckboxGroup from &apos;../packages/checkbox-group/index.js&apos;;import Switch from &apos;../packages/switch/index.js&apos;;import Select from &apos;../packages/select/index.js&apos;;import Option from &apos;../packages/option/index.js&apos;;import OptionGroup from &apos;../packages/option-group/index.js&apos;;import Button from &apos;../packages/button/index.js&apos;;import ButtonGroup from &apos;../packages/button-group/index.js&apos;;import Table from &apos;../packages/table/index.js&apos;;import TableColumn from &apos;../packages/table-column/index.js&apos;;import DatePicker from &apos;../packages/date-picker/index.js&apos;;import TimeSelect from &apos;../packages/time-select/index.js&apos;;import TimePicker from &apos;../packages/time-picker/index.js&apos;;import Popover from &apos;../packages/popover/index.js&apos;;import Tooltip from &apos;../packages/tooltip/index.js&apos;;import MessageBox from &apos;../packages/message-box/index.js&apos;;import Breadcrumb from &apos;../packages/breadcrumb/index.js&apos;;import BreadcrumbItem from &apos;../packages/breadcrumb-item/index.js&apos;;import Form from &apos;../packages/form/index.js&apos;;import FormItem from &apos;../packages/form-item/index.js&apos;;import Tabs from &apos;../packages/tabs/index.js&apos;;import TabPane from &apos;../packages/tab-pane/index.js&apos;;import Tag from &apos;../packages/tag/index.js&apos;;import Tree from &apos;../packages/tree/index.js&apos;;import Alert from &apos;../packages/alert/index.js&apos;;import Notification from &apos;../packages/notification/index.js&apos;;import Slider from &apos;../packages/slider/index.js&apos;;import Loading from &apos;../packages/loading/index.js&apos;;import Icon from &apos;../packages/icon/index.js&apos;;import Row from &apos;../packages/row/index.js&apos;;import Col from &apos;../packages/col/index.js&apos;;import Upload from &apos;../packages/upload/index.js&apos;;import Progress from &apos;../packages/progress/index.js&apos;;import Spinner from &apos;../packages/spinner/index.js&apos;;import Message from &apos;../packages/message/index.js&apos;;import Badge from &apos;../packages/badge/index.js&apos;;import Card from &apos;../packages/card/index.js&apos;;import Rate from &apos;../packages/rate/index.js&apos;;import Steps from &apos;../packages/steps/index.js&apos;;import Step from &apos;../packages/step/index.js&apos;;import Carousel from &apos;../packages/carousel/index.js&apos;;import Scrollbar from &apos;../packages/scrollbar/index.js&apos;;import CarouselItem from &apos;../packages/carousel-item/index.js&apos;;import Collapse from &apos;../packages/collapse/index.js&apos;;import CollapseItem from &apos;../packages/collapse-item/index.js&apos;;import Cascader from &apos;../packages/cascader/index.js&apos;;import ColorPicker from &apos;../packages/color-picker/index.js&apos;;import Transfer from &apos;../packages/transfer/index.js&apos;;import Container from &apos;../packages/container/index.js&apos;;import Header from &apos;../packages/header/index.js&apos;;import Aside from &apos;../packages/aside/index.js&apos;;import Main from &apos;../packages/main/index.js&apos;;import Footer from &apos;../packages/footer/index.js&apos;;import Timeline from &apos;../packages/timeline/index.js&apos;;import TimelineItem from &apos;../packages/timeline-item/index.js&apos;;import Link from &apos;../packages/link/index.js&apos;;import Divider from &apos;../packages/divider/index.js&apos;;import Image from &apos;../packages/image/index.js&apos;;import Calendar from &apos;../packages/calendar/index.js&apos;;import Backtop from &apos;../packages/backtop/index.js&apos;;import InfiniteScroll from &apos;../packages/infinite-scroll/index.js&apos;;import PageHeader from &apos;../packages/page-header/index.js&apos;;import CascaderPanel from &apos;../packages/cascader-panel/index.js&apos;;import Avatar from &apos;../packages/avatar/index.js&apos;;import Drawer from &apos;../packages/drawer/index.js&apos;;import Popconfirm from &apos;../packages/popconfirm/index.js&apos;;import locale from &apos;element-ui/src/locale&apos;;import CollapseTransition from &apos;element-ui/src/transitions/collapse-transition&apos;;const components = [ Pagination, Dialog, Autocomplete, Dropdown, DropdownMenu, DropdownItem, Menu, Submenu, MenuItem, MenuItemGroup, Input, InputNumber, Radio, RadioGroup, RadioButton, Checkbox, CheckboxButton, CheckboxGroup, Switch, Select, Option, OptionGroup, Button, ButtonGroup, Table, TableColumn, DatePicker, TimeSelect, TimePicker, Popover, Tooltip, Breadcrumb, BreadcrumbItem, Form, FormItem, Tabs, TabPane, Tag, Tree, Alert, Slider, Icon, Row, Col, Upload, Progress, Spinner, Badge, Card, Rate, Steps, Step, Carousel, Scrollbar, CarouselItem, Collapse, CollapseItem, Cascader, ColorPicker, Transfer, Container, Header, Aside, Main, Footer, Timeline, TimelineItem, Link, Divider, Image, Calendar, Backtop, PageHeader, CascaderPanel, Avatar, Drawer, Popconfirm, CollapseTransition];const install = function(Vue, opts = {}) { locale.use(opts.locale); locale.i18n(opts.i18n); components.forEach(component =&gt; { Vue.component(component.name, component); }); Vue.use(InfiniteScroll); Vue.use(Loading.directive); Vue.prototype.$ELEMENT = { size: opts.size || &apos;&apos;, zIndex: opts.zIndex || 2000 }; Vue.prototype.$loading = Loading.service; Vue.prototype.$msgbox = MessageBox; Vue.prototype.$alert = MessageBox.alert; Vue.prototype.$confirm = MessageBox.confirm; Vue.prototype.$prompt = MessageBox.prompt; Vue.prototype.$notify = Notification; Vue.prototype.$message = Message;};/* istanbul ignore if */if (typeof window !== &apos;undefined&apos; &amp;&amp; window.Vue) { install(window.Vue);}export default { version: &apos;2.13.0&apos;, locale: locale.use, i18n: locale.i18n, install, CollapseTransition, Loading, Pagination, Dialog, Autocomplete, Dropdown, DropdownMenu, DropdownItem, Menu, Submenu, MenuItem, MenuItemGroup, Input, InputNumber, Radio, RadioGroup, RadioButton, Checkbox, CheckboxButton, CheckboxGroup, Switch, Select, Option, OptionGroup, Button, ButtonGroup, Table, TableColumn, DatePicker, TimeSelect, TimePicker, Popover, Tooltip, MessageBox, Breadcrumb, BreadcrumbItem, Form, FormItem, Tabs, TabPane, Tag, Tree, Alert, Notification, Slider, Icon, Row, Col, Upload, Progress, Spinner, Message, Badge, Card, Rate, Steps, Step, Carousel, Scrollbar, CarouselItem, Collapse, CollapseItem, Cascader, ColorPicker, Transfer, Container, Header, Aside, Main, Footer, Timeline, TimelineItem, Link, Divider, Image, Calendar, Backtop, InfiniteScroll, PageHeader, CascaderPanel, Avatar, Drawer, Popconfirm}; 當我們在main.js使用Vue.use注册插件的时候，就会执行對應的install方法來註冊我們的組件。當然，自定義組件還有很多東西沒有說，今天在忙著吧一百多億的數據遷移到其他數據庫，有點小忙，所有其他的東西後面我們再來細說，比如自定義組件中的自定義事件等。大家如果有深入研究的興趣，可以去https://cn.vuejs.org/，在結合一個開源的UI框架邊學習邊分析源碼，對學習的幫助比較大，例如可以結合element的源碼來一起學習。","link":"/posts/2976021726.html"},{"title":"Spring Boot使用log4j2记录日志","text":"日志可以帮我们在开发过程中乃至于生产上快速有效的定位问题的好帮手，所以如何记录有效的日子变得更加重要，在Spring Boot中，已经为我们默认添加了logback和log4j2,默认情况下，Spring Boot采用的是logback。关于日志组件的性能，我并没有亲自验证过，只是在logback log4j log4j2 性能实测中说log4j2性能是最高的。 这里咱们就选用log4j2来作为我们的日志组件,由于默认是使用的logback ,所以在使用之前需要排除掉，并且引入log4j2,pom.xml配置如下: 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt;&lt;!-- 去掉springboot默认配置 --&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--log4j2--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt; &lt;/dependency&gt; 在spring-boot-starter-log4j2中，我们可以看到依赖包: 1234567891011121314151617181920212223242526&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-jul&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jul-to-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.29&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 接下来，我们正式开始启用吧,首先在resources目录下新建一个log4j2.xml配置文件，配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--日志级别以及优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL --&gt;&lt;!--status=&quot;WARN&quot; :用于设置log4j2自身内部日志的信息输出级别，默认是OFF--&gt;&lt;!--monitorInterval=&quot;30&quot; :间隔秒数,自动检测配置文件的变更和重新配置本身--&gt;&lt;configuration status=&quot;WARN&quot; monitorInterval=&quot;30&quot;&gt; &lt;Properties&gt; &lt;!--自定义一些常量，之后使用${变量名}引用--&gt; &lt;Property name=&quot;logFilePath&quot;&gt;D://log&lt;/Property&gt; &lt;Property name=&quot;logFileName&quot;&gt;test.log&lt;/Property&gt; &lt;Property name=&quot;logPattern&quot;&gt; %d{yyyy-MM-dd HH:mm:ss.SSS} %5p %l ${hostName} -&amp;#45;&amp;#45; [%15.15t] %-40.40c{1.} : %m%n%ex &lt;/Property&gt; &lt;/Properties&gt; &lt;!--appenders:定义输出内容,输出格式,输出方式,日志保存策略等,常用其下三种标签[console,File,RollingFile]--&gt; &lt;appenders&gt; &lt;!--console :控制台输出的配置--&gt; &lt;console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!--PatternLayout :输出日志的格式,LOG4J2定义了输出代码,详见第二部分--&gt; &lt;PatternLayout pattern=&quot;${logPattern}&quot;/&gt; &lt;/console&gt; &lt;!--File :同步输出日志到本地文件--&gt; &lt;!--append=&quot;false&quot; :根据其下日志策略,每次清空文件重新输入日志,可用于测试--&gt;&lt;!-- &lt;File name=&quot;log&quot; fileName=&quot;${logFilePath}/logs/info.log&quot; append=&quot;false&quot;&gt;--&gt;&lt;!-- &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} %-5level %class{36} %L %M - %msg%xEx%n&quot;/&gt;--&gt;&lt;!-- &lt;/File&gt;--&gt; &lt;!--SMTP :邮件发送日志--&gt;&lt;!-- &lt;SMTP name=&quot;Mail&quot; subject=&quot;****SaaS系统正式版异常信息&quot; to=&quot;message@message.info&quot; from=&quot;message@lengjing.info&quot; smtpUsername=&quot;message@message.info&quot; smtpPassword=&quot;LENG****1234&quot; smtpHost=&quot;mail.lengjing.info&quot; smtpDebug=&quot;false&quot; smtpPort=&quot;25&quot; bufferSize=&quot;10&quot;&gt;--&gt;&lt;!-- &lt;PatternLayout pattern=&quot;[%-5p]:%d{YYYY-MM-dd HH:mm:ss} [%t] %c{1}:%L - %msg%n&quot; /&gt;--&gt;&lt;!-- &lt;/SMTP&gt;--&gt; &lt;!-- ${sys:user.home} :项目路径 --&gt; &lt;RollingFile name=&quot;RollingFileInfo&quot; fileName=&quot;${logFilePath}/logs/info.log&quot; filePattern=&quot;${logFilePath}/logs/$${date:yyyy-MM}/info-%d{yyyy-MM-dd}-%i.log&quot;&gt; &lt;!--ThresholdFilter :日志输出过滤--&gt; &lt;!--level=&quot;info&quot; :日志级别,onMatch=&quot;ACCEPT&quot; :级别在info之上则接受,onMismatch=&quot;DENY&quot; :级别在info之下则拒绝--&gt; &lt;ThresholdFilter level=&quot;info&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;[%d{HH:mm:ss:SSS}] [%p] - %l - %m%n&quot;/&gt; &lt;!-- Policies :日志滚动策略--&gt; &lt;Policies&gt; &lt;!-- TimeBasedTriggeringPolicy :时间滚动策略,默认0点小时产生新的文件,interval=&quot;6&quot; : 自定义文件滚动时间间隔,每隔6小时产生新文件, modulate=&quot;true&quot; : 产生文件是否以0点偏移时间,即6点,12点,18点,0点--&gt; &lt;TimeBasedTriggeringPolicy interval=&quot;6&quot; modulate=&quot;true&quot;/&gt; &lt;!-- SizeBasedTriggeringPolicy :文件大小滚动策略--&gt; &lt;SizeBasedTriggeringPolicy size=&quot;100 MB&quot;/&gt; &lt;/Policies&gt; &lt;!-- DefaultRolloverStrategy属性如不设置，则默认为最多同一文件夹下7个文件，这里设置了20 --&gt; &lt;DefaultRolloverStrategy max=&quot;20&quot;/&gt; &lt;/RollingFile&gt; &lt;RollingFile name=&quot;RollingFileWarn&quot; fileName=&quot;${logFilePath}/logs/warn.log&quot; filePattern=&quot;${logFilePath}/logs/$${date:yyyy-MM}/warn-%d{yyyy-MM-dd}-%i.log&quot;&gt; &lt;ThresholdFilter level=&quot;warn&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;[%d{HH:mm:ss:SSS}] [%p] - %l - %m%n&quot;/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size=&quot;100 MB&quot;/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name=&quot;RollingFileError&quot; fileName=&quot;${logFilePath}/logs/error.log&quot; filePattern=&quot;${logFilePath}/logs/$${date:yyyy-MM}/error-%d{yyyy-MM-dd}-%i.log&quot;&gt; &lt;ThresholdFilter level=&quot;error&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;[%d{HH:mm:ss:SSS}] [%p] - %l - %m%n&quot;/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size=&quot;100 MB&quot;/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/appenders&gt; &lt;!--然后定义logger，只有定义了logger并引入的appender，appender才会生效--&gt; &lt;loggers&gt; &lt;!--过滤掉spring和mybatis的一些无用的DEBUG信息--&gt; &lt;!--Logger节点用来单独指定日志的形式，name为包路径,比如要为org.springframework包下所有日志指定为INFO级别等。 --&gt; &lt;logger name=&quot;org.springframework&quot; level=&quot;INFO&quot;&gt;&lt;/logger&gt; &lt;logger name=&quot;org.mybatis&quot; level=&quot;INFO&quot;&gt;&lt;/logger&gt; &lt;!-- Root节点用来指定项目的根日志，如果没有单独指定Logger，那么就会默认使用该Root日志输出 --&gt; &lt;Root level=&quot;ALL&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;appender-ref ref=&quot;RollingFileInfo&quot;/&gt; &lt;appender-ref ref=&quot;RollingFileWarn&quot;/&gt; &lt;appender-ref ref=&quot;RollingFileError&quot;/&gt; &lt;/Root&gt; &lt;!--AsyncLogger :异步日志,LOG4J有三种日志模式,全异步日志,混合模式,同步日志,性能从高到底,线程越多效率越高,也可以避免日志卡死线程情况发生--&gt; &lt;!--additivity=&quot;false&quot; : additivity设置事件是否在root logger输出，为了避免重复输出，可以在Logger 标签下设置additivity为”false”--&gt;&lt;!-- &lt;AsyncLogger name=&quot;AsyncLogger&quot; level=&quot;ALL&quot; includeLocation=&quot;true&quot; additivity=&quot;false&quot;&gt;--&gt;&lt;!-- &lt;appender-ref ref=&quot;RollingFileError&quot;/&gt;--&gt;&lt;!-- &lt;appender-ref ref=&quot;RollingFileInfo&quot;/&gt;--&gt;&lt;!-- &lt;/AsyncLogger&gt;--&gt; &lt;/loggers&gt;&lt;/configuration&gt; 以上配置类源于LOG4J2 完整配置详解,在代码中: 123456789101112131415161718192021222324protected void loadDefaults(LoggingInitializationContext initializationContext, LogFile logFile) { if (logFile != null) { loadConfiguration(getPackagedConfigFile(&quot;log4j2-file.xml&quot;), logFile); } else { loadConfiguration(getPackagedConfigFile(&quot;log4j2.xml&quot;), logFile); } } @Override protected String[] getStandardConfigLocations() { return getCurrentlySupportedConfigLocations(); }private String[] getCurrentlySupportedConfigLocations() { List&lt;String&gt; supportedConfigLocations = new ArrayList&lt;&gt;(); supportedConfigLocations.add(&quot;log4j2.properties&quot;); if (isClassAvailable(&quot;com.fasterxml.jackson.dataformat.yaml.YAMLParser&quot;)) { Collections.addAll(supportedConfigLocations, &quot;log4j2.yaml&quot;, &quot;log4j2.yml&quot;); } if (isClassAvailable(&quot;com.fasterxml.jackson.databind.ObjectMapper&quot;)) { Collections.addAll(supportedConfigLocations, &quot;log4j2.json&quot;, &quot;log4j2.jsn&quot;); } supportedConfigLocations.add(&quot;log4j2.xml&quot;); return StringUtils.toStringArray(supportedConfigLocations); } 大概可以看出Spring Boot是可以自动识别log4j2.xml或者log4j2.yaml等配置文件,当然，如果是其他命名方式，可以在application.properties配置文件中配置,如:logging.config=classpath:log4j2-config.xml,yaml配置类似。接下来就是在我们的程序中使用， 123456789101112@RequestMapping(&quot;/exception&quot;)@RestController@Api(tags = &quot;制造bug&quot;)public class ExceptionController { private static final Logger logger = LogManager.getLogger(ExceptionController.class); @GetMapping(&quot;err&quot;) public String getErr(){ logger.info(&quot;制造一个bug&quot;); int calc= 1/0; return &quot;error&quot;; }} 调用结果: 122019-12-25 09:49:31.199 DEBUG springfox.documentation.spring.web.PropertySourcedRequestMappingHandlerMapping.lookupHandlerMethod(PropertySourcedRequestMappingHandlerMapping.java:108) 2CNU7X5OLAUE004 --- [nio-8080-exec-1] pertySourcedRequestMappingHandlerMapping : looking up handler for path: /exception/err2019-12-25 09:49:31.205 INFO com.eyiadmin.demo.controller.ExceptionController.getErr(ExceptionController.java:17) 2CNU7X5OLAUE004 --- [nio-8080-exec-1] c.e.d.c.ExceptionController : 制造一个bug 这样一来，我们就可以在之前说的GlobalExceptionHandler来记录我们的错误日志了, 12345678910111213141516171819202122@RestControllerAdvicepublic class GlobalExceptionHandler { private static final Logger logger = LogManager.getLogger(GlobalExceptionHandler.class); @ExceptionHandler(BindException.class) public ResponseResult&lt;?&gt; handleValidationException(BindException ex) { BindingResult validResult = ex.getBindingResult(); if (validResult.hasErrors()) { logger.error(validResult.getAllErrors().stream().map(err-&gt;err.getDefaultMessage()).collect(Collectors.joining(&quot;|&quot;))); return ResponseResult.error(validResult.getAllErrors().get(0).getDefaultMessage()); } else { return ResponseResult.error(&quot;操作失败&quot;); } } //捕捉所有Exception异常，这里可以捕捉自定义的业务异常 @ExceptionHandler(Exception.class) ResponseResult&lt;?&gt; handleException(Exception ex){ logger.error(ex.getMessage()); logger.error(ex.getStackTrace()); return ResponseResult.error(ex.getMessage()); }} 最后效果: 12019-12-25 10:14:38.086 ERROR com.eyiadmin.demo.handler.GlobalExceptionHandler.handleValidationException(GlobalExceptionHandler.java:21) 2CNU7X5OLAUE004 --- [nio-8080-exec-2] c.e.d.h.GlobalExceptionHandler : 学生名字不能为空 至此， log4j2.xml整合完毕,但是我们的工作还没有完，我们在需要记录日志的每个类文件都要写一个private static final Logger logger = LogManager.getLogger(GlobalExceptionHandler.class);,虽然工作量不大，但有时候可能会马虎的忽视掉GlobalExceptionHandler.class，那么我们有没有更简便的使用方式呢？这里我们可以借助强大的Lombok插件为我们提供好的@Log4j2注解，在需要打印日志的类上加上这个注解即可。 12345@Retention(RetentionPolicy.SOURCE)@Target({ElementType.TYPE})public @interface Log4j2 { String topic() default &quot;&quot;;} 我们可以看到@Log4j2有一个参数，它用于标识我们记录器的类别，默认情况下，它会将使用注解的类型，接下来我们来尝试一下: 12345678910111213141516171819202122@RestControllerAdvice@Log4j2public class GlobalExceptionHandler { @ExceptionHandler(BindException.class) public ResponseResult&lt;?&gt; handleValidationException(BindException ex) { BindingResult validResult = ex.getBindingResult(); if (validResult.hasErrors()) { log.error(validResult.getAllErrors().stream().map(err-&gt;err.getDefaultMessage()).collect(Collectors.joining(&quot;|&quot;))); return ResponseResult.error(validResult.getAllErrors().get(0).getDefaultMessage()); } else { return ResponseResult.error(&quot;操作失败&quot;); } } //捕捉所有Exception异常，这里可以捕捉自定义的业务异常 @ExceptionHandler(Exception.class) ResponseResult&lt;?&gt; handleException(Exception ex){ log.error(ex.getMessage()); log.error(ex.getStackTrace()); return ResponseResult.error(ex.getMessage()); }} 最后打印的日志如下: 12019-12-25 10:21:33.684 ERROR com.eyiadmin.demo.handler.GlobalExceptionHandler.handleValidationException(GlobalExceptionHandler.java:22) 2CNU7X5OLAUE004 --- [nio-8080-exec-1] c.e.d.h.GlobalExceptionHandler : 学生名字不能为空 如果我们给@Log4j2带上topic会输出什么呢？: 12345678910111213@RequestMapping(&quot;/exception&quot;)@RestController@Api(tags = &quot;制造bug&quot;)@Log4j2(topic = &quot;exception&quot;)public class ExceptionController { @GetMapping(&quot;err&quot;) public String getErr(){ log.info(&quot;制造一个bug&quot;); int calc= 1/0; return &quot;error&quot;; }} 效果: 12019-12-25 10:22:57.078 INFO com.eyiadmin.demo.controller.ExceptionController.getErr(ExceptionController.java:19) 2CNU7X5OLAUE004 --- [nio-8080-exec-3] exception : 制造一个bug 这里只是简单的说了一下日志组件的使用，我们在实际使用过程中，也会结合AOP来应用。以及在分布式、微服务中的链路追踪，","link":"/posts/3023194237.html"},{"title":"我想用gatsbyjs搞一个静态网站","text":"最近想弄一个生活、办公技巧分项的静态网站，为什么要用静态的呢？成本啊兄dei。静态网页生成有很多工具，hexo、hugo、nuxt等等，很多的。这次呢我想试试gatsbyjs,所以才有了此文,盆友们可以去https://www.gatsbyjs.org/看详细介绍。 gatsbyjs安装在安装gatsbyjs之前，首先得确保Node和Git的环境已经正确安装，未安装的，可以看我之前的博文，或者自行百度，在此不再详述。我直接安装gatsbyjs. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071C:\\Users\\lenovo&gt;npm install -g gatsby-clinpm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.C:\\Users\\lenovo\\AppData\\Roaming\\npm\\gatsby -&gt; C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\gatsby-cli\\lib\\index.js&gt; core-js@2.6.11 postinstall C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\gatsby-cli\\node_modules\\core-js&gt; node -e &quot;try{require(&apos;./postinstall&apos;)}catch(e){}&quot;Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!The project needs your help! Please consider supporting of core-js on Open Collective or Patreon:&gt; https://opencollective.com/core-js&gt; https://www.patreon.com/zloirockAlso, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)&gt; gatsby-telemetry@1.1.46 postinstall C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\gatsby-cli\\node_modules\\gatsby-telemetry&gt; node src/postinstall.js || true╔════════════════════════════════════════════════════════════════════════╗║ ║║ Gatsby collects anonymous usage analytics ║║ to help improve Gatsby for all users. ║║ ║║ If you&apos;d like to opt-out, you can use `gatsby telemetry --disable` ║║ To learn more, checkout https://gatsby.dev/telemetry ║║ ║╚════════════════════════════════════════════════════════════════════════╝&gt; gatsby-cli@2.8.22 postinstall C:\\Users\\lenovo\\AppData\\Roaming\\npm\\node_modules\\gatsby-cli&gt; node scripts/postinstall.js╔════════════════════════════════════════════════════════════════════════╗║ ║║ Gatsby collects anonymous usage analytics ║║ to help improve Gatsby for all users. ║║ ║║ If you&apos;d like to opt-out, you can use `gatsby telemetry --disable` ║║ To learn more, checkout https://gatsby.dev/telemetry ║║ ║╚════════════════════════════════════════════════════════════════════════╝Success!Welcome to the Gatsby CLI! Please visit https://www.gatsbyjs.org/docs/gatsby-cli/ for more information.Usage: gatsby &lt;command&gt; [options]Commands: gatsby develop Start development server. Watches files, rebuilds, and hot reloads if something changes gatsby build Build a Gatsby project. gatsby serve Serve previously built Gatsby site. gatsby info Get environment information for debugging and issue reporting gatsby clean Wipe the local gatsby environment including built assets and cache gatsby repl Get a node repl with context of Gatsby environment, see (https://www.gatsbyjs.org/docs/gatsby-repl/) gatsby new [rootPath] [starter] Create new Gatsby project. gatsby plugin Useful commands relating to Gatsby plugins gatsby telemetry Enable or disable Gatsby anonymous analytics collection.Options: --verbose Turn on verbose output [boolean] [default: false] --no-color, --no-colors Turn off the color in output [boolean] [default: false] --json Turn on the JSON logger [boolean] [default: false] -h, --help Show help [boolean] -v, --version Show the version of the Gatsby CLI and the Gatsby package in the current project [boolean]npm WARN ink@2.6.0 requires a peer of @types/react@&gt;=16.8.0 but none is installed. You must install peer dependencies yourself.npm WARN auto-bind@3.0.0 requires a peer of @types/react@&gt;=16.8.0 but none is installed. You must install peer dependencies yourself.+ gatsby-cli@2.8.22added 376 packages from 164 contributors in 76.778s 可以看看我本机的环境: 123456789101112C:\\Users\\lenovo&gt;gatsby info System: OS: Windows 10 10.0.18362 CPU: (8) x64 Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz Binaries: Node: 13.5.0 - D:\\Program Files\\nodejs\\node.EXE npm: 6.13.4 - D:\\Program Files\\nodejs\\npm.CMD Languages: Python: 3.8.1 Browsers: Edge: 44.18362.449.0 新建网站首先在本机创建一个文件夹用于存放网页，然后执行新建命令gatsby new [sitename] [模版路径]，gatsbyjs会通过git去下载模版并创建项目。创建完成后会自动安装，但是可能会安装失败，因为默认npm的镜像地址会被我们的大型局域网屏蔽，我们可以借助nrm来管理镜像地址。我在安装过程中，遇到了 1unbuild: sill gentlyRm target.inParent = false 简单粗暴的方式就是直接删除node_modules目录，重新安装。如果是windows的话，请事先安装： 1npm install windows-build-tools -g 供gatsby-transformer-sharp编译时用，否则会提示编译失败。最后安装成功: 123456789101112131415161718E:\\gatsbyjs\\happylife&gt;npm install&gt; sharp@0.23.4 install E:\\gatsbyjs\\happylife\\node_modules\\sharp&gt; (node install/libvips &amp;&amp; node install/dll-copy &amp;&amp; prebuild-install) || (node-gyp rebuild &amp;&amp; node install/dll-copy)info sharp Downloading https://github.com/lovell/sharp-libvips/releases/download/v8.8.1/libvips-8.8.1-win32-x64.tar.gzinfo sharp Creating E:\\gatsbyjs\\happylife\\node_modules\\sharp\\build\\Releaseinfo sharp Copying DLLs from E:\\gatsbyjs\\happylife\\node_modules\\sharp\\vendor\\lib to E:\\gatsbyjs\\happylife\\node_modules\\sharp\\build\\Releasenpm WARN tsutils@3.17.1 requires a peer of typescript@&gt;=2.8.0 || &gt;= 3.2.0-dev || &gt;= 3.3.0-dev || &gt;= 3.4.0-dev || &gt;= 3.5.0-dev || &gt;= 3.6.0-dev || &gt;= 3.6.0-beta || &gt;= 3.7.0-dev || &gt;= 3.7.0-beta but none is installed. You must install peer dependencies yourself.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules\\chokidar\\node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;})added 41 packages from 91 contributors in 252.487s27 packages are looking for funding run `npm fund` for details 我们来试试gatsbyjs的初步呈现效果,我们通过cmd进入到网站目录执行gatsby develop即可运行 我们来换个主题吧，在https://www.gatsbyjs.org/starters/?v=2可以看到很多风格非常漂亮的模版。我这里选择的是gatsby-starter-hero-blog,进入到gatsby-starter-hero-blog的页面，可以看到如何下载模版: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121E:\\gatsbyjs&gt;gatsby new happylife https://github.com/greglobinski/gatsby-starter-hero-bloginfo Creating new site from git: https://github.com/greglobinski/gatsby-starter-hero-blog.gitCloning into &apos;happylife&apos;...remote: Enumerating objects: 197, done.remote: Counting objects: 100% (197/197), done.remote: Compressing objects: 100% (159/159), done.remote: Total 197 (delta 16), reused 150 (delta 11), pack-reused 0 eceiving objects: 100% (197/197), 8.79 MiB | 71.00 KiReceiving objects: 100% (197/197), 8.83 MiB | 58.00 KiB/s, done.Resolving deltas: 100% (16/16), done.success Created starter directory layoutinfo Installing packages...npm WARN deprecated postcss-cssnext@3.1.0: &apos;postcss-cssnext&apos; has been deprecated in favor of &apos;postcss-preset-env&apos;. Read more at https://moox.io/blog/deprecating-cssnext/npm WARN deprecated bfj-node4@5.3.1: Switch to the `bfj` package for fixes and new features!npm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.npm WARN deprecated md5-file@3.2.3: This module is looking for a new maintainer.npm WARN deprecated browserslist@2.11.3: Browserslist 2 could fail on reading Browserslist &gt;3.0 config used in other tools.npm WARN deprecated joi@11.4.0: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).npm WARN deprecated core-js@1.2.7: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.npm WARN deprecated browserslist@1.7.7: Browserslist 2 could fail on reading Browserslist &gt;3.0 config used in other tools.npm WARN deprecated hoek@4.2.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).npm WARN deprecated topo@2.0.2: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor.npm WARN deprecated @types/vfile-message@2.0.0: This is a stub types definition. vfile-message provides its own type definitions, so you do not need this installed.&gt; deasync@0.1.19 install E:\\gatsbyjs\\happylife\\node_modules\\deasync&gt; node ./build.js`win32-x64-node-13` exists; testingBinary is fine; exiting&gt; sharp@0.23.4 install E:\\gatsbyjs\\happylife\\node_modules\\sharp&gt; (node install/libvips &amp;&amp; node install/dll-copy &amp;&amp; prebuild-install) || (node-gyp rebuild &amp;&amp; node install/dll-copy)info sharp Using cached C:\\Users\\lenovo\\AppData\\Roaming\\npm-cache\\_libvips\\libvips-8.8.1-win32-x64.tar.gzinfo sharp Creating E:\\gatsbyjs\\happylife\\node_modules\\sharp\\build\\Releaseinfo sharp Copying DLLs from E:\\gatsbyjs\\happylife\\node_modules\\sharp\\vendor\\lib to E:\\gatsbyjs\\happylife\\node_modules\\sharp\\build\\Release&gt; sharp@0.23.3 install E:\\gatsbyjs\\happylife\\node_modules\\sharp-cli\\node_modules\\sharp&gt; (node install/libvips &amp;&amp; node install/dll-copy &amp;&amp; prebuild-install) || (node-gyp rebuild &amp;&amp; node install/dll-copy)info sharp Using cached C:\\Users\\lenovo\\AppData\\Roaming\\npm-cache\\_libvips\\libvips-8.8.1-win32-x64.tar.gzinfo sharp Creating E:\\gatsbyjs\\happylife\\node_modules\\sharp-cli\\node_modules\\sharp\\build\\Releaseinfo sharp Copying DLLs from E:\\gatsbyjs\\happylife\\node_modules\\sharp-cli\\node_modules\\sharp\\vendor\\lib to E:\\gatsbyjs\\happylife\\node_modules\\sharp-cli\\node_modules\\sharp\\build\\Release&gt; core-js@2.6.11 postinstall E:\\gatsbyjs\\happylife\\node_modules\\core-js&gt; node -e &quot;try{require(&apos;./postinstall&apos;)}catch(e){}&quot;Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!The project needs your help! Please consider supporting of core-js on Open Collective or Patreon:&gt; https://opencollective.com/core-js&gt; https://www.patreon.com/zloirockAlso, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)&gt; core-js-pure@3.6.2 postinstall E:\\gatsbyjs\\happylife\\node_modules\\core-js-pure&gt; node -e &quot;try{require(&apos;./postinstall&apos;)}catch(e){}&quot;&gt; ejs@2.7.4 postinstall E:\\gatsbyjs\\happylife\\node_modules\\ejs&gt; node ./postinstall.jsThank you for installing EJS: built with the Jake JavaScript build tool (https://jakejs.com/)&gt; gatsby-telemetry@1.1.46 postinstall E:\\gatsbyjs\\happylife\\node_modules\\gatsby-telemetry&gt; node src/postinstall.js || true&gt; cwebp-bin@5.1.0 postinstall E:\\gatsbyjs\\happylife\\node_modules\\cwebp-bin&gt; node lib/install.js √ cwebp pre-build test passed successfully&gt; mozjpeg@6.0.1 postinstall E:\\gatsbyjs\\happylife\\node_modules\\mozjpeg&gt; node lib/install.js √ mozjpeg pre-build test passed successfully&gt; pngquant-bin@5.0.2 postinstall E:\\gatsbyjs\\happylife\\node_modules\\pngquant-bin&gt; node lib/install.js √ pngquant pre-build test passed successfully&gt; gatsby-cli@2.8.23 postinstall E:\\gatsbyjs\\happylife\\node_modules\\gatsby\\node_modules\\gatsby-cli&gt; node scripts/postinstall.js&gt; gatsby@2.18.18 postinstall E:\\gatsbyjs\\happylife\\node_modules\\gatsby&gt; node scripts/postinstall.jsnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN gatsby-plugin-styled-jsx@3.1.17 requires a peer of styled-jsx@^3.0.2 but none is installed. You must install peer dependencies yourself.npm WARN react-addons-perf@15.4.2 requires a peer of react-dom@^15.4.2 but none is installed. You must install peer dependencies yourself.npm WARN eslint-config-react-app@5.1.0 requires a peer of babel-eslint@10.x but none is installed. You must install peer dependencies yourself.npm WARN eslint-config-react-app@5.1.0 requires a peer of eslint@6.x but none is installed. You must install peer dependencies yourself.npm WARN tsutils@3.17.1 requires a peer of typescript@&gt;=2.8.0 || &gt;= 3.2.0-dev || &gt;= 3.3.0-dev || &gt;= 3.4.0-dev || &gt;= 3.5.0-dev || &gt;= 3.6.0-dev || &gt;= 3.6.0-beta || &gt;= 3.7.0-dev || &gt;= 3.7.0-beta but none is installed. You must install peer dependencies yourself.npm WARN ws@7.2.1 requires a peer of bufferutil@^4.0.1 but none is installed. You must install peer dependencies yourself.npm WARN ws@7.2.1 requires a peer of utf-8-validate@^5.0.2 but none is installed. You must install peer dependencies yourself.npm WARN react-instantsearch-native@5.7.0 requires a peer of react-native@&gt;= 0.32.0 but none is installed. You must install peer dependencies yourself.npm WARN eslint-plugin-graphql@2.1.1 requires a peer of graphql@^0.12.0 || ^0.13.0 but none is installed. You must install peer dependencies yourself.npm WARN gatsby-starter-hero-blog@2.0.0 No repository field.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules\\chokidar\\node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;})added 3046 packages from 1481 contributors in 1263.674s56 packages are looking for funding run `npm fund` for detailsinfo Initialising git in happylifeInitialized empty Git repository in E:/gatsbyjs/happylife/.git/info Create initial git commit in happylifeinfoYour new Gatsby site has been successfully bootstrapped. Start developing it by running: cd happylife gatsby develop 我们可以看到其目录结构:可以在content\\meta\\config.js修改网站的一些基本属性，需要新建文章的话，就按content\\posts下面的结构类似就可.详情可以访问https://dev.greglobinski.com/install-blog-starter/查看。我们的菜单可以再src\\components\\Menu\\Menu.js修改: 1234567this.items = [ { to: &quot;/&quot;, label: &quot;Home&quot;, icon: FaHome }, { to: &quot;/category/&quot;, label: &quot;Categories&quot;, icon: FaTag }, { to: &quot;/search/&quot;, label: &quot;Search&quot;, icon: FaSearch }, ...pages, { to: &quot;/contact/&quot;, label: &quot;Contact&quot;, icon: FaEnvelope } ]; 这里的pages是只想content\\pages目录,可以自己按照/No--title/的格式新增菜单。编译上传到七牛云: 1gatsby build &amp;&amp; qshell qupload2 --overwrite=true --rescan-local=true --src-dir=E:\\gatsbyjs\\happylife\\public --bucket=eyiadminxxxxx 最后献上地址:https://live.52fx.biz/ 参考https://www.gatsbyjs.org/docs/gatsby-on-windows/","link":"/posts/2363611189.html"},{"title":"golang下载妹子图","text":"想用golang下载妹子图吗？点进来看看吧!很方便 闲来无趣，就想着看用golang来做点什么事情，这不，到处都是python下载妹子图,我就想着用golang来弄一个下载妹子图的简单小工具 简单分析页面结构访问https://www.meizitu.com 页面后, 进入到详情页面，可以看到url变为https://www.meizitu.com/a/5511.html ，我们将url中的数字任意修改，发现都能访问，那么，我们暂且就通过手动输入页面索引的方式来访问页面。我们再看看页面结构,通过google浏览器的开发者工具很容易就可以看到： 下载图片刚才我们已经大致的分析了页面结构，接下来 ，我们就开始简单的实现图片下载的功能，在这里我们选用了：colly:用于采集页面和图片uuid:生产UUIDcli:命令行工具包1、我们先初始化一个队列，用于存放需要访问的url 1234var q, _ = queue.New( 2, // Number of consumer threads &amp;queue.InMemoryQueueStorage{MaxSize: 10000}, // Use default queue storage ) 2、初始化需要下载的页面，我用命令行的方式来决定起始页 123456789101112if args := context.Args(); len(args) &gt; 0 { return fmt.Errorf(&quot;invalid command: %q&quot;, args.Get(0)) } start := context.Int(&quot;s&quot;) log.Println(&quot;起始页:&quot;, start) end := context.Int(&quot;e&quot;) log.Println(&quot;截止页:&quot;, end) for i := start; i &lt;= end; i++ { url := fmt.Sprintf(&quot;https://www.meizitu.com/a/%d.html&quot;, i) q.AddURL(url)} 3、初始化一个colly来处理队列中的url,我在OnHTML中，查找页面的postContent&gt;img的dom节点，获取图片的路径，又放在队列中 1234567891011121314151617181920c := colly.NewCollector() c.UserAgent = &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36&quot; c.OnHTML(&quot;.postContent&quot;, func(e *colly.HTMLElement) { //e.Request.Visit(e.Attr(&quot;href&quot;)) e.ForEach(&quot;img&quot;, func(i int, element *colly.HTMLElement) { //e.Request.Visit(element.Attr(&quot;src&quot;)) q.AddURL(element.Attr(&quot;src&quot;)) }) }) c.OnResponse(func(resp *colly.Response) { if strings.Contains(resp.Headers.Get(&quot;Content-Type&quot;), &quot;image/jpeg&quot;) { download(resp.Body) } }) c.OnRequest(func(r *colly.Request) { fmt.Println(&quot;Visiting&quot;, r.URL) }) q.Run(c) 最后的效果：代码很简单，如果需要可以去https://github.com/eyiadmin/meizitu 看看，如果想下载图片，就直接下载exe文件。在命令行输入main -s 1 -e 100即可下载 12345678910111213141516171819202122232425main -s 100 -e 1082019/10/07 14:26:42 起始页: 1002019/10/07 14:26:43 截止页: 108Visiting https://www.meizitu.com/a/100.htmlVisiting https://www.meizitu.com/a/101.htmlVisiting https://www.meizitu.com/a/102.htmlVisiting https://www.meizitu.com/a/103.htmlVisiting https://www.meizitu.com/a/104.htmlVisiting https://www.meizitu.com/a/105.htmlVisiting https://www.meizitu.com/a/106.htmlVisiting https://www.meizitu.com/a/107.htmlVisiting https://www.meizitu.com/a/108.htmlVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/01.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/02.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/03.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/04.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/05.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/06.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/30/01.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/07.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/08.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/29/09.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/30/02.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/30/03.jpgVisiting http://pic.topmeizi.com/wp-content/uploads/2012a/01/30/04.jpg","link":"/posts/3719689165.html"},{"title":"展示一下Github的常规操作","text":"一般情况下，大部分人会自己使用Gitlab搭建一个私有的Git服务器，但是有时候为了避免麻烦，也可以使用诸如：Gitee,Github等免费的Git服务器来托管代码，这里个人使用的是Github 初始化项目Github账号注册很简单，这里就不说了。我们来看看怎么创建项目，登录到Github之后，右上角会有一个加号，从这里来new repository创建一个新repository这里public(公开项目),private(私有项目)都是免费的，根据个人情况选择常见完成后，会有操作提示，可以使用git clone到本地或者直接下载。这里我们还是使用git init来初始化一个项目吧 1234561. 先创建一个存放项目的文件夹，随意创建一个文件，通过cmd命令行cd到该目录开始执行命令2. git init #初始化仓库3. git add * #添加所有文件到暂存区4. git commit -m &quot;仓库初始化&quot; #此次的提交注释说明,并提交到本地仓库5. git remote add origin https://github.com/eyiadmin/demo.git # 与远程仓库建立关联关系6. git push -u origin master # 提交到远程仓库 常用命令 git add 文件名 添加单个文件到暂存区 git add -A 添加当前目录所有文件到暂存区 git add . 添加当前变更文件到暂存区 git commit -m '说明' 提交暂存区文件到本地仓库 git commit 文件名 -m '说明' 单个文件提交到本地仓库 git fetch 拉取代码 git pull 拉取最新内容并合并到当前分支 git pull origin develop 拉取具体的远程分支 git push origin master 提交到具体的远程分支,如果不存在会自动创建本地同名的远程分支 git checkout 分支名称 切换本地分支 git checkout origin/分支名称 切换远程分支 git checkout -b 新分支名称 基于本地分支新建新分支 git branch -d 分支名称 删除分支 git branch -M 旧分支名称 新分支名称 移动或者重命名分支 git checkout --track origin/分支名称 基于远程分支创建本地分支，并跟踪对应来自 ‘origin’ 的远程分支 git merge --no-ff 分支名称 保留合并分支的提交记录 git remote add origin 仓库地址 建立远程连接 git remote set-url origin 仓库地址 修改推送源 git reset --soft HEAD~1 软回滚到上一个版本 简单的代码提交流程 git status 查看仓库的状态 git add . 提交代码到暂存区 git commit -m &quot;提交内容说明&quot; 将暂存区代码提交到本地仓库 git pull 提交之前从远程拉取项目更新,git diff 对比内容，一般使用GUI查看 git push origin 分支名称 提交到指定分区 Git回滚到指定版本 git log 查看记录获取commit_sha1 git reset ---hard commit_sha1 硬回滚,抛弃回滚之后的内容,git reset ---soft commit_sha1 软回滚 未完待续","link":"/posts/1834150584.html"},{"title":"golang平滑升级服务","text":"不中断服务进行服务升级 最近在学习go语言，想着用gin+gorm来做一个微信小程序，在gin的github中看到了如何优雅的重启或者停止，因为在生产项目中，如果强行kill掉的话会造成正在执行的业务中断、用户访问被拒绝。显然，平滑的重启或升级我们的应用是多么的重要。 .net的发布过程我做了很多年的.net ，但是从来没有想过如何去平滑更新服务端，也是因为业务要求的可用性不高。一般发布新版本都是采用最笨拙的方式，就是发通知(xx时间升级会暂停服务)-&gt;到点停止服务备份、copy新文件-&gt;启动服务。 java的发布过程在用java做项目的时候，当时就用到了nginx中的upstream来负载均衡，当然，这时候要求的可用性较高，所以在发布服务的时候，就不能再采用简单粗暴的停止-启动大法。我们可以配合nginx -s reload来实现后端服务的灰度发布并且让用户无感知。 golang的发布过程Graceful restart or stopgolang中有很多开源的解决方案：endlessgraceoverseer 实践出真知在这里我们来试试grace(star最多，commits最多)，在github上面有详细的demo和测试步骤。v1 12345router := gin.Default()router.GET(&quot;/&quot;, func(c *gin.Context) { c.String(http.StatusOK, &quot;Welcome Gin Server&quot;)})gracehttp.Serve(&amp;http.Server{Addr:&quot;:8080&quot;,Handler:router}) v2 12345router := gin.Default()router.GET(&quot;/&quot;, func(c *gin.Context) { c.String(http.StatusOK, &quot;Welcome Gin New Server&quot;)})gracehttp.Serve(&amp;http.Server{Addr:&quot;:8080&quot;,Handler:router}) 如果是在windows环境下开发，会提示: 1github.com\\facebookgo\\grace\\gracehttp\\http.go:104:53: undefined: syscall.SIGUSR2 貌似是因为grace不支持windows的原因，我们可以暂时不用搭理他，先编译一个二进制文件， 123SET GOARCH=amd64SET GOOS=linuxgo build main.go 把生成好的文件复制到linux上，先启动起来(我是用的nohup ./grace &amp;来启动的)，这时候我们查看pid是多少 1234ps -ef|grep graceroot 11915 11364 0 13:06 pts/0 00:00:00 ./graceroot 11921 11364 0 13:08 pts/0 00:00:00 grep --color=auto grace 此时访问服务结果：这时候，我们把之前的文件备份，将v2的代码编译好并拷贝上去，执行命令 123456kill -USR2 11915这时候在看看应用进程ps -ef|grep graceroot 11928 1 0 13:11 pts/0 00:00:00 ./graceroot 11935 11364 0 13:12 pts/0 00:00:00 grep --color=auto grace pid已经变为11928,此时访问服务结果：[参考]https://github.com/facebookarchive/gracehttps://github.com/gin-gonic/gin","link":"/posts/3411219991.html"},{"title":"mysql慢查询语句分析总结","text":"我们经常会接触到MySQL，也经常会遇到一些MySQL的性能问题。我们可以借助慢查询日志和explain命令初步分析出SQL语句存在的性能问题 通过SHOW FULL PROCESSLIST查看问题SHOW FULL PROCESSLIST相当于select * from information_schema.processlist可以列出正在运行的连接线程， 说明： id 连接id，可以使用kill+连接id的方式关闭连接(kill 9339) user显示当前用户 host显示连接的客户端IP和端口 db显示进程连接的数据库 command显示当前连接的当前执行的状态，sleep、query、connect time显示当前状态持续的时间(秒) state显示当前连接的sql语句的执行状态，copying to tmp table、sorting result、sending data等 info显示sql语句,如果发现比较耗时的语句可以复制出来使用explain分析。 慢查询日志慢查询日志是MySQL用于记录响应时间超过设置阈值(long_query_time)的SQL语句，默认情况下未开启慢查询日志，需要手动配置。下面我们要记住几个常用的属性: slow_query_log:是否开启慢查询(ON为开启，OFF则为关闭) long_query_time:慢查询阀值，表示SQL语句执行时间超过这个值就会记录,默认为10s slow_query_log_file:慢查询日志存储的文件路径 log_queries_not_using_indexes: 记录没有使用索引查询语句(ON为开启,OFF为关闭) log_output:日志存储方式(FILE表示将日志写入文件,TABLE表示写入数据库中，默认值为FILE,如果存入数据库中，我们可以通过select * from mysql.slow_log的方式去查询，一般性能要求相对较高的建议存文件) 我们可以通过show variables like ‘%关键字%’的方式查询我们设置的属性值我们有两种方式设置我们的属性,一种是set global 属性=值的方式(重启失效)，另一种是配置文件(重启生效)命令方式: 123set global slow_query_log=1;set global long_query_time=1; set global slow_query_log_file=&apos;mysql-slow.log&apos; 配置文件方式: 1234slow_query_log = &apos;ON&apos;slow_query_log_file = D:/Tools/mysql-8.0.16/slow.loglong_query_time = 1log-queries-not-using-indexes pt-qurey-digest分析慢查询语句percona-toolkit包含了很多实用强大的mysql工具包，pt-qurey-digest只是其中一个用于分析慢查询日志是工具。需要去官网下载,使用方法也很简单: 1./pt-query-digest slow2.log &gt;&gt; slow2.txt 即可得出一个分析结果: 12345678910111213141516171819202122232425262728293031323334# Query 9: 0.00 QPS, 0.00x concurrency, ID 0xF914D8CC2938CE6CAA13F8E57DF04B2F at byte 499246# This item is included in the report because it matches --limit.# Scores: V/M = 0.22# Time range: 2019-07-08T03:56:12 to 2019-07-12T00:46:28# Attribute pct total min max avg 95% stddev median# ============ === ======= ======= ======= ======= ======= ======= =======# Count 8 69# Exec time 1 147s 1s 3s 2s 3s 685ms 2s# Lock time 0 140ms 2ms 22ms 2ms 3ms 2ms 2ms# Rows sent 0 0 0 0 0 0 0 0# Rows examine 0 23.96M 225.33k 482.77k 355.65k 462.39k 81.66k 345.04k# Query size 2 17.72k 263 263 263 263 0 263# String:# Databases xxxx# Hosts xx.xxx.xxx.xxx# Users root# Query_time distribution# 1us# 10us# 100us# 1ms# 10ms# 100ms# 1s ################################################################# 10s+# Tables# SHOW TABLE STATUS FROM `xxxx` LIKE &apos;xxxxx_track_exec_channel&apos;\\G# SHOW CREATE TABLE `xxxx`.`xxxxxxxx_exec_channel`\\G# SHOW TABLE STATUS FROM `xxx` LIKE &apos;xxxxx_TRACK_ASSIGN&apos;\\G# SHOW CREATE TABLE `xxxx`.`xxxxx_EFFECTIVE_TRACK_ASSIGN`\\G# SHOW TABLE STATUS FROM `xxx` LIKE &apos;xxxx_task_exec&apos;\\G# SHOW CREATE TABLE `xxxx`.`xxxxx_task_exec`\\GUPDATExxxxxx_effective_track_exec_channel a SET EXEC_CHANNEL_CODE=(SELECT GROUP_CONCAT(DISTINCT(channel_id)) FROM xxxxxx_EFFECTIVE_TRACK_ASSIGN WHERE status in (1,2,4) AND id IN (SELECT assgin_id FROM xxxxxx_task_exec WHERE task_id=a.task_id))\\G explain分析SQL语句上面几点大概的介绍到了几种获取慢查询SQL语句的方式，现在，我们就需要借助explain来分析查找SQL语句慢的原因。explain使用也很简单，直接在SELECT|UPDATE等语句前加上EXPLAIN即可 id表的执行顺序，复制的sql语句往往会分为很多步,序号越大越先执行,id相同执行顺序从上往下 select_type数据读取操作的操作类型: SIMPLE(简单SELECT，不使用UNION或子查询等) PRIMARY(子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的select被标记为PRIMARY) UNION(UNION中的第二个或后面的SELECT语句) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询) UNION RESULT(UNION的结果，union语句中第二个select开始后面所有select) SUBQUERY(子查询中的第一个SELECT，结果不依赖于外部查询) DEPENDENT SUBQUERY(子查询中的第一个SELECT，依赖于外部查询) DERIVED(派生表的SELECT, FROM子句的子查询) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行) table数据来源于那张表，关联等复杂查询时会用临时虚拟表 type检索数据的方式 system:表只有一行记录 const:通过索引查找并且一次性找到 eq_ref:唯一性索引扫描 ref:非唯一行索引扫描 range:按范围查找 index:遍历索引树 all:全表扫描 possible_keys显示可能使用的索引 Key实际使用的索引 key_len索引的长度，一般来说，长度越短越好 ref列与索引的比较，表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows估算查找的结果记录条数 ExtraSQL查询的详细信息 Using where:表示使用where条件过滤 Using temporary:使用了临时表暂存结果 Using filesort:说明mysql对数据使用一个外部索引排序。未按照表内的索引顺序进行读取。 Using index:表示select语句中使用了覆盖索引，直接从索引中取值 Using join buffer:使用了连接缓存 Using index condition:表示查询的列有非索引的列 [参考]MySQL Explain详解","link":"/posts/2170526710.html"},{"title":".net core3.x发布到Docker运行","text":"我们开发完之后，需要进行发布部署，一般高端的公司是采用CI/CD方式自动发布，但是我所处的公司都是使用手动发布，之前我都是输入命令的方式: 1dotnet publish -c Release -r win7-x64 -o ./bin/output 执行之后就会在相应目录生成所有dll 随迎时代潮流，我们也该有些骚气的操作了，这次咱们就把.net core3.x采用docker发布，因为我的电脑无法安装docker： 那么就想到要么先publish然后把DLL构建成docker镜像，要么就直接通过源码构建镜像，我就偷偷懒，选择后者。那么首先在vs添加【Docker支持】-&gt;【Linux】就会生成一个Dockerfile: 1234567891011121314151617181920212223242526FROM mcr.microsoft.com/dotnet/core/aspnet:3.0-buster-slim AS baseWORKDIR /appEXPOSE 80EXPOSE 443FROM mcr.microsoft.com/dotnet/core/sdk:3.0-buster AS build# 相当于cd /srcWORKDIR /src# 将当前本地Web/Web.csproj 复制到/src/Web/目录下COPY [&quot;Web/Web.csproj&quot;, &quot;Web/&quot;]# 还原项目依赖库RUN dotnet restore &quot;Web/Web.csproj&quot;将当前本地目录复制到/srcCOPY . .WORKDIR &quot;/src/Web&quot;RUN dotnet build &quot;Web.csproj&quot; -c Release -o /app/buildFROM build AS publishRUN dotnet publish &quot;Web.csproj&quot; -c Release -o /app/publishFROM base AS finalWORKDIR /app# 从编译阶段的中的publish层镜像拷贝/app/publish目录到/app目录下COPY --from=publish /app/publish .COPY [&quot;Web/Web.xml&quot;, &quot;.&quot;]ENTRYPOINT [&quot;dotnet&quot;, &quot;Web.dll&quot;] 如果你是提前编译好的，那么可以将Dockerfile修改一下: 12345678FROM mcr.microsoft.com/dotnet/core/aspnet:3.0-buster-slim EXPOSE 80EXPOSE 443COPY . /appWORKDIR /appENTRYPOINT [&quot;dotnet&quot;, &quot;Web.dll&quot;] 我们将整个源码复制到Centos服务器上,cd到dockerfile目录执行命令: 1docker build -t core3.x-swagger -f Dockerfile . 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071Sending build context to Docker daemon 40.45kBStep 1/17 : FROM mcr.microsoft.com/dotnet/core/aspnet:3.0-buster-slim AS base ---&gt; 880d85db3775Step 2/17 : WORKDIR /app ---&gt; Using cache ---&gt; 2686a6832749Step 3/17 : EXPOSE 80 ---&gt; Using cache ---&gt; f910a2eacda4Step 4/17 : EXPOSE 443 ---&gt; Using cache ---&gt; a157473c89eeStep 5/17 : FROM mcr.microsoft.com/dotnet/core/sdk:3.0-buster AS build ---&gt; a345b68a725eStep 6/17 : WORKDIR /src ---&gt; Using cache ---&gt; 8b2a2b0743f5Step 7/17 : COPY [&quot;Web/Web.csproj&quot;, &quot;Web/&quot;] ---&gt; 29f558699c69Step 8/17 : RUN dotnet restore &quot;Web/Web.csproj&quot; ---&gt; Running in 31e08f04df6c Restore completed in 1.4 min for /src/Web/Web.csproj.Removing intermediate container 31e08f04df6c ---&gt; ed0f20f4056aStep 9/17 : COPY . . ---&gt; 1999712c7eceStep 10/17 : WORKDIR &quot;/src/Web&quot; ---&gt; Running in 6d81af8f98dcRemoving intermediate container 6d81af8f98dc ---&gt; b31d42182d2aStep 11/17 : RUN dotnet build &quot;Web.csproj&quot; -c Release -o /app/build ---&gt; Running in b2c16188ce9aMicrosoft (R) Build Engine version 16.3.2+e481bbf88 for .NET CoreCopyright (C) Microsoft Corporation. All rights reserved. Restore completed in 29.82 ms for /src/Web/Web.csproj. Web -&gt; /app/build/Web.dllBuild succeeded. 0 Warning(s) 0 Error(s)Time Elapsed 00:00:04.89Removing intermediate container b2c16188ce9a ---&gt; b249fe8e976bStep 12/17 : FROM build AS publish ---&gt; b249fe8e976bStep 13/17 : RUN dotnet publish &quot;Web.csproj&quot; -c Release -o /app/publish ---&gt; Running in 43766cf2eb2dMicrosoft (R) Build Engine version 16.3.2+e481bbf88 for .NET CoreCopyright (C) Microsoft Corporation. All rights reserved. Restore completed in 35.91 ms for /src/Web/Web.csproj. Web -&gt; /src/Web/bin/Release/netcoreapp3.0/Web.dll Web -&gt; /app/publish/Removing intermediate container 43766cf2eb2d ---&gt; e07c5fc08616Step 14/17 : FROM base AS final ---&gt; a157473c89eeStep 15/17 : WORKDIR /app ---&gt; Running in 3ab7aca5e289Removing intermediate container 3ab7aca5e289 ---&gt; 0b7196de9a3fStep 16/17 : COPY --from=publish /app/publish . ---&gt; 6178981834aeStep 17/17 : ENTRYPOINT [&quot;dotnet&quot;, &quot;Web.dll&quot;] ---&gt; Running in 488f804c2854Removing intermediate container 488f804c2854 ---&gt; 5aa2ed7e047fSuccessfully built 5aa2ed7e047fSuccessfully tagged core3.x-swagger:latest 生成之后便可以查询到我们的镜像文件: 123[root@VM_175_142_centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcore3.x-swagger latest 5aa2ed7e047f 2 minutes ago 237MB 我们可以运行我们的image镜像文件了： 12# 将容器80端口映射到宿主机的9999端口docker run --name swagger -d -p 9999:80 core3.x-swagger 但是当我访问的时候并没有给我惊喜，而是令人头皮发麻的服务端500错误,那么我们就去看看容器的日志吧: 1docker logs 083d0c79c078 错误日志如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980info: Microsoft.Hosting.Lifetime[0] Now listening on: http://[::]:80info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down.info: Microsoft.Hosting.Lifetime[0] Hosting environment: Productioninfo: Microsoft.Hosting.Lifetime[0] Content root path: /appfail: Microsoft.AspNetCore.Server.Kestrel[13] Connection id &quot;0HLRJSUAI4BAT&quot;, Request id &quot;0HLRJSUAI4BAT:00000001&quot;: An unhandled exception was thrown by the application.System.IO.FileNotFoundException: Could not find file &apos;/app/Web.xml&apos;.File name: &apos;/app/Web.xml&apos; at Interop.ThrowExceptionForIoErrno(ErrorInfo errorInfo, String path, Boolean isDirectory, Func`2 errorRewriter) at Microsoft.Win32.SafeHandles.SafeFileHandle.Open(String path, OpenFlags flags, Int32 mode) at System.IO.FileStream.OpenHandle(FileMode mode, FileShare share, FileOptions options) at System.IO.FileStream..ctor(String path, FileMode mode, FileAccess access, FileShare share, Int32 bufferSize, FileOptions options) at System.IO.FileStream..ctor(String path, FileMode mode, FileAccess access, FileShare share, Int32 bufferSize) at System.Xml.XmlDownloadManager.GetStream(Uri uri, ICredentials credentials, IWebProxy proxy, RequestCachePolicy cachePolicy) at System.Xml.XmlUrlResolver.GetEntity(Uri absoluteUri, String role, Type ofObjectToReturn) at System.Xml.XmlTextReaderImpl.OpenUrl() at System.Xml.XmlTextReaderImpl.Read() at System.Xml.XPath.XPathDocument.LoadFromReader(XmlReader reader, XmlSpace space) at System.Xml.XPath.XPathDocument..ctor(String uri, XmlSpace space) at System.Xml.XPath.XPathDocument..ctor(String uri) at Microsoft.Extensions.DependencyInjection.SwaggerGenOptionsExtensions.&lt;&gt;c__DisplayClass23_0.&lt;IncludeXmlComments&gt;b__0() at Microsoft.Extensions.DependencyInjection.SwaggerGenOptionsExtensions.IncludeXmlComments(SwaggerGenOptions swaggerGenOptions, Func`1 xmlDocFactory, Boolean includeControllerXmlComments) at Microsoft.Extensions.DependencyInjection.SwaggerGenOptionsExtensions.IncludeXmlComments(SwaggerGenOptions swaggerGenOptions, String filePath, Boolean includeControllerXmlComments) at Microsoft.Extensions.Options.OptionsFactory`1.Create(String name) at Microsoft.Extensions.Options.OptionsManager`1.&lt;&gt;c__DisplayClass5_0.&lt;Get&gt;b__0() at System.Lazy`1.ViaFactory(LazyThreadSafetyMode mode) at System.Lazy`1.ExecutionAndPublication(LazyHelper executionAndPublication, Boolean useDefaultConstructor) at System.Lazy`1.CreateValue() at System.Lazy`1.get_Value() at Microsoft.Extensions.Options.OptionsCache`1.GetOrAdd(String name, Func`1 createOptions) at Microsoft.Extensions.Options.OptionsManager`1.Get(String name) at Microsoft.Extensions.Options.OptionsManager`1.get_Value() at Swashbuckle.AspNetCore.SwaggerGen.ConfigureSchemaGeneratorOptions..ctor(IServiceProvider serviceProvider, IOptions`1 swaggerGenOptionsAccessor) at System.RuntimeMethodHandle.InvokeMethod(Object target, Object[] arguments, Signature sig, Boolean constructor, Boolean wrapExceptions) at System.Reflection.RuntimeConstructorInfo.Invoke(BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitDisposeCache(ServiceCallSite transientCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitIEnumerable(IEnumerableCallSite enumerableCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitNoCache(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitDisposeCache(ServiceCallSite transientCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitCache(ServiceCallSite callSite, RuntimeResolverContext context, ServiceProviderEngineScope serviceProviderEngine, RuntimeResolverLock lockType) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite singletonCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitDisposeCache(ServiceCallSite transientCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitDisposeCache(ServiceCallSite transientCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitDisposeCache(ServiceCallSite transientCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope) at Microsoft.Extensions.DependencyInjection.ServiceLookup.DynamicServiceProviderEngine.&lt;&gt;c__DisplayClass1_0.&lt;RealizeService&gt;b__0(ServiceProviderEngineScope scope) at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngine.GetService(Type serviceType, ServiceProviderEngineScope serviceProviderEngineScope) at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngineScope.GetService(Type serviceType) at Microsoft.AspNetCore.Builder.UseMiddlewareExtensions.GetService(IServiceProvider sp, Type type, Type middleware) at lambda_method(Closure , Object , HttpContext , IServiceProvider ) at Microsoft.AspNetCore.Builder.UseMiddlewareExtensions.&lt;&gt;c__DisplayClass4_1.&lt;UseMiddleware&gt;b__2(HttpContext context) at Microsoft.AspNetCore.Mvc.Versioning.ApiVersioningMiddleware.InvokeAsync(HttpContext context) at Microsoft.AspNetCore.HostFiltering.HostFilteringMiddleware.Invoke(HttpContext context) at Microsoft.AspNetCore.Hosting.HostingApplication.ProcessRequestAsync(Context context) at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http.HttpProtocol.ProcessRequests[TContext](IHttpApplication`1 application)fail: Microsoft.AspNetCore.Server.Kestrel[13] 很明显，是因为publish的时候没有把我们的Web.xml文件复制进去,那么我们在Dockerfile中添加一条 1COPY [&quot;Web/Web.xml&quot;, &quot;.&quot;] 在把之前是容器删除掉: 1docker rm 083d0c79c078 然后重新build一次 1docker build -t core3.x-swagger -f Dockerfile . 重启启动容器,很好，是想要的结果: 123[root@VM_175_142_centos Swagger.Demo]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb475841bece6 core3.x-swagger &quot;dotnet Web.dll&quot; 3 minutes ago Up 3 minutes 443/tcp, 0.0.0.0:9999-&gt;80/tcp swagger 后面，我们尝试使用腾讯云容器服务（Tencent Kubernetes Engine ，TKE）或者阿里容器服务 Kubernetes 版（简称 ACK）来部署我们的Docker镜像","link":"/posts/4207309142.html"},{"title":"一个开源的高性能对象存储系统-MinIO","text":"我们经常会遇到文件存储的场景，一般避免怎么运维成本，可能会选择成熟稳定且性价比高的产品，国内有很多OSS产品，比如:阿里、腾讯、七牛、青云等都有自己的对象存储产品，最终我选择了七牛，因为它0-10GB空间免费(我把我的图片还有我的所有静态网页都放在上面)，而且提供的API也比较丰富，青云也有12个月30 GB免费政策，其他的就没看过，因为要钱嘛，我是能省则省。 其实在github也有多款热门的对象存储系统，如:FastDFS、HDFS、Ceph等，这次也是无意见看到了基于Minio和Thumbor搭建独立图片服务这篇文章，所以也想着自己搭建一个类似于七牛云的私有对象存储空间,那么首先我们要先基本了解一下MinIO和thumbor的用途: MinIO,是与Amazon S3云存储服务API兼容的高性能对象存储系统 thumbor,是一个智能图像服务。它可以按需裁剪、调整大小和翻转图像 基于Docker搭建MinIO对象存储服务器基于Docker来安装MinIO非常方便一句命令就OK了。 1234567891011121314151617181920[root@instance-p0a4erj8 ~]# docker run -p 19000:9000 minio/minio server /minio_dataUnable to find image &apos;minio/minio:latest&apos; locallylatest: Pulling from minio/minio89d9c30c1d48: Pull complete 4795054645d0: Pull complete 4f7991ccecfe: Pull complete Digest: sha256:53c65fca691336b77b74180937d8d6f7845ef1afea0e0ae9780598835d7a5979Status: Downloaded newer image for minio/minio:latestEndpoint: http://172.17.0.5:9000 http://127.0.0.1:9000Browser Access: http://172.17.0.5:9000 http://127.0.0.1:9000Object API (Amazon S3 compatible): Go: https://docs.min.io/docs/golang-client-quickstart-guide Java: https://docs.min.io/docs/java-client-quickstart-guide Python: https://docs.min.io/docs/python-client-quickstart-guide JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide .NET: https://docs.min.io/docs/dotnet-client-quickstart-guideDetected default credentials &apos;minioadmin:minioadmin&apos;, please change the credentials immediately using &apos;MINIO_ACCESS_KEY&apos; and &apos;MINIO_SECRET_KEY&apos; 当然这不是后台运行方式,所以得加一个参数docker run -d -p 19000:9000 minio/minio server /minio_data,但是这时候是看不到日志的，只有通过docker logs [CONTAINER ID]来查看,我们先通过docker ps -a查看运行起的容器: 123[root@instance-p0a4erj8 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9118cbd8c05d minio/minio &quot;/usr/bin/docker-ent…&quot; 29 seconds ago Up 27 seconds 0.0.0.0:19000-&gt;9000/tcp 再docker logs 9118cbd8c05d即可看到MinIO启动时的日志： 12345678910111213[root@instance-p0a4erj8 ~]# docker logs 9118cbd8c05dEndpoint: http://172.17.0.5:9000 http://127.0.0.1:9000Browser Access: http://172.17.0.5:9000 http://127.0.0.1:9000Object API (Amazon S3 compatible): Go: https://docs.min.io/docs/golang-client-quickstart-guide Java: https://docs.min.io/docs/java-client-quickstart-guide Python: https://docs.min.io/docs/python-client-quickstart-guide JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide .NET: https://docs.min.io/docs/dotnet-client-quickstart-guideDetected default credentials &apos;minioadmin:minioadmin&apos;, please change the credentials immediately using &apos;MINIO_ACCESS_KEY&apos; and &apos;MINIO_SECRET_KEY&apos; 我们打开浏览器访问我们刚才映射的端口19000,使用默认的用户名密码minioadmin:minioadmin来登录我们的MinIO Web文件系统.然上传一个文件：我们可以复制图片链接到其他浏览器访问.至此，我们单机版的的S3对象存储服务器就搭建好了，由于设备有限，集群版就不弄了，在此附上其他人的经验: minio 集群搭建 MinIO 分布式集群搭建 基于Docker搭建thumbor图片处理服务跟上面的MinIO类似，直接启动docker即可： 123456789101112131415161718192021222324252627[root@instance-p0a4erj8 ~]# docker run -d -p 1800:80 minimalcompact/thumborUnable to find image &apos;minimalcompact/thumbor:latest&apos; locallylatest: Pulling from minimalcompact/thumbor16ea0e8c8879: Pull complete 50024b0106d5: Pull complete ff95660c6937: Pull complete 9c7d0e5c0bc2: Pull complete 29c4fb388fdf: Pull complete 02ced85d1576: Pull complete dfe2cd30976f: Pull complete d3ec517ed16e: Pull complete ed6982ebc0cc: Pull complete 7a410d29bed3: Pull complete 1c062599a579: Pull complete 2b6e7f7d0ea8: Pull complete c47dad1e5ead: Pull complete 1eea835ac597: Pull complete 77558b8bbf29: Pull complete 156cf949b8be: Pull complete bb3b169ae09d: Pull complete a296ec13d049: Pull complete 4c03d2a2c52d: Pull complete Digest: sha256:f888ed733ad57414289a6e2072b8b210ee3a18af5206d60d34619ee265e6df80Status: Downloaded newer image for minimalcompact/thumbor:latestc062f3a72259ec0835214a4da11532823cb2257827bb80c008150a4fcafb45bb[root@instance-p0a4erj8 ~]# docker logs c062f3a72259---&gt; Starting thumbor solo... 至此，我们的thumbor图片处理服务搭建好了，我们来试试它的各项功能吧. thumbor图片处理首先我们先看看原图这样才能感受到thumbor给我们带来的变化。这里我们直接从thumbor官网教程可以知道,使用http://localhost:8888/unsafe/300x200/图片地址的方式访问就可以实现图片裁剪 使用http://localhost:8888/unsafe/-0x-0/图片地址可以实现图片翻转其功能还是比较多，其他的功能请参照Getting Started 一般情况下，MinIO创建的bucket是私有的，只有通过分享链接的方式访问，但是thumbor无法直接访问MinIO分享的私有文件,会提示无法连接到服务,那么我们怎么才能访问呢?有两种方式: 把bucket变更为public; 配置thumbor的LOADER,在thumbor社区已经为我们提供了多种插件,如:thumbor_aws、thumbor_hbase、thumbor_mongodb等，由于MinIO首先了S3的API，所以需要thumbor_aws插件，并配置tc_aws.loaders.s3_loader。不过今天我们还是用简单点的方式把–把bucket变更为public。这里我们要么下载一个MinIO Client，但是下载Windows客户端是真的慢啊,所以我选择了Docker方式:1234567891011121314151617181920212223242526272829docker run -it --entrypoint=/bin/sh minio/mc/ # mc config host lsgcs URL : https://storage.googleapis.com AccessKey : YOUR-ACCESS-KEY-HERE SecretKey : YOUR-SECRET-KEY-HERE API : S3v2 Lookup : dnslocal URL : http://localhost:9000 AccessKey : SecretKey : API : Lookup : autoplay URL : https://play.min.io AccessKey : Q3AM3UQ867SPQQA43P2F SecretKey : zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG API : S3v4 Lookup : autos3 URL : https://s3.amazonaws.com AccessKey : YOUR-ACCESS-KEY-HERE SecretKey : YOUR-SECRET-KEY-HERE API : S3v4 Lookup : dns 然后新增一个12/ # mc config host add local http://xxx.xx.xx.xx:9000 minioadmin minioadminAdded `local` successfully. 这里我新建一个bucket:12/ # mc mb local/publicbucketBucket created successfully `local/publicbucket`. 把它变为public：12/ # mc policy set public local/publicbucketAccess permission for `local/publicbucket` is set to `public` 现在我上传一张图片,后即可直接访问。 小结这次只是简单的采用Docker安装了thumbor和MinIO来实现类似于七牛云的图片存储、处理服务器。也只是大概了体验了它们强大的功能，为以后再项目中实践奠定基础。后面有时间，我会针对thumbor和MinIO进行伪集群部署(条件有限)。","link":"/posts/3832913427.html"},{"title":".Net Core 3.x 使用Autofac替换默认Ioc容器","text":".net 中的IOC容器也不少，如Autofac、Windsor Castle、Spring.NET、Unity、Ninject等，现在使用Autofac作为IOC容器的较多,据说速度是最快的一个。 那么我们有必要将其应用到我们的项目中，来体验其带给我们的极速快感。 安装Autofac依赖我们需要通过nuget包管理安装两个包: 12AutofacAutofac.Extensions.DependencyInjection 使用Autofac包在Autofac中有多种生命周期: Instance Per Dependency 对于一个服务每次请求都会返回一个唯一的实例. Single Instance 所有的请求都将会返回同一个实例. Instance Per Lifetime Scope 每个生命周期作用域的组件在每个嵌套的生命周期作用域中最多只会有一个单一实例 Instance Per Matching Lifetime Scope 每个匹配生命周期作用域的组件在每个名称匹配的嵌套生命周期作用域中最多只会有一个单一实例. Instance Per Request 每个请求一个实例建立于每个匹配生命周期一个实例之上 Instance Per Owned Owned 隐式关系类型 创建了一个嵌套的生命周期作用域. 使用每次被拥有一个实例注册, 可以把该依赖的作用域绑定到拥有它的实例上. Thread Scope 每个线程就有了它各自的生命周期作用域,在这种多线程场景中, 你必须得注意父级作用域不能在派生出的线程下被释放了 新建一个ITransientDependency空接口123public interface ITransientDependency { } 这里的主要目的是为了标识接口注册不同的声明周期。你们可以根据自己的情况新增不同的标识注入不同的生命周期 新建一个WebModule类继承Autofac的Module1234567891011121314151617public class WebModule : Module { protected override void Load(ContainerBuilder builder) { //获取当前程序集 var dataAccess = Reflection.Assembly.GetExecutingAssembly(); var transientDependencyType = typeof(ITransientDependency); //查找ITransientDependency接口类型程序并注册为每次调用创建一个实例 builder.RegisterAssemblyTypes(dataAccess) .Where(t =&gt; transientDependencyType.IsAssignableFrom(t) &amp;&amp; t != transientDependencyType) .AsImplementedInterfaces().InstancePerLifetimeScope().PropertiesAutowired(); } } 主要是查找程序集，批量注册相应依赖， 配置使用在Program中稍作修改 1234567public static IHostBuilder CreateHostBuilder(string[] args) =&gt; Host.CreateDefaultBuilder(args) .UseServiceProviderFactory(new AutofacServiceProviderFactory()) .ConfigureWebHostDefaults(webBuilder =&gt; { webBuilder.UseStartup&lt;Startup&gt;(); }); 在Startup文件中加入ConfigureContainer方法: 1234public void ConfigureContainer(ContainerBuilder builder) { builder.RegisterModule(new WebModule()); } 至此，我们对Autofac已经配置完成 示例接下来我们新建一个IExampleService示例接口继承ITransientDependency, 1234public interface IExampleService: ITransientDependency { string GetName(string Name); } 新建IExampleService接口实现类: 1234567public class ExampleService : IExampleService { public string GetName(string Name) { return $&quot;My name is {Name}&quot;; } } 新建一个Controller,并注入IExampleService: 123456789101112131415161718192021222324252627namespace Web.Controllers.v1{ [ApiVersion(&quot;1.0&quot;)] [Route(&quot;api/v{version:apiVersion}/[controller]&quot;)] [ApiController] public class ExampleController : ControllerBase { IExampleService _exampleService; public ExampleController(IExampleService exampleService) { _exampleService = exampleService; } /// &lt;summary&gt; /// 查询名字 /// &lt;/summary&gt; /// &lt;param name=&quot;name&quot;&gt;&lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; [HttpGet] [Route(&quot;GetName/{Name}&quot;)] public string GetName(string Name) { return _exampleService.GetName(Name); } }} 参考https://autofaccn.readthedocs.io/zh/latest/https://docs.autofac.org/en/latest/","link":"/posts/1628413452.html"},{"title":".net core 3.x Web API使用Swagger添加多版本以及JWT Authorization","text":"很多时候，要给我们的接口升级，但是又怕影响之前的接口业务或者是要开发特定的接口，我们会给接口加上一个版本号，保障老接口依然能稳定运行，例如百度的坐标转换服务 1http://api.map.baidu.com/geoconv/v1/?coords=114.21892734521,29.575429778924&amp;from=1&amp;to=5&amp;ak=你的密钥 //GET请求 在.net core中，想给接口加上版本号，有多种方式: 1、通过URL Path Segment来实现; 2、通过HTTP Headers来实现; 3、通过QueryString来实现;具体的实现可以去百度或者bing。这里暂时只讲swagger doc实现多版本切换。我个人喜欢使用第一种方式。 ApiExplorer组件的使用在实现多版本Web API，我们需要借助一个组件: 1Microsoft.AspNetCore.Mvc.Versioning.ApiExplorer 成功添加依赖包后，就可以在程序中使用相应的API了，首先我们先创建如下的目录结构(大家可随意):接下来，就将我们之前的Controller类上添加相应的特性: 1234//当前Controller的版本号[ApiVersion(&quot;1.0&quot;)]//接口访问路径[Route(&quot;api/v{version:apiVersion}/[controller]&quot;)] 然后在Startup的ConfigureServices中使用ApiExplorer 12services.AddApiVersioning();services.AddVersionedApiExplorer(options =&gt; options.GroupNameFormat = &quot;&apos;v&apos;VVV&quot;); Swagger中多版本API文档下面，我就用Swagger来自动生成多版本文档，为了使我的Startup类相对简洁，我们新建一个ConfigureSwaggerOptions类来放置Swagger的相关配置,完整代码如下： 1234567891011121314151617181920212223242526272829public class ConfigureSwaggerOptions : IConfigureOptions&lt;SwaggerGenOptions&gt; { readonly IApiVersionDescriptionProvider provider; public ConfigureSwaggerOptions(IApiVersionDescriptionProvider provider) =&gt; this.provider = provider; public void Configure(SwaggerGenOptions options) { foreach (var description in provider.ApiVersionDescriptions) { options.SwaggerDoc(description.GroupName, new OpenApiInfo { Description = $&quot;Demo API {description.ApiVersion} Description&quot;, Version = description.ApiVersion.ToString(), Title = $&quot;Demo API 文档{description.ApiVersion}&quot;, Contact = new OpenApiContact() { Name = &quot;eyiadmin&quot;, Email = &quot;188781475@qq.com&quot; }, License = new OpenApiLicense { Name = &quot;Apache 2.0&quot;, Url = new Uri(&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;) } }); } var docXmlPath = Path.Combine(AppContext.BaseDirectory, &quot;Web.xml&quot;); options.IncludeXmlComments(docXmlPath); } } 修改Startup类ConfigureServices中: 12services.AddTransient&lt;IConfigureOptions&lt;SwaggerGenOptions&gt;, ConfigureSwaggerOptions&gt;();services.AddSwaggerGen(); Configure中 1234567891011121314151617181920212223242526272829public void Configure(IApplicationBuilder app, IWebHostEnvironment env, IApiVersionDescriptionProvider provider) { if (env.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(options =&gt; { foreach (var description in provider.ApiVersionDescriptions) { options.SwaggerEndpoint( $&quot;/swagger/{description.GroupName}/swagger.json&quot;, description.GroupName.ToUpperInvariant()); } }); app.UseDeveloperExceptionPage(); } app.UseHttpsRedirection(); app.UseRouting(); app.UseAuthorization(); app.UseEndpoints(endpoints =&gt; { endpoints.MapControllers(); }); } 最终效果如下：可以点击Select a definition切换我们不同版本的doc 在Swagger中添加Header Authorization 一般情况下，我们的Web API是使用Jwt来保障接口安全，当然还有很多其他的方式，我只是选择一种相对简单一些的方式。这里只是说怎么在Swagger调试的时候带上我们的Jwt Token，具体的Jwt实现，也请自己实现。 其实Swashbuckle已经给我们提供了很方便的API，只需要配置一下即可: 123456789101112131415161718options.AddSecurityDefinition(&quot;bearer&quot;, new OpenApiSecurityScheme { Type = SecuritySchemeType.Http, In = ParameterLocation.Header, Name = &quot;Authorization&quot;, Scheme = &quot;bearer&quot;, BearerFormat = &quot;JWT&quot;, Description = &quot;JWT Authorization header using the Bearer scheme.&quot;, }); var req = new OpenApiSecurityRequirement(); req.Add(new OpenApiSecurityScheme { Reference = new OpenApiReference { Type = ReferenceType.SecurityScheme, Id = &quot;bearer&quot; } }, new[] { &quot;&quot; }); options.AddSecurityRequirement(req); 效果如下:我们会看到有一个Authorization的按钮，我们点击可以输入我们获取到的Jwt Token,在文本框内不需要输入Bearer关键字，Swagger会自动为我们加上Bearer关键字。在这里也需要注意到Swagger的坑，因为大写的问题可能会导致Authorization无法添加到request请求头中.下面是Swagger的build-request.js 123456789101112131415161718else if (type === &apos;http&apos;) { if (schema.scheme === &apos;basic&apos;) { let scheme = schema.scheme; if (scheme) { scheme = scheme.toLowerCase() } if (scheme === &apos;basic&apos;) { const {username, password} = value const encoded = btoa(`${username}:${password}`) result.headers.Authorization = `Basic ${encoded}` } if (schema.scheme === &apos;bearer&apos;) { if (scheme === &apos;bearer&apos;) { result.headers.Authorization = `Bearer ${value}` } } 大家可以自己去看看代码，我继续说Authorization的设置，点击按钮就会出现设置token的文本框:设置完后，我们的小锁会变为关闭状态:最后就可以通过Swagger UI调用我们的接口，就可以在我们的请求中看到Authorization的参数内容，后端就可以接受处理.当然，Swagger还有其他的自定义UI功能，大家可以去官网查询相关文档。","link":"/posts/1196750307.html"},{"title":".net core 3.x使用Swagger","text":".net core3.0问世已经两个多月了，我并没有急着将生产上的项目升级到3.0，因为怕踩坑，这不3.1马上就要出来了，想着core3.x逐渐稳定，所以就开始来琢磨一下，此次就简单是说一下.net core3.0中使丝袜哥(Swagger)生成API文档。 在这里，我使用的开发工具是VS2019,Swashbuckle.AspNetCore 5.0.0-rc4， 安装Swashbuckle.AspNetCore1、首先新建一个Web API项目2、安装swagger,打开程序包管理器控制台执行： 1Install-Package Swashbuckle.AspNetCore -Version 5.0.0-rc4 引用Swagger1、在Startup.cs中引入命名空间 12using Microsoft.OpenApi.Models;using Swashbuckle.AspNetCore.Swagger; 2、在ConfigureServices中添加Swagger 1234services.AddSwaggerGen(swagger =&gt; { swagger.SwaggerDoc(&quot;v1&quot;, new OpenApiInfo { Description = &quot;Demo API Description&quot;, Version = &quot;v1.0&quot;, Title = &quot;Demo API&quot; }); }); 3、在Configure中添加Swagger中间件,我们最好放在env.IsDevelopment()中 123456789if (env.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(c =&gt; { c.SwaggerEndpoint(&quot;/swagger/v1/swagger.json&quot;, &quot;Demo API&quot;); }); app.UseDeveloperExceptionPage(); } 4、配置完后，运行一下，在浏览器中输入http://localhost:5000/swagger, 看看效果： 为Api添加注释刚才的配置，只是能看到我们的API方法，并且可以发送调试请求，但是没有注释，接下来，我们就给API加上注释1、在想要添加注释的API对应分方法上添加注释,格式如下： 12345678910111213141516/// &lt;summary&gt; /// 获取天气预报信息 /// &lt;/summary&gt; /// &lt;returns&gt;&lt;/returns&gt; [HttpGet] public IEnumerable&lt;WeatherForecast&gt; Get() { var rng = new Random(); return Enumerable.Range(1, 5).Select(index =&gt; new WeatherForecast { Date = DateTime.Now.AddDays(index), TemperatureC = rng.Next(-20, 55), Summary = Summaries[rng.Next(Summaries.Length)] }) .ToArray(); } 2、在项目属性选项卡中，设置生成中的XML文档输出目录:3、在AddSwaggerGen中添加xml文件路径: 123456services.AddSwaggerGen(swagger =&gt; { swagger.SwaggerDoc(&quot;v1&quot;, new OpenApiInfo { Description = &quot;Demo API Description&quot;, Version = &quot;v1.0&quot;, Title = &quot;Demo API&quot;, Contact = new OpenApiContact() { Name = &quot;eyiadmin&quot;, Email = &quot;188781475@qq.com&quot; } }); var docXmlPath = Path.Combine(AppContext.BaseDirectory, &quot;Web.xml&quot;); swagger.IncludeXmlComments(docXmlPath); }); 4、最终效果: 每次启动都会打开默认的IE浏览器，由于个人喜欢Google浏览器，所以，现在修改vs启动的默认浏览器：另外，每次启动，在浏览中打开的是默认的Controller，我们可以设置一下启动参数，让每次启动都是Swagger目录，就不需要我们每次都手动输入Swagger目录：。 这次写的是Swagger最基本的设置,后面我们会继续学习一些常用的高级功能。","link":"/posts/685970588.html"},{"title":"shell脚本学习","text":"Shell 脚本（shell script），是一种为 shell 编写的脚本程序。业界所说的 shell 通常都是指 shell 脚本，但读者朋友要知道，shell 和 shell script 是两个不同的概念。由于习惯的原因，简洁起见，本文出现的 “shell编程” 都是指 shell 脚本编程，不是指开发 shell 自身。 以上内容来源于runoob 上半年花了一个多月时间修改一个开源的BI项目(Redash),改完后,由于没有使用也就暂时告一段落，就在昨天，一个客户说想找一个开源的BI系统，我们就给他推荐这个工具，因为它简单、方便、快捷，而且也比较受欢迎。但是客户对Docker不熟悉，就给他大致的讲了一个怎么在 Windows搭建一个测试环境。后来，突然想看看Redash的Dockerfile文件，看到里面的执行文件： 12ENTRYPOINT [&quot;/app/bin/docker-entrypoint&quot;]CMD [&quot;server&quot;] docker-entrypoint是一个shell脚本，但是因为自己对shell的使用还未入门，所以就着这个机会，去简单了解一下。我们先看一下docker-entrypoint中的代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167#!/bin/bashset -ecelery_worker() { WORKERS_COUNT=${WORKERS_COUNT:-2} QUEUES=${QUEUES:-queries,scheduled_queries} WORKER_EXTRA_OPTIONS=${WORKER_EXTRA_OPTIONS:-} echo &quot;Starting $WORKERS_COUNT workers for queues: $QUEUES...&quot; exec /usr/local/bin/celery worker --app=redash.worker -c$WORKERS_COUNT -Q$QUEUES -linfo --max-tasks-per-child=10 -Ofair $WORKER_EXTRA_OPTIONS}scheduler() { echo &quot;Starting RQ scheduler...&quot; exec /app/manage.py rq scheduler}dev_scheduler() { echo &quot;Starting dev RQ scheduler...&quot; exec watchmedo auto-restart --directory=./redash/ --pattern=*.py --recursive -- ./manage.py rq scheduler}worker() { echo &quot;Starting RQ worker...&quot; exec /app/manage.py rq worker $QUEUES}dev_worker() { echo &quot;Starting dev RQ worker...&quot; exec watchmedo auto-restart --directory=./redash/ --pattern=*.py --recursive -- ./manage.py rq worker $QUEUES}dev_celery_worker() { WORKERS_COUNT=${WORKERS_COUNT:-2} QUEUES=${QUEUES:-queries,scheduled_queries} echo &quot;Starting $WORKERS_COUNT workers for queues: $QUEUES...&quot; exec watchmedo auto-restart --directory=./redash/ --pattern=*.py --recursive -- /usr/local/bin/celery worker --app=redash.worker -c$WORKERS_COUNT -Q$QUEUES -linfo --max-tasks-per-child=10 -Ofair}server() { # Recycle gunicorn workers every n-th request. See http://docs.gunicorn.org/en/stable/settings.html#max-requests for more details. MAX_REQUESTS=${MAX_REQUESTS:-1000} MAX_REQUESTS_JITTER=${MAX_REQUESTS_JITTER:-100} exec /usr/local/bin/gunicorn -b 0.0.0.0:5000 --name redash -w${REDASH_WEB_WORKERS:-4} redash.wsgi:app --max-requests $MAX_REQUESTS --max-requests-jitter $MAX_REQUESTS_JITTER}create_db() { exec /app/manage.py database create_tables}celery_healthcheck() { exec /usr/local/bin/celery inspect ping --app=redash.worker -d celery@$HOSTNAME}rq_healthcheck() { exec /app/manage.py rq healthcheck}help() { echo &quot;Redash Docker.&quot; echo &quot;&quot; echo &quot;Usage:&quot; echo &quot;&quot; echo &quot;server -- start Redash server (with gunicorn)&quot; echo &quot;celery_worker -- start Celery worker&quot; echo &quot;dev_celery_worker -- start Celery worker process which picks up code changes and reloads&quot; echo &quot;worker -- start a single RQ worker&quot; echo &quot;dev_worker -- start a single RQ worker with code reloading&quot; echo &quot;scheduler -- start an rq-scheduler instance&quot; echo &quot;dev_scheduler -- start an rq-scheduler instance with code reloading&quot; echo &quot;celery_healthcheck -- runs a Celery healthcheck. Useful for Docker&apos;s HEALTHCHECK mechanism.&quot; echo &quot;rq_healthcheck -- runs a RQ healthcheck that verifies that all local workers are active. Useful for Docker&apos;s HEALTHCHECK mechanism.&quot; echo &quot;&quot; echo &quot;shell -- open shell&quot; echo &quot;dev_server -- start Flask development server with debugger and auto reload&quot; echo &quot;debug -- start Flask development server with remote debugger via ptvsd&quot; echo &quot;create_db -- create database tables&quot; echo &quot;manage -- CLI to manage redash&quot; echo &quot;tests -- run tests&quot;}tests() { export REDASH_DATABASE_URL=&quot;postgresql://postgres@postgres/tests&quot; if [ $# -eq 0 ]; then TEST_ARGS=tests/ else TEST_ARGS=$@ fi exec pytest $TEST_ARGS}case &quot;$1&quot; in worker) shift worker ;; server) shift server ;; scheduler) shift scheduler ;; dev_scheduler) shift dev_scheduler ;; celery_worker) shift celery_worker ;; dev_celery_worker) shift dev_celery_worker ;; dev_worker) shift dev_worker ;; rq_healthcheck) shift rq_healthcheck ;; celery_healthcheck) shift celery_healthcheck ;; dev_server) export FLASK_DEBUG=1 exec /app/manage.py runserver --debugger --reload -h 0.0.0.0 ;; debug) export FLASK_DEBUG=1 export REMOTE_DEBUG=1 exec /app/manage.py runserver --debugger --no-reload -h 0.0.0.0 ;; shell) exec /app/manage.py shell ;; create_db) create_db ;; manage) shift exec /app/manage.py $* ;; tests) shift tests $@ ;; help) shift help ;; *) exec &quot;$@&quot; ;;esac #!/bin/bash的作用我们可以再很多sh脚本里面看到#!/bin/bash这段代码，那么它有什么作用呢?shell是一种脚本命令语言，它有多种解析器，例如：/bin/csh、/bin/perl、/bin/bash、bin/sh等等，那么在在第一行加上#!/bin/bash，就是告诉系统这个脚本需要bin/bash解释器来执行。 set -e的作用set -e的作用是：当命令的返回值为非零状态时，则立即退出脚本的执行，防止导致一个致命的错误，而这些错误本应该在之前就被处理掉。set -e 命令用法总结如下： 当命令的返回值为非零状态时，则立即退出脚本的执行。 作用范围只限于脚本执行的当前进行，不作用于其创建的子进程（https://blog.csdn.net/fc34235/article/details/76598448 ）。 另外，当想根据命令执行的返回值，输出对应的log时，最好不要采用set -e选项，而是通过配合exit 命令来达到输出log并退出执行的目的。shell 中的 set -e 和 set +e的区别 shell 函数shell函数的定义格式如下： 12345[ function ] funname [()]{ action; [return int;]} 函数定义可以带function funname() 定义，也可以直接funname() 定义,不带任何参数。函数返回值可加return也可以不加，不加则以最后一条命令运行结果作为返回值。 12345678celery_worker() { WORKERS_COUNT=${WORKERS_COUNT:-2} QUEUES=${QUEUES:-queries,scheduled_queries} WORKER_EXTRA_OPTIONS=${WORKER_EXTRA_OPTIONS:-} echo &quot;Starting $WORKERS_COUNT workers for queues: $QUEUES...&quot; exec /usr/local/bin/celery worker --app=redash.worker -c$WORKERS_COUNT -Q$QUEUES -linfo --max-tasks-per-child=10 -Ofair $WORKER_EXTRA_OPTIONS} shell变量12# 如果WORKERS_COUNT未定义或者为空字符串，则返回默认值，否则返回WORKERS_COUNT的值WORKERS_COUNT=${WORKERS_COUNT:-2} 注意，变量名和等号之间不能有空格,而且还需要遵循如下规则： 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 使用一个定义过的变量，只要在变量名前面加美元符号即可。例如: 1$WORKERS_COUNT或者${WORKERS_COUNT} shell 参数我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：$n,n 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推实例以下实例我们向脚本传递三个参数，并分别输出，其中 $0 为执行的文件名： 123456789101112131415161718#!/bin/bash# author:菜鸟教程# url:www.runoob.comecho &quot;Shell 传递参数实例！&quot;;echo &quot;执行的文件名：$0&quot;;echo &quot;第一个参数为：$1&quot;;echo &quot;第二个参数为：$2&quot;;echo &quot;第三个参数为：$3&quot;;为脚本设置可执行权限，并执行脚本，输出结果如下所示：$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！执行的文件名：./test.sh第一个参数为：1第二个参数为：2第三个参数为：3 123456789$# 传递到脚本的参数个数$* 以一个单字符串显示所有向脚本传递的参数。如&quot;$*&quot;用「&quot;」括起来的情况、以&quot;$1 $2 … $n&quot;的形式输出所有参数。$$ 脚本运行的当前进程ID号$! 后台运行的最后一个进程的ID号$@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。如&quot;$@&quot;用「&quot;」括起来的情况、以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数。$- 显示Shell使用的当前选项，与set命令功能相同。$? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。 这里直接copy runoob的内容 shell case 用法12345678910111213case &quot;$1&quot; in worker) shift worker ;; help) shift help ;; *) exec &quot;$@&quot; ;;esac shift命令：在shell中，经常可能会遇到多参数传递，例如$1,$2,…$9，借助shift命令可以访问更多传递的参数case语句： 以case开头，esac结尾； 取值后面必须为单词in 匹配一个值与一个模式以”)”(右括号)结束 双分号 “;;” 表示命令序列结束； 默认模式使用”*)”表示，在不满足前面的模式后，执行默认模式后的命令示例:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748worker() { WORKERS_COUNT=${WORKERS_COUNT:-2} QUEUES=${QUEUES:-queries,scheduled_queries} WORKER_EXTRA_OPTIONS=${WORKER_EXTRA_OPTIONS:-} echo &quot;Starting $WORKERS_COUNT workers for queues: $QUEUES...&quot; echo &quot;$WORKERS_COUNT&quot; }help() { echo &quot;Redash Docker.&quot; echo &quot;&quot; echo &quot;Usage:&quot; echo &quot;&quot; echo &quot;server -- start Redash server (with gunicorn)&quot; echo &quot;celery_worker -- start Celery worker&quot; echo &quot;dev_celery_worker -- start Celery worker process which picks up code changes and reloads&quot; echo &quot;worker -- start a single RQ worker&quot; echo &quot;dev_worker -- start a single RQ worker with code reloading&quot; echo &quot;scheduler -- start an rq-scheduler instance&quot; echo &quot;dev_scheduler -- start an rq-scheduler instance with code reloading&quot; echo &quot;celery_healthcheck -- runs a Celery healthcheck. Useful for Docker&apos;s HEALTHCHECK mechanism.&quot; echo &quot;rq_healthcheck -- runs a RQ healthcheck that verifies that all local workers are active. Useful for Docker&apos;s HEALTHCHECK mechanism.&quot; echo &quot;&quot; echo &quot;shell -- open shell&quot; echo &quot;dev_server -- start Flask development server with debugger and auto reload&quot; echo &quot;debug -- start Flask development server with remote debugger via ptvsd&quot; echo &quot;create_db -- create database tables&quot; echo &quot;manage -- CLI to manage redash&quot; echo &quot;tests -- run tests&quot;}case &quot;$1&quot; in worker) shift worker ;; help) shift help ;; *) exec &quot;$@&quot; ;;esac 效果如下:123[root@VM_175_142_centos ~]# ./shell_echo.sh workerStarting 2 workers for queues: queries,scheduled_queries...2 结束这里只是借助redash的sh脚本来简单了解了一下shell的一些常用命令及语法，当然，shell的强大之处肯定不止于此，后面再遇到的时候，再来学习。大家也可以去runoob系统的学习一下","link":"/posts/4067385871.html"},{"title":"基于redis key失效机制实现状态实时更新","text":"基于redis key失效事件通知机制来处理状态实时更新 在我们的业务中，有这样一个场景，在手机端实时采集用用户经纬度，判断用户是否在某个场景(小区、商场等)内,如果再场景内则变更用户任务状态为”执行中”，当用户离开场景超过20分钟，需要将用户任务状态更改为”离场”状态。 一般，我们更新状态，要么定时去扫描数据库，要么就是触发某个事件，在最开始，有想到几种方案：1、定时（采用quartz定时执行作业，去扫描数据库）2、用hangfire、rabbitmq等实现延迟执行3、redis key失效事件，实时处理经过评估，最后选择了redis key失效机制来处理这个业务。 redis key失效事件监听redis自2.8之后就提供Keyspace Notifications功能，允许客户订阅Pub/Sub。 开启事件通知默认情况下，redis是没有开启事件通知的，所以我们需要手动配置： 12打开redis.conf配置文件搜索notify-keyspace-events，该配置默认是被注释掉的，需要将其修改为notify-keyspace-events Ex ,然后重启便可生效 值说明: 1234567891011# K 键空间通知，以__keyspace@&lt;db&gt;__为前缀# E 键事件通知，以__keysevent@&lt;db&gt;__为前缀# g del , expipre , rename 等类型无关的通用命令的通知, ...# $ String命令# l List命令# s Set命令# h Hash命令# z 有序集合命令# x 过期事件（每次key过期时生成）# e 驱逐事件（当key在内存满了被清除时生成）# A g$lshzxe的别名，因此”AKE”意味着所有的事件 spring boot实现消息监听器类pom.xml: 12345678910111213141516171819202122 &lt;!-- https://mvnrepository.com/artifact/redis.clients/jedis --&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970@Componentpublic class RedisMsgPubSubListener extends JedisPubSub { private Logger logger = LoggerFactory.getLogger(getClass()); @Autowired ITaskService taskService; @Override public void unsubscribe() { super.unsubscribe(); } @Override public void unsubscribe(String... channels) { super.unsubscribe(channels); } @Override public void subscribe(String... channels) { super.subscribe(channels); } @Override public void psubscribe(String... patterns) { super.psubscribe(patterns); } @Override public void punsubscribe() { super.punsubscribe(); } @Override public void punsubscribe(String... patterns) { super.punsubscribe(patterns); } @Override public void onMessage(String channel, String message) { System.out.println(&quot;channel:&quot; + channel + &quot;receives message :&quot; + message); String key = message; } @Override public void onPMessage(String pattern, String channel, String message) { } @Override public void onSubscribe(String channel, int subscribedChannels) { } @Override public void onPUnsubscribe(String pattern, int subscribedChannels) { } @Override public void onPSubscribe(String pattern, int subscribedChannels) { } @Override public void onUnsubscribe(String channel, int subscribedChannels) { }} 创建一个Runner： 123456789101112131415161718192021public class RedisApplicationRunner implements ApplicationRunner { @Autowired RedisMsgPubSubListener redisMsgPubSubListener; @Autowired JedisPool jedisPool; @Override public void run(ApplicationArguments applicationArguments) throws Exception { Jedis jedis = jedisPool.getResource();// jedis.set(&quot;zhcj:mobile:13548074395:2018122222212&quot;,&quot;1&quot;);// jedis.expire(&quot;zhcj:mobile:13548074395:2018122222212&quot;,10); jedis.subscribe(redisMsgPubSubListener, &quot;__keyevent@0__:expired&quot;); }} 最终效果:","link":"/posts/1222340544.html"},{"title":"使用Lombok简化java代码","text":"Lombok是一款提高javaer开发效率的插件工具，特别是在繁多是bean类中的getter、setter，使用Lombok可以使用注解的方式省去了添加getter、setter的时间 在使用Lombok之前，需要在IDE上安装插件，eclipse安装步骤稍微多一点，具体请自行百度或者去官方查看https://projectlombok.org/features/configuration，IntelliJ安装就相对来说比较方便，直接在插件中心搜索Lombok,安装Lombok Plugin即可。接下来，我们在程序中引入jar包： 1234567&lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 我们平时的POJO类一般都是这样写: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class DataFileDownloadAudit { private Timestamp startTime; private String userName; private String operator; private String source; private String displayName; private String filePath; private long fileSize; private String creator; private Timestamp createTime; public DataFileDownloadAudit() { } public String getUserName() { return this.userName; } public void setUserName(String userName) { this.userName = userName; } public String getOperator() { return this.operator; } public void setOperator(String operator) { this.operator = operator; } public String getSource() { return this.source; } public void setSource(String source) { this.source = source; } public String getDisplayName() { return this.displayName; } public void setDisplayName(String displayName) { this.displayName = displayName; } public String getFilePath() { return this.filePath; } public void setFilePath(String filePath) { this.filePath = filePath; } public String getCreator() { return this.creator; } public void setCreator(String creator) { this.creator = creator; } public Timestamp getStartTime() { return this.startTime; } public void setStartTime(Timestamp startTime) { this.startTime = startTime; } public Timestamp getCreateTime() { return this.createTime; } public void setCreateTime(Timestamp createTime) { this.createTime = createTime; } public long getFileSize() { return this.fileSize; } public void setFileSize(long fileSize) { this.fileSize = fileSize; }} 如果属性更多的话，添加getter、setter就需要花更多的时间。如果使用Lombok插件的话，不仅可以节省时间，而且代码也更加简洁: 1234567891011121314151617181920public class DataFileDownloadAudit { @Setter(AccessLevel.PUBLIC) @Getter(AccessLevel.PROTECTED) private Timestamp startTime; @Setter(AccessLevel.PUBLIC) @Getter(AccessLevel.PROTECTED) private String userName; private String operator; private String source; private String displayName; private String filePath; private long fileSize; private String creator; private Timestamp createTime; public DataFileDownloadAudit() { }} 这样只是给属性加getter、setter，我们也可以用@Data来继续简化: 123456789101112@Datapublic class DataFileDownloadAudit { private Timestamp startTime; private String userName; private String operator; private String source; private String displayName; private String filePath; private long fileSize; private String creator; private Timestamp createTime;} 这两种方式有什么差异呢？@Getter和@Setter是作用在bean属性上，可以自动生成getter、setter方法，@Data是作用在类上，他结合了@ToString, @EqualsAndHashCode, @Getter,@Setter,@RequiredArgsConstructor注解的功能。这仅仅说了Lombok两个常用的注解，总的来说，它会给我们带来更加简洁的代码和快速的开发效率，其他更加强大的功能可以去https://projectlombok.org/features/all自行查看。","link":"/posts/392357706.html"},{"title":"nginx搭建https服务","text":"nginx搭建https服务,转发到http后端服务 花了差不多5个小时用golang做了一个短域名服务+简单的微信小程序，在发布的时候需要用过https协议的服务，遂出此文 gin发布https因为是第一次用gin发布一个真实的环境，很多东西也不是很熟，所以只能百度，当时找到gin中间件把端口转换为https协议这篇文章，看了一下，很简单，就尝试着cv实现以下。由于的采用了gin在github提到的Graceful restart or stop功能，简单cv没有实现，就想了一个便捷的方式(毕竟菜嘛)–采用nginx代理一层。 采用nginx部署https安装步骤 123456789101112# 安装依赖yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel# 下载nginxwget http://nginx.org/download/nginx-1.17.4.tar.gz# 解压文件tar -zxvf nginx-1.17.4.tar.gz# 进入nginx源码目录cd nginx-1.17.4# 配置https模块./configure --prefix=/root/nginx --with-http_stub_status_module --with-http_ssl_module# 编译并安装make &amp;&amp; make install 如果上面的步骤没有异常，那么久算安装成功了，下面就开始配置https代理。 1、先把申请下来的证书复制到nginx目录下，我是新建了一个cert的文件目录，2、修改nginx.conf123456789101112131415161718192021server { listen 443 ssl; server_name domain; ssl_certificate /root/nginx/cert/1_domain_bundle.crt; #证书公钥 ssl_certificate_key /root/nginx/cert/2_domain.key; #证书私钥 ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDH:AESGCM:HIGH:!RC4:!DH:!MD5:!3DES:!aNULL:!eNULL; ssl_prefer_server_ciphers on; location / { proxy_pass http://0.0.0.0:80; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Cookie $http_cookie; #proxy_cookie_path chunked_transfer_encoding off; } } 3、进入到nginx目录执行sbin/nginx执行二进制文件启动nginx服务，如果修改了conf配置文件，则执行sbin/nginx -s reload 刷新配置。至此，nginx服务便搭建好了。在微信小程序开发设置里面配置好https的域名地址，就可以愉快的玩耍了。","link":"/posts/2874639355.html"},{"title":"微信小程序上传图片到七牛云","text":"微信小程序上传图片到七牛云,小程序Webview嵌入H5上传图片&amp;原生小程序上传图片 最近在帮朋友做微信小程序，没有选择mpvue，因为时间紧加上不熟悉，怕遇到坑不能快速处理，拖了进度，所以采用了原生小程序+webview的方式做了第一版。 小程序webview上传图片因为涉及到H5，所以图片上传这块就用到了微信中的jssdk， 第一步是wx.config配置前端代码如下: 123456789101112131415161718192021222324252627282930313233343536373839abp.services.app.wxAccess.getOfficialAccountJsdkConfig().done(function (data) { if (data) { var appId = data.appId; var timestamp = data.timestamp; var nonceStr = data.noncestr; var signature = data.signature; wx.config({ debug: false, //调试模式 当为tru时，开启调试模式 appId: appId, timestamp: timestamp.toString(), //签名时间戳 nonceStr: nonceStr, //生成签名的随机串 signature: signature, //签名 jsApiList: [&apos;chooseImage&apos;, &apos;uploadImage&apos;], success: function () { alert(&quot;配置成功&quot;); }, fail: function () { alert(&quot;配置失败&quot;); } }); wx.ready(function () { // 在这里调用 API wx.checkJsApi({ jsApiList: [ &apos;chooseImage&apos;, &apos;uploadImage&apos; ], success: function (res) { //console.log(JSON.stringify(res)); } }); }); wx.error(function(res){ alert(JSON.stringify(res)); }); } }); 后端代码： 1234567891011121314151617181920212223242526272829303132/// &lt;summary&gt;/// 获取公众号JsdkConfig/// &lt;/summary&gt;/// &lt;returns&gt;&lt;/returns&gt;public async Task&lt;Dtos.GetOfficialAccountJsdkConfigOutput&gt; GetOfficialAccountJsdkConfig() { var input = new Dtos.GetAccessInput { AppId = _appConfiguration[&quot;WechatOfficialAccount:AppId&quot;], Secret = _appConfiguration[&quot;WechatOfficialAccount:Secret&quot;] }; var noncestr = Guid.NewGuid().ToString(&quot;N&quot;); var jsapi = await GetJsapiTicket(input); //var timestamp = (DateTime.Now.Ticks - new DateTime(1970, 1, 1, 0, 0, 0, 0).Ticks) / 10000000; var timestamp = new Helper.UnixTime().DateTimeToUnix(DateTime.Now); //var url = Request.UrlReferrer.OriginalString; var url = _iHttpContextAccessor.HttpContext.Request.Headers[Microsoft.Net.Http.Headers.HeaderNames.Referer].ToString(); var shaStr = $&quot;jsapi_ticket={jsapi.Permit}&amp;noncestr={noncestr}&amp;timestamp={timestamp}&amp;url={url}&quot;; var signature = new Helper.Encrypt().Sha1Encrypt(shaStr); return new Dtos.GetOfficialAccountJsdkConfigOutput { AppId = input.AppId, Noncestr = noncestr, Timestamp = timestamp, Signature = signature }; } 第二步就是调用jssdk前端js代码如下： 123456789101112131415161718192021222324252627282930313233wx.chooseImage({ count: 9, needResult: 1, sizeType: [&apos;original&apos;, &apos;compressed&apos;], // 可以指定是原图还是压缩图，默认二者都有 sourceType: [&apos;album&apos;, &apos;camera&apos;], // 可以指定来源是相册还是相机，默认二者都有 success: function (data) { //localIds = data.localIds[0]; // 返回选定照片的本地ID列表，localId可以作为img标签的src属性显示图片 for (var localId = 0; localId &lt;= data.localIds.length - 1; localId++) { if (isIOS) { wx.getLocalImgData({ localId: data.localIds[localId], // 图片的localID success: function (res) { // var localData = res.localData; // localData是图片的base64数据，可以用img标签显示 //console.log(localData); } }); } else { } } }, fail: function (res) { alert(JSON.stringify(res)); //alterShowMessage(&quot;操作提示&quot;, JSON.stringify(res), &quot;1&quot;, &quot;确定&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;); } }); jssdk中，如果是iOS的话，前端无法直接使用localIds资源id做展示，需要调用wx.getLocalImgData方法来获取图片的base64编码 提交前端选择的图片到服务端并由服务的上传到七牛云前端部分代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546wx.uploadImage({ //获取图片媒体ID localId: localIds[erp.common.idx].toString(), // 需要上传的图片的本地ID isShowProgressTips: 1, // 默认为1，显示进度提示 success: function (res) { //获取成功 // 上传序号，上传一张 累计 +1 erp.common.idx++; //存储图片媒体ID，用，号分割 // serverIds += res.serverId + &apos;,&apos;; erp.common.serverIdsArr.push(res.serverId); if (erp.common.idx &lt; localIds.length) { //本地图片ID 还没全部获取完图片媒体ID //调用上传递归函数 erp.common.wxUploadImg(localIds, callback); } else { //上传序号归零 erp.common.idx = 0; //服务器csrf 验证字符串，如果后端框架没开启csrf，则不需要 //alert(erp.common.serverIdsArr); abp.services.app.wxAccess.uploadMediaToQiniu({ WxMediaIds: mediaIds }).done(function (data) { var imageUrl = []; for (var index = 0; index &lt;= data.qiniuFiles.length - 1; index++) { imageUrl.push(data.qiniuFiles[index].qiniuUrl); } $.hideLoading(); callback &amp;&amp; callback(imageUrl); }).always(function(){ $.hideLoading(); }); //serverIds = &apos;&apos;; erp.common.serverIdsArr.length = 0; return true; } }, fail: function (res) { //获取多媒体id失败 返回错误代码 alert(&quot;上传失败，msg：&quot; + JSON.stringify(res)); } }); 后端部分代码: 1234567891011121314151617181920212223242526272829303132333435/// &lt;summary&gt;/// 上传文件到七牛/// &lt;/summary&gt;/// &lt;param name=&quot;input&quot;&gt;&lt;/param&gt;/// &lt;returns&gt;&lt;/returns&gt;//[RemoteService(false)]public async Task&lt;Dtos.UploadMediaToQiniuOutput&gt; UploadMediaToQiniu(Dtos.UploadMediaToQiniuInput input){ var output = new Dtos.UploadMediaToQiniuOutput(); if (input.WxMediaIds.Count &gt; 0) { output.QiniuFiles = new System.Collections.Generic.List&lt;Dtos.QiniuFile&gt;(); var access_token = await GetOfficialAccountAccessToken(); var accessKey = _appConfiguration[&quot;Qny:qiniuyunAK&quot;]; var secretKey = _appConfiguration[&quot;Qny:qiniuyunSK&quot;]; var bucket = _appConfiguration[&quot;Qny:qiniuyunBucket&quot;]; var prefixPath = _appConfiguration[&quot;Qny:prefixPath&quot;]; var qiniuStorage = new Helper.QiniuStorage(accessKey, secretKey, bucket); foreach (var mediaId in input.WxMediaIds) { var url = $&quot;https://api.weixin.qq.com/cgi-bin/media/get?access_token={access_token.Permit}&amp;media_id={mediaId}&quot;; var fileKey = qiniuStorage.UploadStream(url); var fileUrl = $&quot;{prefixPath}/{fileKey}&quot;; output.QiniuFiles.Add(new Dtos.QiniuFile { QiniuUrl = fileUrl, WxMediaId = mediaId }); } } return output;} 原生小程序上传图片到七牛云后面考虑到一些交互上面的问题，就把原来的webview方式改成了全原生的模式。采用原生的方式，在图片上传上面就好处理多了，只需要实现获取到七牛云用于上传的token，然后使用wx.uploadFile即可上传。服务端获取token代码： 12345678910111213/// &lt;summary&gt;/// 获取七牛token/// &lt;/summary&gt;/// &lt;returns&gt;&lt;/returns&gt;public QiniuTokenOutputDto GetQiniuUpToken(){ var accessKey = _appConfiguration[&quot;Qny:qiniuyunAK&quot;]; var secretKey = _appConfiguration[&quot;Qny:qiniuyunSK&quot;]; var bucket = _appConfiguration[&quot;Qny:qiniuyunBucket&quot;]; var qiniuStorage = new Helper.QiniuStorage(accessKey, secretKey, bucket); var output = new QiniuTokenOutputDto() { UpToken = qiniuStorage.CreateUploadToken() }; return output;} 小程序端部分代码: 12345678910111213141516171819202122232425262728293031323334353637383940411、获取token2、调用小程序API，选择图片。 wx.chooseImage({ count: 9-this.data.cardImgList.length, //默认9 sizeType: [&apos;original&apos;, &apos;compressed&apos;], //可以指定是原图还是压缩图，默认二者都有 sourceType: [&apos;album&apos;, &apos;camera&apos;], //从相册选择 success: (res) =&gt; { res.tempFilePaths.forEach((item,index)=&gt;{ that.upload2Qiniu(item); }); } }); } /*** 图片上传七牛云*/ upload2Qiniu(tempFilePaths) { let token = this.data.token; var that = this; wx.uploadFile({ url: &apos;https://up-z0.qiniup.com&apos;, name: &apos;file&apos;, filePath: tempFilePaths, header: { &quot;Content-Type&quot;: &quot;multipart/form-data&quot; }, formData: { token: that.data.upToken, }, success: function (res) { let data = JSON.parse(res.data) //data.hash图片的资源名，可直接通过域名加资源名访问 // to do ... }, fail: function (res) { console.log(res) } }); } 如果需要更详细的资料，那么就请自行百度or谷歌吧","link":"/posts/2925770460.html"},{"title":".net core使用Topshelf注册windows服务","text":"Topshelf注册windows服务，方便快捷 我们经常会用一些定时处理的任务，在.net种一般是结合Quartz或者hangfire这两个组件实现定时周期性作业，但是我想把开发好的定时作业注册成windows服务，这样，在服务器重启之后，可以自动运行服务。Topshelf 就可以很方便的开发windows，而且注册安装也很方便。TopShelf的地址：http://topshelf-project.com/我们首先使用netget安装TopShelf： 1Install-Package TopShelf 然后在program.cs中加入如下代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243var builder = new HostBuilder() .ConfigureAppConfiguration(config =&gt; { config.AddJsonFile(&quot;monitor.json&quot;, optional: true, reloadOnChange: true); }) .ConfigureServices((hostContext, services) =&gt; { services.Configure&lt;Config&gt;(hostContext.Configuration.GetSection(&quot;monitor&quot;)); services.AddSingleton&lt;IHostLifetime, TopshelfLifetime&gt;(); services.AddHostedService&lt;MonitorService&gt;(); });HostFactory.Run(service =&gt; { service.SetServiceName(&quot;服务名&quot;); service.SetDisplayName(&quot;服务显示名称&quot;); service.SetDescription(&quot;服务描述&quot;); service.UseLog4Net(&quot;log4net.config&quot;); service.Service&lt;IHost&gt;(host =&gt; { host.ConstructUsing(() =&gt; builder.Build()); host.WhenStarted(serviceInstance =&gt; { serviceInstance.StartAsync(); }); host.WhenStopped(serviceInstance =&gt; { serviceInstance.StopAsync(); }); }); service.OnException((ex) =&gt; { Console.WriteLine(&quot;Exception thrown - &quot; + ex.Message); }); service.RunAsLocalSystem(); }); } 新建一个类继承IHostedService接口: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class MonitorService : IHostedService, IDisposable { private IScheduler scheduler; static readonly LogWriter _log = HostLogger.Get&lt;MonitorService&gt;(); public readonly Config Config; private Timer _timer; Config _config; public MonitorService() { } public MonitorService(IOptions&lt;Config&gt; config) { _config = config?.Value; //Config = _config; } public async Task StartAsync(CancellationToken cancellationToken) { _log.Info(&quot;service starting&quot;); ISchedulerFactory sf = new StdSchedulerFactory(); scheduler = await sf.GetScheduler(); IJobDetail job = JobBuilder.Create&lt;AreaSyncJob&gt;().WithIdentity(&quot;job1&quot;, &quot;group1&quot;).Build(); ITrigger trigger = TriggerBuilder.Create().WithIdentity(&quot;triggger1&quot;, &quot;group1&quot;).WithSchedule(CronScheduleBuilder.CronSchedule(new CronExpression(_config.AreaJobCronExpr))).Build(); IJobDetail job1 = JobBuilder.Create&lt;LocationSyncJob&gt;().WithIdentity(&quot;job2&quot;, &quot;group2&quot;).Build(); ITrigger trigger1 = TriggerBuilder.Create().WithIdentity(&quot;triggger2&quot;, &quot;group2&quot;).WithSchedule(CronScheduleBuilder.CronSchedule(new CronExpression(_config.LocationCronExpr))).Build(); IJobDetail job2 = JobBuilder.Create&lt;TaskExecAyncJob&gt;().WithIdentity(&quot;job3&quot;, &quot;group3&quot;).Build(); ITrigger trigger2 = TriggerBuilder.Create().WithIdentity(&quot;triggger3&quot;, &quot;group3&quot;).WithSchedule(CronScheduleBuilder.CronSchedule(new CronExpression(_config.TaskExecCronExpr))).Build(); IJobDetail job3 = JobBuilder.Create&lt;MonitorJob&gt;().WithIdentity(&quot;job4&quot;, &quot;group4&quot;).Build(); ITrigger trigger3 = TriggerBuilder.Create().WithIdentity(&quot;triggger4&quot;, &quot;group4&quot;).WithSchedule(CronScheduleBuilder.CronSchedule(new CronExpression(_config.MonitorCronExpr))).Build(); //启动任务 await scheduler.ScheduleJob(job, trigger); await scheduler.ScheduleJob(job1, trigger1); await scheduler.ScheduleJob(job2, trigger2); await scheduler.ScheduleJob(job3, trigger3); await scheduler.Start(); _log.Info(&quot;service started&quot;); } public Task StopAsync(CancellationToken cancellationToken) { _timer?.Change(Timeout.Infinite, 0); scheduler?.Shutdown(); return Task.CompletedTask; } public void Dispose() { scheduler = null; _timer?.Dispose(); } } StartAsync方法为服务启动时运行。然后编译程序： 1dotnet publish -o ./bin/output -c Release -r win7-x64 执行成功后，会在bin/output目录下生成运行所需要的的程序和依赖DLL。将整个目录复制到指定服务器，cmd进入到程序目录，执行： 1xxxx.exe install 就可以完成服务安装.执行: 1xxxx.exe uninstall 就可以完成服务卸载，非常方便。","link":"/posts/2689066179.html"},{"title":"品尝ABP vNext","text":"ABP是一个开源应用程序框架,专注于基于ASP.NET Core的Web应用程序开发,也适合微服务开发。在除夕那天，ABP更新到了2.0.1,其实在一年前我就在关注它，但是那时候还不是很成熟，所以一直处于观望阶段。这回终于有机会了。 在之前我就用ABP开发过一些应用，它能确实给我们带来更加简便的开发体验，像之前要搭建一个新项目需要自己从0开始，ABP的话是直接通过了模版分分钟生成一个可用的项目，而且还包含了权限、多租户、审计日志等功能。 新建ABP项目官方准备了两种新建ABP项目的方式: ABP CLI,需要提前安装dotnet tool install -g Volo.Abp.Cli,然后abp new 项目名称新建项目 通过在线生成项目，生成完成后下载到本地这里我选择在线生成的方式.在线项目生成地址https://abp.io/get-started. 初始化ABP WebAPI使用EF Core Update-Database初始化数据库,这里我的数据库是MariaDB,所以我们还需要安装一个插件Volo.Abp.EntityFrameworkCore.MySQL，然后在DemoEntityFrameworkCoreModule类中，将options.UseSqlServer();修改为options.UseMySQL();,DemoMigrationsDbContextFactory中UseSqlServer改为UseMySql,修改本地数据库连接：&quot;Default&quot;: &quot;server=localhost;Database=abpdemo;user=root;Password=root;pooling=true;CharSet=utf8;port=3306;sslmode=none&quot;,将Abp.Demo.DbMigrator为启动项目。进入到程序包管理器控制台,选择Abp.Demo.EntityFrameworkCore.DbMigrations项目,然后我们删除掉原有的SqlServer的初始化脚本: 123456PM&gt; remove-migrationBuild started...Build succeeded.Removing migration &apos;20200106080719_Initial&apos;.Removing model snapshot.Done. 我们重新生成: 1234PM&gt; add-migration InitialBuild started...Build succeeded.To undo this action, use Remove-Migration. 然后执行update-database即可生成相应的表： 1234PM&gt; update-databaseBuild started...Build succeeded.Applying migration &apos;20200127080347_Initial&apos;. 另外，MariaDB的时候，会出现 12used in key specification without a key length #longtext无法添加为unique等索引Specified key was too long; max key length is 3072 bytes #字段太长无法加索引 我这里是通过修改类型和长度解决这些问题的,执行成功后，我把表都删除掉，然后运行Abp.Demo.DbMigrator项目. 分别启动Abp.Demo.HttpApi.Host、Abp.Demo.IdentityServer、Abp.Demo.Web三个项目，点击登录,默认用户名/密码:admin/1q2w3E*","link":"/posts/2727768428.html"},{"title":"高德地图和google地图整合","text":"整合百度和google地图 因为高德地图的卫星地图不全，但是在高德地图没有的地方，google地图却有，所以客户有一个需求，就是当高德地图显示”无卫星地图”的时候，就显示google地图的图片。 求助万能的百度当然，因为对地图api不是很熟悉，所以就只有百度和看高德官方文档，但是都没有找到合适的解决方案，有看到叠加地图图层并自定义图片路径，这显然不适合我目前的场景，另外一个就是通过API来叠加地图，结合api判断大概的x-y-z的数值范围来加载不同的切片路径. 1234567891011google = new AMap.TileLayer({ map: map, zIndex: 70, //图块取图地址 getTileUrl: function (x, y, z) { //console.log(arguments); if (x &gt;= 12931 &amp;&amp; y &gt;= 3364 &amp;&amp; z &gt;= 16) return &quot;http://mt0.google.cn/vt/lyrs=s&amp;hl=zh-CN&amp;gl=cn&amp;x=&quot; + x + &quot;&amp;y=&quot; + y + &quot;&amp;z=&quot; + z + &quot;&amp;s=Galile&quot;; } }); 当然，这种方式解决起来并不优雅，而且客户也反馈过，很多地方google有卫星地图但是现在依然没有，因为如果要加载得更细，就要判断更多的xyz，所以处理起来也不方便。 突发奇想之后，突然想到，为何不在服务端来判断当前切片是否是正常的卫星地图切片，如果不是则去请求google地图的切片，采用这种方式的话，那么就需要修改jsapi，但是高德并不提供离线的js api，怎么办呢？ 当然办法是肯定有的，就是自己去下载一个高德js api，然后把卫星地图模式所指向的地址改为自己的服务端地址即可，项目中就引用自己下载的离线js。js api修改代码： 123456在离线js中查询关键字:autonavi.com/appmaptile?style=6可以找到这样的地址:http://webst0{1,2,3,4}.is.autonavi.com/appmaptile?style=6&amp;x=[x]&amp;y=[y]&amp;z=[z]将其修改为自己的代理地址:http://211.137.xxx.xxx/map/?style=6&amp;x=[x]&amp;y=[y]&amp;z=[z] 其他调用方式不变，使用这种方式之后，融合率几乎是99.99%，如果高德和google结合后还是有无卫星地图的情况，那我暂时也没想到比较好的处理方式了。最终效果如下:","link":"/posts/133626166.html"},{"title":"进程守护神器-PM2","text":"PM2是一个守护进程管理器，它可以帮助你管理和保持应用程序24/7在线,与PM2类似是进程守护工具还有Supervisor、Forever等，在此，我们先学习一下PM2。 安装PM2有两种方式，第一种就是基于NPM安装: 1npm install pm2 -g 另外一种就是直接下载: 1wget -qO- https://getpm2.com/install.sh | bash 我才用的后者: 12345678910[root@VM_175_142_centos ~]# wget -qO- https://getpm2.com/install.sh | bash&gt; Welcome to the PM2 auto installer┌─────┬───────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │└─────┴───────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘[PM2] Setting changedModule: pm2$ pm2 set pm2:autosave truePM2 Successfully installed 提示我们安装成功，我们来检验一下: 12[root@VM_175_142_centos ~]# pm2 -v4.2.1 这证明我们安装成功了。 PM2常用命令我们先来归纳一下常用的命令,后面部分会进入到实践操作环节。 pm2 start [Node.js, Python, Ruby, binaries in $PATH] #启动应用 pm2 [list|ls|status] #列出所有应用程序管理的状态 pm2 stop &lt;app_name|namespace|id|’all’|json_conf&gt; #停止应用 pm2 restart &lt;app_name|namespace|id|’all’|json_conf&gt; #重启应用 pm2 delete &lt;app_name|namespace|id|’all’|json_conf&gt; #删除应用 pm2 describe &lt;id|app_name&gt; #查看应用详细信息 pm2 monit #应用监控信息 pm2 logs #查看应用日志 pm2 install #安装应用模块 pm2 ecosystem #生成配置文件 升级PM2: npm install pm2@latest -g #下载最新版本的PM2 pm2 update #在内存中更新PM2 露一手我的centos有一个go程序，我们来solo一番。启动我们的应用 123456789[root@VM_175_142_centos shorturl]# pm2 start main[PM2] Starting /root/shorturl/main in fork_mode (1 instance)[PM2] Done.┌─────┬─────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼─────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ main │ default │ N/A │ fork │ 24044 │ 0s │ 0 │ online │ 0% │ 1.5mb │ root │ disabled │└─────┴─────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘[PM2][WARN] Current process list running is not in sync with saved list. Type &apos;pm2 save&apos; to synchronize or enable autosync via &apos;pm2 set pm2:autodump true&apos; 查看我们的应用 1234567[root@VM_175_142_centos shorturl]# pm2 list┌─────┬─────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼─────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ main │ default │ N/A │ fork │ 24044 │ 68s │ 0 │ online │ 0% │ 20.1mb │ root │ disabled │└─────┴─────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘[PM2][WARN] Current process list running is not in sync with saved list. Type &apos;pm2 save&apos; to synchronize or enable autosync via &apos;pm2 set pm2:autodump true&apos; 重启应用 12345678910[root@VM_175_142_centos shorturl]# pm2 restart mainUse --update-env to update environment variables[PM2] Applying action restartProcessId on app [main](ids: [ 0 ])[PM2] [main](0) ✓┌─────┬─────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼─────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ main │ default │ N/A │ fork │ 2078 │ 0s │ 1 │ online │ 0% │ 1.5mb │ root │ disabled │└─────┴─────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘[PM2][WARN] Current process list running is not in sync with saved list. Type &apos;pm2 save&apos; to synchronize or enable autosync via &apos;pm2 set pm2:autodump true&apos; 停止应用 123456789[root@VM_175_142_centos shorturl]# pm2 stop main[PM2] Applying action stopProcessId on app [main](ids: [ 0 ])[PM2] [main](0) ✓┌─────┬─────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼─────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ main │ default │ N/A │ fork │ 0 │ 0 │ 1 │ stopped │ 0% │ 0b │ root │ disabled │└─────┴─────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘[PM2][WARN] Current process list running is not in sync with saved list. Type &apos;pm2 save&apos; to synchronize or enable autosync via &apos;pm2 set pm2:autodump true&apos; 可以看到我们的应用状态为stopped,我们可以查看我们的日志 12345678910111213141516171819[root@VM_175_142_centos shorturl]# pm2 logs main[TAILING] Tailing last 15 lines for [main] process (change the value with --lines option)/root/.pm2/logs/main-out.log last 15 lines:/root/.pm2/logs/main-error.log last 15 lines:0|main | time=&quot;2019-12-12T13:31:55+08:00&quot; level=info msg=&quot;Server running at [ http://0.0.0.0:80 ], with domain [ https://52fx.biz ]&quot;0|main | time=&quot;2019-12-12T13:33:01+08:00&quot; level=info msg=&quot;Shuting down the server&quot;0|main | time=&quot;2019-12-12T13:33:01+08:00&quot; level=info msg=&quot;Server shutdown&quot;0|main | time=&quot;2019-12-12T13:34:12+08:00&quot; level=info msg=&quot;Creating DB with [ bolt ]&quot;0|main | time=&quot;2019-12-12T13:34:12+08:00&quot; level=info msg=&quot;Server running at [ http://0.0.0.0:80 ], with domain [ https://52fx.biz ]&quot;0|main | time=&quot;2019-12-12T13:50:30+08:00&quot; level=info msg=&quot;Shuting down the server&quot;0|main | time=&quot;2019-12-12T13:50:30+08:00&quot; level=info msg=&quot;Server shutdown&quot;0|main | time=&quot;2019-12-12T13:52:26+08:00&quot; level=info msg=&quot;Creating DB with [ bolt ]&quot;0|main | time=&quot;2019-12-12T13:52:26+08:00&quot; level=info msg=&quot;Server running at [ http://0.0.0.0:80 ], with domain [ https://52fx.biz ]&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Shuting down the server&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Server shutdown&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Creating DB with [ bolt ]&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Server running at [ http://0.0.0.0:80 ], with domain [ https://52fx.biz ]&quot;0|main | time=&quot;2019-12-12T13:55:17+08:00&quot; level=info msg=&quot;Shuting down the server&quot;0|main | time=&quot;2019-12-12T13:55:17+08:00&quot; level=info msg=&quot;Server shutdown&quot; 查看前10行的日志 1234567891011121314 [root@VM_175_142_centos shorturl]# pm2 logs main --lines 10[TAILING] Tailing last 10 lines for [main] process (change the value with --lines option)/root/.pm2/logs/main-out.log last 10 lines:/root/.pm2/logs/main-error.log last 10 lines:0|main | time=&quot;2019-12-12T13:50:30+08:00&quot; level=info msg=&quot;Shuting down the server&quot;0|main | time=&quot;2019-12-12T13:50:30+08:00&quot; level=info msg=&quot;Server shutdown&quot;0|main | time=&quot;2019-12-12T13:52:26+08:00&quot; level=info msg=&quot;Creating DB with [ bolt ]&quot;0|main | time=&quot;2019-12-12T13:52:26+08:00&quot; level=info msg=&quot;Server running at [ http://0.0.0.0:80 ], with domain [ https://52fx.biz ]&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Shuting down the server&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Server shutdown&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Creating DB with [ bolt ]&quot;0|main | time=&quot;2019-12-12T13:54:47+08:00&quot; level=info msg=&quot;Server running at [ http://0.0.0.0:80 ], with domain [ https://52fx.biz ]&quot;0|main | time=&quot;2019-12-12T13:55:17+08:00&quot; level=info msg=&quot;Shuting down the server&quot;0|main | time=&quot;2019-12-12T13:55:17+08:00&quot; level=info msg=&quot;Server shutdown&quot; 查看应用状态 1234567 [root@VM_175_142_centos shorturl]# pm2 status main┌─────┬─────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼─────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ main │ default │ N/A │ fork │ 0 │ 0 │ 1 │ stopped │ 0% │ 0b │ root │ disabled │└─────┴─────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘[PM2][WARN] Current process list running is not in sync with saved list. Type &apos;pm2 save&apos; to synchronize or enable autosync via &apos;pm2 set pm2:autodump true&apos; 查看监控 12345678910111213141516171819202122232425262728293031 [root@VM_175_142_centos shorturl]# pm2 monit ┌─ Process List ─────────────────────────────────────────┐┌── main Logs ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐│[ 0] main Mem: 19 MB CPU: 0 % online ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ ││ │└────────────────────────────────────────────────────────┘└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘┌─ Custom Metrics ───────────────────────────────────────┐┌─ Metadata ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐│ ││ App Name main ││ ││ Namespace default ││ ││ Version N/A ││ ││ Restarts 1 ││ ││ Uptime 31s ││ ││ Script path /root/shorturl/main │└────────────────────────────────────────────────────────┘└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ left/right: switch boards | up/down/mouse: scroll | Ctrl-C: exit PM2配置方式启动 12 [root@VM_175_142_centos shorturl]# pm2 ecosystem File /root/shorturl/ecosystem.config.js generated 会生成一个名为ecosystem.config.js的模版配置文件，我们稍作修改： 123456789101112131415161718192021222324252627282930 module.exports = { apps : [{ name: &apos;short&apos;, script: &apos;main&apos;, // Options reference: https://pm2.keymetrics.io/docs/usage/application-declaration/ args: &apos;config ./config.yml&apos;, instances: 1, autorestart: true, watch: false, max_memory_restart: &apos;1G&apos;, env: { NODE_ENV: &apos;development&apos; }, env_production: { NODE_ENV: &apos;production&apos; } }], deploy : { production : { user : &apos;node&apos;, host : &apos;212.83.163.1&apos;, ref : &apos;origin/master&apos;, repo : &apos;git@github.com:repo.git&apos;, path : &apos;/var/www/production&apos;, &apos;post-deploy&apos; : &apos;npm install &amp;&amp; pm2 reload ecosystem.config.js --env production&apos; } }};运行: 12345678910[root@VM_175_142_centos shorturl]# pm2 start ecosystem.config.js[PM2][WARN] Applications short not running, starting...[PM2] App [short] launched (1 instances)┌─────┬──────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼──────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ main │ default │ N/A │ fork │ 0 │ 0 │ 1 │ stopped │ 0% │ 0b │ root │ disabled ││ 1 │ short │ default │ N/A │ fork │ 9525 │ 0s │ 0 │ online │ 0% │ 1.5mb │ root │ disabled │└─────┴──────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘[PM2][WARN] Current process list running is not in sync with saved list. Type &apos;pm2 save&apos; to synchronize or enable autosync via &apos;pm2 set pm2:autodump true&apos; 具体的参数说明，可以去官网查阅. 未完待续，后面又更多骚操作，会继续更新该文档","link":"/posts/3486741546.html"},{"title":"Docker搭建Redis集群","text":"今天是除夕，但是我并有感受到像别人家那般浓厚的年味，我妈早早的做了几个菜，中午随便对付一下就算过了年了。入正题吧，今天主要是记录一下Docker搭建Redis集群的过程，在2017年的时候，当时接触到了中移在线的一个项目，因为考虑到高并发，所以就用到了Redis，但是当时都是手动基于Centos搭建和运维。 Redis的Dockerfile我们既然我们要使用Docker来安装Redis,那么我们最好还是得对他的Dcokerfile还是需要有一定的了解。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115FROM debian:buster-slim# add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get addedRUN groupadd -r -g 999 redis &amp;&amp; useradd -r -g redis -u 999 redis# grab gosu for easy step-down from root# https://github.com/tianon/gosu/releasesENV GOSU_VERSION 1.11RUN set -eux; \\# save list of currently installed packages for later so we can clean up savedAptMark=&quot;$(apt-mark showmanual)&quot;; \\ apt-get update; \\ apt-get install -y --no-install-recommends \\ ca-certificates \\ dirmngr \\ gnupg \\ wget \\ ; \\ rm -rf /var/lib/apt/lists/*; \\ \\ dpkgArch=&quot;$(dpkg --print-architecture | awk -F- &apos;{ print $NF }&apos;)&quot;; \\ wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch&quot;; \\ wget -O /usr/local/bin/gosu.asc &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch.asc&quot;; \\ \\# verify the signature export GNUPGHOME=&quot;$(mktemp -d)&quot;; \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4; \\ gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \\ gpgconf --kill all; \\ rm -rf &quot;$GNUPGHOME&quot; /usr/local/bin/gosu.asc; \\ \\# clean up fetch dependencies apt-mark auto &apos;.*&apos; &gt; /dev/null; \\ [ -z &quot;$savedAptMark&quot; ] || apt-mark manual $savedAptMark &gt; /dev/null; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ \\ chmod +x /usr/local/bin/gosu; \\# verify that the binary works gosu --version; \\ gosu nobody trueENV REDIS_VERSION 5.0.7ENV REDIS_DOWNLOAD_URL http://download.redis.io/releases/redis-5.0.7.tar.gzENV REDIS_DOWNLOAD_SHA 61db74eabf6801f057fd24b590232f2f337d422280fd19486eca03be87d3a82bRUN set -eux; \\ \\ savedAptMark=&quot;$(apt-mark showmanual)&quot;; \\ apt-get update; \\ apt-get install -y --no-install-recommends \\ ca-certificates \\ wget \\ \\ gcc \\ libc6-dev \\ make \\ ; \\ rm -rf /var/lib/apt/lists/*; \\ \\ wget -O redis.tar.gz &quot;$REDIS_DOWNLOAD_URL&quot;; \\ echo &quot;$REDIS_DOWNLOAD_SHA *redis.tar.gz&quot; | sha256sum -c -; \\ mkdir -p /usr/src/redis; \\ tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1; \\ rm redis.tar.gz; \\ \\# disable Redis protected mode [1] as it is unnecessary in context of Docker# (ports are not automatically exposed when running inside Docker, but rather explicitly by specifying -p / -P)# [1]: https://github.com/antirez/redis/commit/edd4d555df57dc84265fdfb4ef59a4678832f6da grep -q &apos;^#define CONFIG_DEFAULT_PROTECTED_MODE 1$&apos; /usr/src/redis/src/server.h; \\ sed -ri &apos;s!^(#define CONFIG_DEFAULT_PROTECTED_MODE) 1$!\\1 0!&apos; /usr/src/redis/src/server.h; \\ grep -q &apos;^#define CONFIG_DEFAULT_PROTECTED_MODE 0$&apos; /usr/src/redis/src/server.h; \\# for future reference, we modify this directly in the source instead of just supplying a default configuration flag because apparently &quot;if you specify any argument to redis-server, [it assumes] you are going to specify everything&quot;# see also https://github.com/docker-library/redis/issues/4#issuecomment-50780840# (more exactly, this makes sure the default behavior of &quot;save on SIGTERM&quot; stays functional by default) \\ make -C /usr/src/redis -j &quot;$(nproc)&quot; all; \\ make -C /usr/src/redis install; \\ \\# TODO https://github.com/antirez/redis/pull/3494 (deduplicate &quot;redis-server&quot; copies) serverMd5=&quot;$(md5sum /usr/local/bin/redis-server | cut -d&apos; &apos; -f1)&quot;; export serverMd5; \\ find /usr/local/bin/redis* -maxdepth 0 \\ -type f -not -name redis-server \\ -exec sh -eux -c &apos; \\ md5=&quot;$(md5sum &quot;$1&quot; | cut -d&quot; &quot; -f1)&quot;; \\ test &quot;$md5&quot; = &quot;$serverMd5&quot;; \\ &apos; -- &apos;{}&apos; &apos;;&apos; \\ -exec ln -svfT &apos;redis-server&apos; &apos;{}&apos; &apos;;&apos; \\ ; \\ \\ rm -r /usr/src/redis; \\ \\ apt-mark auto &apos;.*&apos; &gt; /dev/null; \\ [ -z &quot;$savedAptMark&quot; ] || apt-mark manual $savedAptMark &gt; /dev/null; \\ find /usr/local -type f -executable -exec ldd &apos;{}&apos; &apos;;&apos; \\ | awk &apos;/=&gt;/ { print $(NF-1) }&apos; \\ | sort -u \\ | xargs -r dpkg-query --search \\ | cut -d: -f1 \\ | sort -u \\ | xargs -r apt-mark manual \\ ; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ \\ redis-cli --version; \\ redis-server --versionRUN mkdir /data &amp;&amp; chown redis:redis /dataVOLUME /dataWORKDIR /dataCOPY docker-entrypoint.sh /usr/local/bin/ENTRYPOINT [&quot;docker-entrypoint.sh&quot;]EXPOSE 6379CMD [&quot;redis-server&quot;] 从代码中，我们能看出。它大概做了这几件事： 1.创建一个供Redis运行的redis用户。 2.下载Redis源文件并编译 3.设置运行目录、复制shell脚本并启动Redis 这里的ENTRYPOINT和CMD以为要一起写呢？有什么区别吗？这里我简单介绍一下这二者的作用，它们俩都是在启动容器的执行的命令，ENTRYPOINT是一定会执行，CMD就是执行一些默认的参数，当容器运行的时候，如果参数了其他参数则不会执行CMD命令。我们再来看看docker-entrypoint.sh脚本的内容: 12345678910111213141516#!/bin/shset -e# first arg is `-f` or `--some-option`# or first arg is `something.conf`if [ &quot;${1#-}&quot; != &quot;$1&quot; ] || [ &quot;${1%.conf}&quot; != &quot;$1&quot; ]; then set -- redis-server &quot;$@&quot;fi# allow the container to be started with `--user`if [ &quot;$1&quot; = &apos;redis-server&apos; -a &quot;$(id -u)&quot; = &apos;0&apos; ]; then find . \\! -user redis -exec chown redis &apos;{}&apos; + exec gosu redis &quot;$0&quot; &quot;$@&quot;fiexec &quot;$@&quot; 可以看出，如果容器运行时带有redis-server的参数，就会执行最下面的语句。 基于Docker安装单实例Redis我们先运行一个最最简单的Redis单实例: 123456789101112[root@instance-p0a4erj8 ~]# docker run --name single-redis -p 16379:6379 -d redisUnable to find image &apos;redis:latest&apos; locallylatest: Pulling from library/redis8ec398bc0356: Pull complete da01136793fa: Pull complete cf1486a2c0b8: Pull complete a44f7da98d9e: Pull complete c677fde73875: Pull complete 727f8da63ac2: Pull complete Digest: sha256:90d44d431229683cadd75274e6fcb22c3e0396d149a8f8b7da9925021ee75c30Status: Downloaded newer image for redis:latest6cbde1e7d6d7511d2c0724d09ebf1f17c5823bbfc42821805b6be75c700063d5 现在就可以查看到运行的Docker容器 123[root@instance-p0a4erj8 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6cbde1e7d6d7 redis &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:16379-&gt;6379/tcp single-redis 我们进入到容器连接到redis查看我们的基础信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142[root@instance-p0a4erj8 ~]# sudo docker exec -it 6cbde1e7d6d7 /bin/bashroot@6cbde1e7d6d7:/data# redis-cli -h 127.0.0.1127.0.0.1:6379&gt; info# Serverredis_version:5.0.7redis_git_sha1:00000000redis_git_dirty:0redis_build_id:b671f408463590b9redis_mode:standaloneos:Linux 3.10.0-957.27.2.el7.x86_64 x86_64arch_bits:64multiplexing_api:epollatomicvar_api:atomic-builtingcc_version:8.3.0process_id:1run_id:c55ee8eb658ae70d4bc1f87217a9ba14ef5d872ctcp_port:6379uptime_in_seconds:1404uptime_in_days:0hz:10configured_hz:10lru_clock:2792994executable:/data/redis-serverconfig_file:# Clientsconnected_clients:3client_recent_max_input_buffer:2client_recent_max_output_buffer:0blocked_clients:0# Memoryused_memory:896128used_memory_human:875.12Kused_memory_rss:2867200used_memory_rss_human:2.73Mused_memory_peak:4995904used_memory_peak_human:4.76Mused_memory_peak_perc:17.94%used_memory_overhead:874802used_memory_startup:791264used_memory_dataset:21326used_memory_dataset_perc:20.34%allocator_allocated:1449240allocator_active:1744896allocator_resident:8658944total_system_memory:4142325760total_system_memory_human:3.86Gused_memory_lua:37888used_memory_lua_human:37.00Kused_memory_scripts:0used_memory_scripts_human:0Bnumber_of_cached_scripts:0maxmemory:0maxmemory_human:0Bmaxmemory_policy:noevictionallocator_frag_ratio:1.20allocator_frag_bytes:295656allocator_rss_ratio:4.96allocator_rss_bytes:6914048rss_overhead_ratio:0.33rss_overhead_bytes:-5791744mem_fragmentation_ratio:3.36mem_fragmentation_bytes:2013072mem_not_counted_for_evict:0mem_replication_backlog:0mem_clients_slaves:0mem_clients_normal:83538mem_aof_buffer:0mem_allocator:jemalloc-5.1.0active_defrag_running:0lazyfree_pending_objects:0# Persistenceloading:0rdb_changes_since_last_save:0rdb_bgsave_in_progress:0rdb_last_save_time:1579849894rdb_last_bgsave_status:okrdb_last_bgsave_time_sec:-1rdb_current_bgsave_time_sec:-1rdb_last_cow_size:0aof_enabled:0aof_rewrite_in_progress:0aof_rewrite_scheduled:0aof_last_rewrite_time_sec:-1aof_current_rewrite_time_sec:-1aof_last_bgrewrite_status:okaof_last_write_status:okaof_last_cow_size:0# Statstotal_connections_received:3total_commands_processed:3instantaneous_ops_per_sec:0total_net_input_bytes:59total_net_output_bytes:17953instantaneous_input_kbps:0.00instantaneous_output_kbps:0.00rejected_connections:0sync_full:0sync_partial_ok:0sync_partial_err:0expired_keys:0expired_stale_perc:0.00expired_time_cap_reached_count:0evicted_keys:0keyspace_hits:0keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:0migrate_cached_sockets:0slave_expires_tracked_keys:0active_defrag_hits:0active_defrag_misses:0active_defrag_key_hits:0active_defrag_key_misses:0# Replicationrole:masterconnected_slaves:0master_replid:1f4d9501e8ac6f6290436805ee7dae04972f6bc0master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0# CPUused_cpu_sys:1.231650used_cpu_user:1.380298used_cpu_sys_children:0.003946used_cpu_user_children:0.003628# Clustercluster_enabled:0# Keyspace127.0.0.1:6379&gt; 但是一般情况，我们肯定不会用默认配置来运行Redis,所以在运行时需要指定配置参数。比如数据存储路径呀、redis密码呀还有主从配置啊，等等。这里我们就修改redis.conf文件,给redis配置密码: 1requirepass dockerredis 在启动的时候，我们加入启动参数: 12[root@instance-p0a4erj8 ~]# docker run -p 16379:6379 --name single-redis -v /root/redis.conf:/usr/local/etc/redis.conf -d redis redis-server /usr/local/etc/redis.conf5ad95dab31df10d466e164f4dd305564ab68f77ddc63eec882d271aa3bc7338a 这是再登录我们的redis server就会提示需要输入密码: 12345[root@instance-p0a4erj8 ~]# sudo docker exec -it 5ad95dab31df /bin/bashroot@5ad95dab31df:/data# redis-cli -h 127.0.0.1127.0.0.1:6379&gt; infoNOAUTH Authentication required.127.0.0.1:6379&gt; 输入密码即可操作: 12127.0.0.1:6379&gt; auth dockerredisOK Docker搭建Redis集群刚才我们都是手动搭建的，如果是搭建集群至少启动动6个实例，这样一一启动，还是有些耗时的，所以需要借助Shell脚本帮我们节省一些时间。 准备配置文件这里我们需要修改redis.conf中的配置: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won&apos;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&apos;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 0.0.0.0# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode no# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 1${port}# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# TLS/SSL ###################################### By default, TLS/SSL is disabled. To enable it, the &quot;tls-port&quot; configuration# directive can be used to define TLS-listening ports. To enable TLS on the# default port, use:## port 0# tls-port 6379# Configure a X.509 certificate and private key to use for authenticating the# server to connected clients, masters or cluster peers. These files should be# PEM formatted.## tls-cert-file redis.crt tls-key-file redis.key# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:## tls-dh-params-file redis.dh# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL# clients and peers. Redis requires an explicit configuration of at least one# of these, and will not implicitly use the system wide configuration.## tls-ca-cert-file ca.crt# tls-ca-cert-dir /etc/ssl/certs# If TLS/SSL clients are required to authenticate using a client side# certificate, use this directive.## Note: this applies to all incoming clients, including replicas.## tls-auth-clients yes# If TLS/SSL should be used when connecting as a replica to a master, enable# this configuration directive:## tls-replication yes# If TLS/SSL should be used for the Redis Cluster bus, enable this configuration# directive.## NOTE: If TLS/SSL is enabled for Cluster Bus, mutual authentication is always# enforced.## tls-cluster yes# Explicitly specify TLS versions to support. Allowed values are case insensitive# and include &quot;TLSv1&quot;, &quot;TLSv1.1&quot;, &quot;TLSv1.2&quot;, &quot;TLSv1.3&quot; (OpenSSL &gt;= 1.1.1) or# &quot;default&quot; which is currently &gt;= TLSv1.1.## tls-protocols TLSv1.2# Configure allowed ciphers. See the ciphers(1ssl) manpage for more information# about the syntax of this string.## Note: this configuration applies only to &lt;= TLSv1.2.## tls-ciphers DEFAULT:!MEDIUM# Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more# information about the syntax of this string, and specifically for TLSv1.3# ciphersuites.## tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256# When choosing a cipher, use the server&apos;s preference instead of the client# preference. By default, the server follows the client&apos;s preference.## tls-prefer-server-ciphers yes################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &apos;yes&apos; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &apos;databases&apos;-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&apos;s set to &apos;yes&apos; as it&apos;s almost always a win.# If you want to save some CPU in the saving child set it to &apos;no&apos; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &apos;dbfilename&apos; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.#masterauth dockerredis## However this is not enough if you are using Redis ACLs (for Redis version# 6 or greater), and the default user is not capable of running the PSYNC# command and/or other commands needed for replication. In this case it&apos;s# better to configure a special user to use with replication, and specify the# masteruser configuration as such:## masteruser &lt;username&gt;## When masteruser is specified, the replica will authenticate against its# master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to &apos;yes&apos; (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to &apos;no&apos; the replica will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It&apos;s just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using &apos;rename-command&apos; to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## New replicas and reconnecting replicas that are not able to continue the# replication process just receiving differences, need to do what is called a# &quot;full synchronization&quot;. An RDB file is transmitted from the master to the# replicas.## The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child# producing the RDB file finishes its work. With diskless replication instead# once the transfer starts, new replicas arriving will be queued and a new# transfer will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple# replicas will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the# server waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# -----------------------------------------------------------------------------# WARNING: RDB diskless load is experimental. Since in this setup the replica# does not immediately store an RDB on disk, it may cause data loss during# failovers. RDB diskless load + Redis modules not handling I/O reads may also# cause Redis to abort in case of I/O errors during the initial synchronization# stage with the master. Use only if your do what you are doing.# -----------------------------------------------------------------------------## Replica can load the RDB it reads from the replication link directly from the# socket, or store the RDB to a file and read that file after it was completely# recived from the master.## In many cases the disk is slower than the network, and storing and loading# the RDB file may increase replication time (and even increase the master&apos;s# Copy on Write memory and salve buffers).# However, parsing the RDB file directly from the socket may mean that we have# to flush the contents of the current database before the full rdb was# received. For this reason we have the following options:## &quot;disabled&quot; - Don&apos;t use diskless load (store the rdb file to the disk first)# &quot;on-empty-db&quot; - Use diskless load only when it is completely safe.# &quot;swapdb&quot; - Keep a copy of the current db contents in RAM while parsing# the data directly from the socket. note that this requires# sufficient memory, if you don&apos;t have it, you risk an OOM kill.#repl-diskless-load disabled# Replicas send PINGs to server in a predefined interval. It&apos;s possible to# change this interval with the repl_ping_replica_period option. The default# value is 10 seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a# replica wants to reconnect again, often a full resync is not needed, but a# partial resync is enough, just passing the portion of data the replica# missed while disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO# output. It is used by Redis Sentinel in order to select a replica to promote# into a master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel# will pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234############################### KEYS TRACKING ################################## Redis implements server assisted support for client side caching of values.# This is implemented using an invalidation table that remembers, using# 16 millions of slots, what clients may have certain subsets of keys. In turn# this is used in order to send invalidation messages to clients. Please# to understand more about the feature check this page:## https://redis.io/topics/client-side-caching## When tracking is enabled for a client, all the read only queries are assumed# to be cached: this will force Redis to store information in the invalidation# table. When keys are modified, such information is flushed away, and# invalidation messages are sent to the clients. However if the workload is# heavily dominated by reads, Redis could use more and more memory in order# to track the keys fetched by many clients.## For this reason it is possible to configure a maximum fill value for the# invalidation table. By default it is set to 10%, and once this limit is# reached, Redis will start to evict caching slots in the invalidation table# even if keys are not modified, just to reclaim memory: this will in turn# force the clients to invalidate the cached values. Basically the table# maximum fill rate is a trade off between the memory you want to spend server# side to track information about who cached what, and the ability of clients# to retain cached objects in memory.## If you set the value to 0, it means there are no limits, and all the 16# millions of caching slots can be used at the same time. In the &quot;stats&quot;# INFO section, you can find information about the amount of caching slots# used at every given moment.## tracking-table-max-fill 10################################## SECURITY #################################### Warning: since Redis is pretty fast an outside user can try up to# 1 million passwords per second against a modern box. This means that you# should use very strong passwords, otherwise they will be very easy to break.# Note that because the password is really a shared secret between the client# and the server, and should not be memorized by any human, the password# can be easily a long string from /dev/urandom or whatever, so by using a# long and unguessable password no brute force attack will be possible.# Redis ACL users are defined in the following format:## user &lt;username&gt; ... acl rules ...## For example:## user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99## The special username &quot;default&quot; is used for new connections. If this user# has the &quot;nopass&quot; rule, then new connections will be immediately authenticated# as the &quot;default&quot; user without the need of any password provided via the# AUTH command. Otherwise if the &quot;default&quot; user is not flagged with &quot;nopass&quot;# the connections will start in not authenticated state, and will require# AUTH (or the HELLO command AUTH option) in order to be authenticated and# start to work.## The ACL rules that describe what an user can do are the following:## on Enable the user: it is possible to authenticate as this user.# off Disable the user: it&apos;s no longer possible to authenticate# with this user, however the already authenticated connections# will still work.# +&lt;command&gt; Allow the execution of that command# -&lt;command&gt; Disallow the execution of that command# +@&lt;category&gt; Allow the execution of all the commands in such category# with valid categories are like @admin, @set, @sortedset, ...# and so forth, see the full list in the server.c file where# the Redis command table is described and defined.# The special category @all means all the commands, but currently# present in the server, and that will be loaded in the future# via modules.# +&lt;command&gt;|subcommand Allow a specific subcommand of an otherwise# disabled command. Note that this form is not# allowed as negative like -DEBUG|SEGFAULT, but# only additive starting with &quot;+&quot;.# allcommands Alias for +@all. Note that it implies the ability to execute# all the future commands loaded via the modules system.# nocommands Alias for -@all.# ~&lt;pattern&gt; Add a pattern of keys that can be mentioned as part of# commands. For instance ~* allows all the keys. The pattern# is a glob-style pattern like the one of KEYS.# It is possible to specify multiple patterns.# allkeys Alias for ~*# resetkeys Flush the list of allowed keys patterns.# &gt;&lt;password&gt; Add this passowrd to the list of valid password for the user.# For example &gt;mypass will add &quot;mypass&quot; to the list.# This directive clears the &quot;nopass&quot; flag (see later).# &lt;&lt;password&gt; Remove this password from the list of valid passwords.# nopass All the set passwords of the user are removed, and the user# is flagged as requiring no password: it means that every# password will work against this user. If this directive is# used for the default user, every new connection will be# immediately authenticated with the default user without# any explicit AUTH command required. Note that the &quot;resetpass&quot;# directive will clear this condition.# resetpass Flush the list of allowed passwords. Moreover removes the# &quot;nopass&quot; status. After &quot;resetpass&quot; the user has no associated# passwords and there is no way to authenticate without adding# some password (or setting it as &quot;nopass&quot; later).# reset Performs the following actions: resetpass, resetkeys, off,# -@all. The user returns to the same state it has immediately# after its creation.## ACL rules can be specified in any order: for instance you can start with# passwords, then flags, or key patterns. However note that the additive# and subtractive rules will CHANGE MEANING depending on the ordering.# For instance see the following example:## user alice on +@all -DEBUG ~* &gt;somepassword## This will allow &quot;alice&quot; to use all the commands with the exception of the# DEBUG command, since +@all added all the commands to the set of the commands# alice can use, and later DEBUG was removed. However if we invert the order# of two ACL rules the result will be different:## user alice on -DEBUG +@all ~* &gt;somepassword## Now DEBUG was removed when alice had yet no commands in the set of allowed# commands, later all the commands are added, so the user will be able to# execute everything.## Basically ACL rules are processed left-to-right.## For more information about ACL configuration please refer to# the Redis web site at https://redis.io/topics/acl# Using an external ACL file## Instead of configuring users here in this file, it is possible to use# a stand-alone file just listing users. The two methods cannot be mixed:# if you configure users here and at the same time you activate the exteranl# ACL file, the server will refuse to start.## The format of the external ACL user file is exactly the same as the# format that is used inside redis.conf to describe users.## aclfile /etc/redis/users.acl# IMPORTANT NOTE: starting with Redis 6 &quot;requirepass&quot; is just a compatiblity# layer on top of the new ACL system. The option effect will be just setting# the password for the default user. Clients will still authenticate using# AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;# if they follow the new protocol: both will work.# requirepass dockerredis# Command renaming (DEPRECATED).## ------------------------------------------------------------------------# WARNING: avoid using this option if possible. Instead use ACLs to remove# commands from the default user, and put them only in some admin user you# create for administrative purposes.# ------------------------------------------------------------------------## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &apos;max number of clients reached&apos;.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&apos;t remove keys according to the policy, or if the policy is# set to &apos;noeviction&apos;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the &apos;noeviction&apos; policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is &apos;noeviction&apos;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select one from the following behaviors:## volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key having an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don&apos;t evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica# to have a different memory setting, and you are sure all the writes performed# to the replica are idempotent, then you may change this default (but be sure# to understand what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory# and so forth). So make sure you monitor your replicas and make sure they# have enough memory to never hit a real out-of-memory condition before the# master hits the configured maxmemory setting.## replica-ignore-maxmemory yes# Redis reclaims expired keys in two ways: upon access when those keys are# found to be expired, and also in background, in what is called the# &quot;active expire key&quot;. The key space is slowly and interactively scanned# looking for expired keys to reclaim, so that it is possible to free memory# of keys that are expired and will never be accessed again in a short time.## The default effort of the expire cycle will try to avoid having more than# ten percent of expired keys still in memory, and will try to avoid consuming# more than 25% of total memory and to add latency to the system. However# it is possible to increase the expire &quot;effort&quot; that is normally set to# &quot;1&quot;, to a greater value, up to the value &quot;10&quot;. At its maximum value the# system will use more CPU, longer cycles (and technically may introduce# more latency), and will tollerate less already expired keys still present# in the system. It&apos;s a tradeoff betweeen memory, CPU and latecy.## active-expire-effort 1############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It&apos;s up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no################################ THREADED I/O ################################## Redis is mostly single threaded, however there are certain threaded# operations such as UNLINK, slow I/O accesses and other things that are# performed on side threads.## Now it is also possible to handle Redis clients socket reads and writes# in different I/O threads. Since especially writing is so slow, normally# Redis users use pipelining in order to speedup the Redis performances per# core, and spawn multiple instances in order to scale more. Using I/O# threads it is possible to easily speedup two times Redis without resorting# to pipelining nor sharding of the instance.## By default threading is disabled, we suggest enabling it only in machines# that have at least 4 or more cores, leaving at least one spare core.# Using more than 8 threads is unlikely to help much. We also recommend using# threaded I/O only if you actually have performance problems, with Redis# instances being able to use a quite big percentage of CPU time, otherwise# there is no point in using this feature.## So for instance if you have a four cores boxes, try to use 2 or 3 I/O# threads, if you have a 8 cores, try to use 6 threads. In order to# enable I/O threads use the following configuration directive:## io-threads 4## Setting io-threads to 1 will just use the main thread as usually.# When I/O threads are enabled, we only use threads for writes, that is# to thread the write(2) syscall and transfer the client buffers to the# socket. However it is also possible to enable threading of reads and# protocol parsing using the following configuration directive, by setting# it to yes:## io-threads-do-reads no## Usually threading reads doesn&apos;t help much.## NOTE 1: This configuration directive cannot be changed at runtime via# CONFIG SET. Aso this feature currently does not work when SSL is# enabled.## NOTE 2: If you want to test the Redis speedup using redis-benchmark, make# sure you also run the benchmark itself in threaded mode, using the# --threads option to match the number of Redis theads, otherwise you&apos;ll not# be able to notice the improvements.############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly yes# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&apos;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&apos;s usually the right compromise between# speed and data safety. It&apos;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&apos;s snapshotting),# or on the contrary, use &quot;always&quot; that&apos;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&apos;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&apos;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&apos;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################ Normal Redis instances can&apos;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:# cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.#cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they&apos;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&apos;t be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# This option, when set to yes, allows nodes to serve read traffic while the# the cluster is in a down state, as long as it believes it owns the slots. ## This is useful for two cases. The first case is for when an application # doesn&apos;t require consistency of data during node failures or network partitions.# One example of this is a cache, where as long as the node has the data it# should be able to serve it. ## The second use case is for configurations that don&apos;t meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the# entire cluster without this option set, with it set there is only a write outage.# Without a quorum of masters, slot ownership will not change automatically. ## cluster-allow-reads-when-down no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:# cluster-announce-ip 182.61.35.33 cluster-announce-port 1${port} cluster-announce-bus-port 2${port}################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&apos;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don&apos;t need# this feature and the feature has some overhead. Note that if you don&apos;t# specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### GOPHER SERVER ################################## Redis contains an implementation of the Gopher protocol, as specified in# the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).## The Gopher protocol was very popular in the late &apos;90s. It is an alternative# to the web, and the implementation both server and client side is so simple# that the Redis server has just 100 lines of code in order to implement this# support.## What do you do with Gopher nowadays? Well Gopher never *really* died, and# lately there is a movement in order for the Gopher more hierarchical content# composed of just plain text documents to be resurrected. Some want a simpler# internet, others believe that the mainstream internet became too much# controlled, and it&apos;s cool to create an alternative space for people that# want a bit of fresh air.## Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol# as a gift.## --- HOW IT WORKS? ---## The Redis Gopher support uses the inline protocol of Redis, and specifically# two kind of inline requests that were anyway illegal: an empty request# or any request that starts with &quot;/&quot; (there are no Redis commands starting# with such a slash). Normal RESP2/RESP3 requests are completely out of the# path of the Gopher protocol implementation and are served as usually as well.## If you open a connection to Redis when Gopher is enabled and send it# a string like &quot;/foo&quot;, if there is a key named &quot;/foo&quot; it is served via the# Gopher protocol.## In order to create a real Gopher &quot;hole&quot; (the name of a Gopher site in Gopher# talking), you likely need a script like the following:## https://github.com/antirez/gopher2redis## --- SECURITY WARNING ---## If you plan to put Redis on the internet in a publicly accessible address# to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.# Once a password is set:## 1. The Gopher server (when enabled, not by default) will still serve# content via Gopher.# 2. However other commands cannot be called before the client will# authenticate.## So use the &apos;requirepass&apos; option to protect your instance.## To enable Gopher support uncomment the following line and set# the option from no (the default) to yes.## gopher-enabled no############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&apos;t start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don&apos;t compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&apos;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&apos;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&apos;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here.## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used as# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it&apos;s maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don&apos;t have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag no# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage, to be used when the lower# threshold is reached# active-defrag-cycle-min 1# Maximal effort for defrag in CPU percentage, to be used when the upper# threshold is reached# active-defrag-cycle-max 25# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000 准备Shell脚本首先创建脚本init-redis-config.sh用于初始化配置文件: 1234for port in {6379..6384}do mkdir -p /root/redis/${port}/conf &amp;&amp; port=${port} envsubst &lt; redis-cluster.conf &gt; /root/redis/${port}/conf/redis.conf &amp;&amp; mkdir -p /root/redis/${port}/data; done 执行脚本即可创建相应的文件夹和填充redis.conf配置文件。 12345#!/bin/bash for port in {6379..6384}do docker run -p 1${port}:1${port} -p 2${port}:2${port} --name redis-${port} -v /root/redis/${port}/conf/redis.conf:/usr/local/etc/redis.conf -v /root/redis/${port}/data:/data -d redis redis-server /usr/local/etc/redis.conf;done 运行我们的脚本: 1234567[root@instance-p0a4erj8 ~]# ./redis-cluster.sh6c34d79cbf0ef06df72d0c35783cdfee7a6d527e82df0a6a8d7456892c321a6b9aae4f0a3e90e5ba56c71b362d66c467a66c690fc1fc2f2491372d4114a7f559c8f844f7c56d06a167c8bb58a28b55552e6db516cb51d03c6c52233e5482802dba447cade59c63c0edb0b028ea043ab259970a7c11d2b795c7fa70d08debf96bee9b0f2e8704459ded0aa307b459eaace3c15ad509cf423709b33e27a374bcafd5aed7b6ce86c745fc538b958709e831205f23e7f23fa5f60a563fc61c1d5b1d 这时候可以看到我们启动好的redis容器: 12345678[root@instance-p0a4erj8 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd5aed7b6ce86 redis &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:16384-&gt;6379/tcp redis-6384ee9b0f2e8704 redis &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:16383-&gt;6379/tcp redis-6383ba447cade59c redis &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:16382-&gt;6379/tcp redis-6382c8f844f7c56d redis &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:16381-&gt;6379/tcp redis-63819aae4f0a3e90 redis &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:16380-&gt;6379/tcp redis-63806c34d79cbf0e redis &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:16379-&gt;6379/tcp 配置集群进入到任意一个redis容器执行集群创建命令(如果是带有密码,需要多加一个参数-a 密码): 1redis-cli --cluster create 182.61.35.33:16379 182.61.35.33:16380 182.61.35.33:16381 182.61.35.33:16382 182.61.35.33:16383 182.61.35.33:16384 --cluster-replicas 1 会得到如下的结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152root@6fba3a16d91d:/data# redis-cli --cluster create 182.61.35.33:16379 182.61.35.33:16380 182.61.35.33:16381 182.61.35.33:16382 182.61.35.33:16383 182.61.35.33:16384 --cluster-replicas 1 -a dockerredisWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 182.61.35.33:16383 to 182.61.35.33:16379Adding replica 182.61.35.33:16384 to 182.61.35.33:16380Adding replica 182.61.35.33:16382 to 182.61.35.33:16381&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity[WARNING] Some slaves are in the same host as their masterM: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots:[0-5460] (5461 slots) masterM: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5461-10922] (5462 slots) masterM: 9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381 slots:[10923-16383] (5461 slots) masterS: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 replicates d1b77c63e1e652b786c9e52e42f7594e9752903cS: 93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383 replicates b1790b6e64961f2da927cd78ad64cf02463727daS: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 replicates 9d1935b3462566449d9f3c3c38b8d3ab9b97b134Can I set the above configuration? (type &apos;yes&apos; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 182.61.35.33:16379)M: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots:[0-5460] (5461 slots) master 1 additional replica(s)S: 93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383 slots: (0 slots) slave replicates b1790b6e64961f2da927cd78ad64cf02463727daS: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 slots: (0 slots) slave replicates d1b77c63e1e652b786c9e52e42f7594e9752903cM: 9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381 slots:[10923-16383] (5461 slots) master 1 additional replica(s)M: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 slots: (0 slots) slave replicates 9d1935b3462566449d9f3c3c38b8d3ab9b97b134[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 我们可以查看我们的集群信息: 1234567891011121314151617182.61.35.33:16379&gt; CLUSTER INFOcluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:272cluster_stats_messages_pong_sent:277cluster_stats_messages_sent:549cluster_stats_messages_ping_received:272cluster_stats_messages_pong_received:272cluster_stats_messages_meet_received:5cluster_stats_messages_received:549 到此，我们一个集群就搭建好了，这里是在单机进行的，多机搭建也是类似的，后面再继续写动态上下线节点，如果本文有错误之处还望指正，多谢。","link":"/posts/2450214213.html"},{"title":"没有java源代码如何修改bug","text":"有时候遇到比较老的产品，公司的产品组也不提供维护了，更可恨的是，源代码也不给。在这种情况，遇到有bug，怎么办呢？ 我们项目组一直在维护着一个2013年基于公司老产品开发的项目，既然是产品，公司产品弄死不肯定提供老产品的代码，原因大概是因为代码管理混乱，已经找不到我们项目的代码了。 这个bug出现在数据授权的时候，按常规组织授权后，下级用户居然能看到上级用户数据，我们的数据授权原理是：获取授权用户所在层级，并获取当前的授权组织表，将组织表数据和当前用户进行匹配，并记录在授权表，然后，用户在查询数据的时候就读取授权表拼接in语句去数据库查询。当出现这个问题的时候，显然是不允许的，这时候，我们的检查步骤是：1、检查用户组织表2、检查授权表3、检查最终执行sql语句通过以上步骤，最终发现是在做sql拼接成in的条件的时候出现的问题，这时候，就只有进入到代码里面一探究竟，由于没有源代码，就只有借用各种反编译工具：1、Java Decompiler2、Jadclipse3、jad4、jd-gui…..等等，还有很多其他的，这里，我选用的是jd-gui，如果是用的idea作为IDE的话，可以直接查看。借助反编译工具反编译出来的代码如下： 12345678910111213141516171819202122static boolean userIsDept(JdbcTemplate sysJdbcTemplate, final String dept, String userId) { String sql = SQLProvider.getSQL(&quot;query.all.sub.dept&quot;); List&lt;String&gt; depts = (List)sysJdbcTemplate.query(sql, new PreparedStatementSetter() { public void setValues(PreparedStatement ps) throws SQLException { ps.setString(1, dept); ps.setString(2, dept); } }, new ResultSetExtractor&lt;List&lt;String&gt;&gt;() { public List&lt;String&gt; extractData(ResultSet rs) throws SQLException, DataAccessException { ArrayList trs = new ArrayList(); while(rs.next()) { trs.add(rs.getString(&quot;DEPT_ID&quot;)); } return trs; } }); String userDeptSql = SQLProvider.getSQL(&quot;query.user.dept&quot;); String userDeptId = (String)sysJdbcTemplate.queryForObject(userDeptSql, String.class, new Object[]{userId}); return depts.contains(userDeptId); } 经过分析，发现该处的组织id过多加载，因为在授权表中已经将当前用户所属组织的授权内容全量写进去，这儿过多加载，其实把父组织的授权内容也一并拼接到了in条件内，所以出现这种情况。最后，我通过idea新建一个java项目，把项目的依赖包添加进去，新建了一个同包名、同类名的类，将代码copy进新类，稍作修改，就可导出一个jar包。然后将class文件复制到原来的jar包中覆盖即可，在这里，一定要注意JDK版本，否者会报错。","link":"/posts/1851977506.html"},{"title":"Hexo搭建个人博客锦集","text":"我想很多人都想有一个自己的博客网站，现在开源的博客系统也很多，但是像java，python这种开源的博客系统需要宿主机、申请域名、安装环境等稍稍复杂的操作，当然现在也可以基于阿里或者腾讯云提供的Docker容器服务搭建也是非常方便。那么，还有没有成本更低的搭建方式呢？这时候就需要提到Hexo了，什么是Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。我可以将生产的静态页面托管到Github、Coding、Gitee等代码托管平台上，从而节约我们的服务器费用。 Hexo环境准备我们首先需要安装一下几个环境: Node.js:安装的时候直接下一步即可(默认已勾选Add to PATH) Git 以上两项安装成功后,即可安装 Hexo: 1npm install -g hexo-cli #全局安装 新建博客打开cmd命令行，进入到要创建博客的目录,执行以下命令: 123hexo init &lt;文件夹名&gt; #会在当前目录创建一个文件夹,并下载hexo模版cd &lt;文件夹名&gt; #进入博客文件夹npm install #安装hexo相应依赖 构建成功后的目录如下:启动起来看看效果: 1hexo serve 我们可以看到hexo会自动生成一篇文章，并会提到4个常用命令: 1234hexo new &quot;My New Post&quot; #创建博客,缩写hexo n &quot;My New Post&quot;hexo server #启动hexo,缩写hexo shexo generate #生成静态页面,缩写hexo ghexo deploy #发布到静态托管服务器,缩写hexo d 一般我们发布都是 hexo g &amp; hexo d 一起执行。 配置博客这时候需要去修改根目录下的 _config.yml 配置文件,我们可以参考一下https://hexo.io/zh-cn/docs/configuration 属性名 作用 title 网站标题 subtitle 网站副标题 description 主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词 keywords 网站的关键词。使用半角逗号 , 分隔多个关键词。 author 用于主题显示文章的作者 language 网站使用的语言 timezone 网站时区。Hexo 默认使用您电脑的时区。请参考 时区列表 进行设置，如 America/New_York, Japan, 和 UTC 。一般的，对于中国大陆地区可以使用 Asia/Shanghai。 url 网址,如果您的网站存放在子目录中，例如 http://yoursite.com/blog，则请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/ root 网站根目录 permalink 文章的 永久链接 格式 :year/:month/:day/:title/ permalink_defaults 永久链接中各部分的默认值 pretty_urls 改写 permalink 的值来美化 URL pretty_urls.trailing_index 是否在永久链接中保留尾部的 index.html，设置为 false 时去除,默认:true source_dir 资源文件夹，这个文件夹用来存放内容。默认:source public_dir 公共文件夹，这个文件夹用于存放生成的站点文件。默认:public tag_dir 标签文件夹,默认:tags archive_dir 归档文件夹,默认:archives category_dir 分类文件夹,默认:categories code_dir Include code 文件夹，source_dir 下的子目录,默认:downloads/code i18n_dir 国际化（i18n）文件夹,默认: :lang skip_render 跳过指定文件的渲染。匹配到的文件将会被不做改动地复制到 public 目录中。您可使用 glob 表达式来匹配路径。 new_post_name 新文章的文件名称,默认: :title.md default_layout 预设布局,默认: post auto_spacing 在中文和英文之间加入空格,默认: false titlecase 把标题转换为 title case,默认:false external_link 在新标签中打开链接,默认:true external_link.enable 在新标签中打开链接,默认:true external_link.field 对整个网站（site）生效或仅对文章（post）生效,默认:site external_link.exclude 需要排除的域名。主域名和子域名如 www 需分别配置 [] 其他的可以去网站查看，或者自行百度吧,实在是懒得抄了。 配置博客主题Hexo为我们提供了很多主题模版,这里我推荐我喜欢的两个: hexo-theme-butterfly hexo-theme-icarus 更换主题步骤: 下载主题 将主题解压复制到themes文件夹中 修改_config.yml中的theme:属性，属性值为themes中文件夹名，如：theme: butterfly。 如果使用butterfly主题的话，需要安装hexo-renderer-jade(pug的编译工具，内包括了pug的渲染引擎),npm install hexo-renderer-jade 部署到 GitHub Pages 新建一个 repository。如果你希望你的站点能通过 &lt;你的 GitHub 用户名&gt;.github.io 域名访问，你的 repository 应该直接命名为 &lt;你的 GitHub 用户名&gt;.github.io 修改_config.yml:12345deploy: type: git repository: github: https://github.com/eyiadmin/eyiadmin.github.io.git #这里我们选择HTTPS的方式,当然也可以通过SSH方式(后期更加方便) branch: master 因为是git，所以需要npm install --save hexo-deployer-git来安装插件, 发布到GitHub Pages:1hexo g &amp; hexo d 上传成功后就可以在我们的repository中看到上传的静态页面,这时候就可以通过https://&lt;你的 GitHub 用户名&gt;.github.io来访问，如：https://eyiadmin.github.io/ Markdown常用语法标题12345# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题 一级标题二级标题三级标题四级标题五级标题超链接123[超链接名称](超链接地址 &quot;超链接title(鼠标hover显示内容)&quot;) 例如：[农民工学编程](http://blog.52fx.biz &quot;快来看农民工学编程啦&quot;) 农民工学编程 图片12![图片描述(显示在图片下方,可不填)](图片连接地址 &quot;图片title(鼠标hover显示内容)&quot;)![](http://xxnote.52fx.biz/publish.png &quot;发布效果图&quot;) 代码1`单行内容` 多行用内容``` ``` First Blood 123456789Double Kill Trible Kill Quadra Kill Penta Kill Ace 团灭! 未完待续，不定期更新 https://github.com/gohugoio/hugo(Hugo是由Go语言实现的静态网站生成器)https://github.com/jekyll/jekyll(jekyll是由Ruby语言实现的静态网站生成器)https://github.com/hexojs/hexo(Hexo是由Nodejs语言实现的静态网站生成器) https://blog.csdn.net/mqdxiaoxiao/article/details/93378785https://www.jianshu.com/p/25145964abf3","link":"/posts/2317190834.html"},{"title":"Hexo静态网页上传到七牛云","text":"很多人都是把Hexo、hugo等工具生成的静态页面都是上传到Github,这样虽然很方便，但是毕竟在国外，而且百度爬虫老是失败，虽然有盆友说可以上传到Gitee上，域名识别不同的访问线路解析不同的空间。我也这样搞过，但是最近发现，好像七牛云可以托管静态页面。那么就来搞搞咯 要上传到七牛云，当然得有个七牛云账号啦，这个没有的自己去搞一下，就不说了。下面我们来说说具体的操作。 配置对象空间 新建一个对象空间 在空间设置中打开【默认首页设置】 安装七牛云上传工具qshell是利用七牛文档上公开的API实现的一个方便开发者测试和使用七牛API服务的命令行工具,下载qshell,下载之后解压， 文件名 描述 qshell_linux_x86 Linux 32位系统 qshell_linux_x64 Linux 64位系统 qshell_windows_x86.exe Windows 32位系统 qshell_windows_x64.exe Windows 64位系统 qshell_darwin_x64 Mac 64位系统，主流的系统 我这里是Win10-X64,所以下载后重命名qshell_windows_x64.exe为qshell.exe,在环境变量中配置qshell文件路径 上传文件到七牛云首选需要添加账号: 1qshell account &lt;Your AccessKey&gt; &lt;Your SecretKey&gt; &lt;Your Name&gt; qshell有qupload配置文件方式和qupload2命令行方式，具体操作去https://github.com/qiniu/qshell查看详细文档，这里我更倾向于命令行: 1qshell qupload2 --src-dir=E:/hexo/hexo/public --bucket=空间名 配置域名 进入域名管理 新增域名,输入域名点击创建 创建成功后，会有一个CNAME，复制CNAME去解析域名: 回到七牛云的内容空间，设置默认域名这时候，我们的博客就可以正常访问了 小技巧:我们可以再package.json自定义一个我们的命令, 如下: 1234567&quot;scripts&quot;: { &quot;build&quot;: &quot;hexo generate&quot;, &quot;clean&quot;: &quot;hexo clean&quot;, &quot;deploy&quot;: &quot;hexo deploy&quot;, &quot;server&quot;: &quot;hexo server&quot;, &quot;d&quot;:&quot;hexo clean &amp; hexo g &amp; hexo d &amp; qshell qupload2 --overwrite=true --rescan-local=true --src-dir=E:/hexo/hexo/public --bucket=七牛云空间名称&quot; }, 这时候我们在命令行执行npm run d 即可清理、生成、上传文件啦","link":"/posts/520013685.html"},{"title":"Redis集群无感知上线下线节点","text":"搭建集群是为了提高性能提高可用性,当业务高速发展时，肯定会需要新增节点来支撑业务,或者某个节点以外down掉，这时候我们就需要做到无感知新增、删除节点。 先说一个题外话，今天是大年初一，我和我十多年的老炮友，去了一个没人的山上，做了一件非常有激情的事。最近的,新型肺炎也是四处蔓延,为了给社会做一点贡献，就早早的回家，蹲在家里写点东西。我继续昨天写的Docker搭建Redis集群 集群信息我们先来看看节点的基本信息: 1234567182.61.35.33:16380&gt; cluster nodes9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579935926810 15 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 myself,master - 0 1579935924000 14 connected 5461-109226c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579935926000 15 connected 10923-163835923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master - 0 1579935925000 18 connected 0-5460d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 slave 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 0 1579935927000 18 connected93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579935927813 14 connected 我们可以看到他们的主从(端口)对应关系. 主 从 16382 16379 16384 16381 16380 16383 新增节点这里先shell脚本初始化2个节点的配置信息, 1234for port in {6385..6386}do mkdir -p /root/redis/${port}/conf &amp;&amp; port=${port} envsubst &lt; redis-cluster.conf &gt; /root/redis/${port}/conf/redis.conf &amp;&amp; mkdir -p /root/redis/${port}/data; done 运行init-redis-config.sh脚本进行配置文件初始化。接下来启动这两个节点: 1234[root@instance-p0a4erj8 ~]# docker run -p 16385:16385 -p 26385:26385 --name redis-6385 -v /root/redis/6385/conf/redis.conf:/usr/local/etc/redis.conf -v /root/redis/6385/data:/data -d redis redis-server /usr/local/etc/redis.conf2bf222ef7923f4d6b7b4dfbc4b2e74b5c6508af3da4a519447b1431bc8a6a4c2[root@instance-p0a4erj8 ~]# docker run -p 16386:16386 -p 26386:26386 --name redis-6386 -v /root/redis/6386/conf/redis.conf:/usr/local/etc/redis.conf -v /root/redis/6386/data:/data -d redis redis-server /usr/local/etc/redis.conf17db2933515d335e781f7701a878c3b73daa5da2ca35963e18b8f4d7f7a442a6 进入到Redis任意节点, 1docker exec -it redis-6385 bash 执行节点添加命令redis-cli --cluster add-node 新节点IP:端口 集群中现有IP:端口 [-a 密码]: 12345678910111213141516171819202122232425262728root@2bf222ef7923:/data# redis-cli --cluster add-node 182.61.35.33:16385 182.61.35.33:16379 -a dockerredisWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.&gt;&gt;&gt; Adding node 182.61.35.33:16385 to cluster 182.61.35.33:16379&gt;&gt;&gt; Performing Cluster Check (using node 182.61.35.33:16379)S: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots: (0 slots) slave replicates 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2S: 93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383 slots: (0 slots) slave replicates b1790b6e64961f2da927cd78ad64cf02463727daM: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 slots:[0-5460] (5461 slots) master 1 additional replica(s)S: 9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381 slots: (0 slots) slave replicates 6c255660ba138e0cb7ac521ad945bafbf0901f1aM: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5461-10922] (5462 slots) master 1 additional replica(s)M: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 slots:[10923-16383] (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 182.61.35.33:16385 to make it join the cluster.[OK] New node added correctly. 可以看到节点添加成功，这时候可以看看集群节点的信息: 12345678182.61.35.33:16385&gt; cluster nodes93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579938935357 14 connected5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master - 0 1579938938363 18 connected 0-5460d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 slave 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 0 1579938936000 18 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 myself,master - 0 1579938934000 0 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579938936357 14 connected 5461-109226c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579938937360 15 connected 10923-163839d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579938934354 15 connected 可以看到新添加的节点是master节点，但是并没有哈希槽，需要我们手动为其添加 1redis-cli --cluster reshard 任意集群中的IP:端口 分配哈希槽有两种方式： 将其他节点随机的哈希槽分配到目标节点 在指定的节点取出指定数量的哈希槽分配到目标节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648root@2bf222ef7923:/data# redis-cli --cluster reshard 182.61.35.33:16379 -a dockerredisWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.&gt;&gt;&gt; Performing Cluster Check (using node 182.61.35.33:16379)S: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots: (0 slots) slave replicates 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2S: 93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383 slots: (0 slots) slave replicates b1790b6e64961f2da927cd78ad64cf02463727daM: fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385 slots: (0 slots) masterM: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 slots:[0-5460] (5461 slots) master 1 additional replica(s)S: 9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381 slots: (0 slots) slave replicates 6c255660ba138e0cb7ac521ad945bafbf0901f1aM: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5461-10922] (5462 slots) master 1 additional replica(s)M: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 slots:[10923-16383] (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 300What is the receiving node ID? fc576f7421e4b453dc500d4cbe028716bb1efbf8Please enter all the source node IDs. Type &apos;all&apos; to use all the nodes as source nodes for the hash slots. Type &apos;done&apos; once you entered all the source nodes IDs.Source node #1: allReady to move 300 slots. Source nodes: M: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 slots:[0-5460] (5461 slots) master 1 additional replica(s) M: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5461-10922] (5462 slots) master 1 additional replica(s) M: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 slots:[10923-16383] (5461 slots) master 1 additional replica(s) Destination node: M: fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385 slots: (0 slots) master Resharding plan: Moving slot 5461 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5462 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5463 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5464 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5465 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5466 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5467 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5468 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5469 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5470 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5471 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5472 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5473 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5474 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5475 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5476 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5477 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5478 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5479 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5480 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5481 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5482 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5483 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5484 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5485 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5486 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5487 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5488 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5489 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5490 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5491 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5492 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5493 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5494 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5495 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5496 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5497 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5498 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5499 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5500 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5501 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5502 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5503 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5504 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5505 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5506 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5507 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5508 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5509 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5510 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5511 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5512 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5513 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5514 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5515 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5516 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5517 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5518 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5519 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5520 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5521 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5522 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5523 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5524 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5525 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5526 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5527 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5528 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5529 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5530 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5531 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5532 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5533 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5534 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5535 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5536 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5537 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5538 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5539 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5540 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5541 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5542 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5543 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5544 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5545 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5546 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5547 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5548 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5549 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5550 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5551 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5552 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5553 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5554 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5555 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5556 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5557 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5558 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5559 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5560 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 5561 from b1790b6e64961f2da927cd78ad64cf02463727da Moving slot 0 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 1 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 2 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 3 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 4 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 5 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 6 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 7 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 8 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 9 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 10 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 11 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 12 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 13 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 14 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 15 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 16 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 17 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 18 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 19 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 20 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 21 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 22 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 23 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 24 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 25 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 26 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 27 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 28 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 29 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 30 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 31 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 32 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 33 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 34 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 35 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 36 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 37 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 38 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 39 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 40 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 41 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 42 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 43 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 44 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 45 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 46 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 47 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 48 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 49 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 50 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 51 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 52 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 53 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 54 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 55 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 56 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 57 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 58 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 59 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 60 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 61 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 62 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 63 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 64 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 65 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 66 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 67 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 68 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 69 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 70 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 71 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 72 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 73 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 74 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 75 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 76 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 77 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 78 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 79 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 80 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 81 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 82 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 83 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 84 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 85 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 86 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 87 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 88 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 89 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 90 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 91 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 92 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 93 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 94 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 95 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 96 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 97 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 98 from 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 Moving slot 10923 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10924 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10925 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10926 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10927 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10928 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10929 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10930 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10931 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10932 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10933 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10934 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10935 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10936 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10937 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10938 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10939 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10940 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10941 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10942 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10943 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10944 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10945 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10946 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10947 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10948 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10949 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10950 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10951 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10952 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10953 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10954 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10955 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10956 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10957 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10958 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10959 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10960 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10961 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10962 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10963 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10964 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10965 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10966 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10967 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10968 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10969 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10970 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10971 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10972 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10973 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10974 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10975 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10976 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10977 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10978 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10979 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10980 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10981 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10982 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10983 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10984 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10985 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10986 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10987 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10988 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10989 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10990 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10991 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10992 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10993 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10994 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10995 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10996 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10997 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10998 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 10999 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11000 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11001 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11002 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11003 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11004 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11005 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11006 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11007 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11008 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11009 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11010 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11011 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11012 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11013 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11014 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11015 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11016 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11017 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11018 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11019 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11020 from 6c255660ba138e0cb7ac521ad945bafbf0901f1a Moving slot 11021 from 6c255660ba138e0cb7ac521ad945bafbf0901f1aDo you want to proceed with the proposed reshard plan (yes/no)? yesMoving slot 5461 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5462 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5463 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5464 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5465 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5466 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5467 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5468 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5469 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5470 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5471 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5472 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5473 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5474 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5475 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5476 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5477 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5478 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5479 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5480 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5481 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5482 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5483 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5484 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5485 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5486 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5487 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5488 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5489 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5490 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5491 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5492 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5493 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5494 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5495 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5496 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5497 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5498 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5499 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5500 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5501 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5502 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5503 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5504 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5505 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5506 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5507 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5508 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5509 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5510 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5511 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5512 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5513 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5514 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5515 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5516 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5517 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5518 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5519 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5520 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5521 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5522 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5523 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5524 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5525 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5526 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5527 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5528 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5529 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5530 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5531 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5532 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5533 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5534 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5535 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5536 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5537 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5538 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5539 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5540 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5541 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5542 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5543 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5544 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5545 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5546 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5547 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5548 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5549 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5550 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5551 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5552 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5553 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5554 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5555 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5556 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5557 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5558 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5559 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5560 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 5561 from 182.61.35.33:16380 to 182.61.35.33:16385: Moving slot 0 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 1 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 2 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 3 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 4 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 5 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 6 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 7 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 8 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 9 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 10 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 11 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 12 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 13 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 14 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 15 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 16 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 17 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 18 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 19 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 20 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 21 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 22 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 23 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 24 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 25 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 26 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 27 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 28 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 29 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 30 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 31 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 32 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 33 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 34 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 35 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 36 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 37 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 38 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 39 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 40 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 41 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 42 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 43 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 44 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 45 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 46 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 47 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 48 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 49 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 50 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 51 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 52 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 53 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 54 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 55 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 56 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 57 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 58 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 59 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 60 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 61 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 62 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 63 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 64 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 65 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 66 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 67 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 68 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 69 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 70 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 71 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 72 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 73 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 74 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 75 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 76 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 77 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 78 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 79 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 80 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 81 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 82 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 83 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 84 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 85 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 86 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 87 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 88 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 89 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 90 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 91 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 92 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 93 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 94 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 95 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 96 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 97 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 98 from 182.61.35.33:16382 to 182.61.35.33:16385: Moving slot 10923 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10924 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10925 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10926 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10927 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10928 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10929 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10930 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10931 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10932 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10933 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10934 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10935 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10936 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10937 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10938 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10939 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10940 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10941 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10942 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10943 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10944 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10945 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10946 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10947 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10948 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10949 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10950 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10951 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10952 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10953 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10954 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10955 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10956 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10957 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10958 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10959 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10960 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10961 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10962 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10963 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10964 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10965 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10966 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10967 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10968 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10969 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10970 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10971 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10972 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10973 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10974 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10975 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10976 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10977 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10978 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10979 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10980 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10981 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10982 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10983 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10984 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10985 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10986 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10987 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10988 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10989 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10990 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10991 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10992 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10993 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10994 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10995 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10996 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10997 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10998 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 10999 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11000 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11001 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11002 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11003 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11004 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11005 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11006 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11007 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11008 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11009 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11010 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11011 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11012 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11013 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11014 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11015 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11016 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11017 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11018 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11019 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11020 from 182.61.35.33:16384 to 182.61.35.33:16385: Moving slot 11021 from 182.61.35.33:16384 to 182.61.35.33:16385: 上面有几个选项: How many slots do you want to move (from 1 to 16384)? 要迁移的数量 What is the receiving node ID? 要迁移到的目的redis node id Please enter all the source node IDs. 这里有两个选项：输入all，则随机在已有哈希槽上随机迁移。输入源node id，则是在指定redis中取出相应数量的哈希槽。当哈希槽分配完毕后，我们就可以看到新节点的哈希槽信息:12345678182.61.35.33:16379&gt; cluster nodes93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579941661922 14 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 master - 0 1579941658912 19 connected 0-98 5461-5561 10923-110215923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master - 0 1579941660000 18 connected 99-54609d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579941659000 15 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579941660919 14 connected 5562-10922d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 myself,slave 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 0 1579941656000 13 connected6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579941659915 15 connected 11022-16383 但是它没有从节点，现在我们就给它添加一个从节点去。这时候我们先把16386的节点添加到集群:123456789101112131415161718192021222324252627282930313233343536373839root@6fba3a16d91d:/data# redis-cli --cluster add-node 182.61.35.33:16386 182.61.35.33:16379 -a dockerredisWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.&gt;&gt;&gt; Adding node 182.61.35.33:16386 to cluster 182.61.35.33:16379&gt;&gt;&gt; Performing Cluster Check (using node 182.61.35.33:16379)S: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots: (0 slots) slave replicates 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2S: 93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383 slots: (0 slots) slave replicates b1790b6e64961f2da927cd78ad64cf02463727daM: fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385 slots:[0-98],[5461-5561],[10923-11021] (299 slots) masterM: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 slots:[99-5460] (5362 slots) master 1 additional replica(s)S: 9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381 slots: (0 slots) slave replicates 6c255660ba138e0cb7ac521ad945bafbf0901f1aM: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5562-10922] (5361 slots) master 1 additional replica(s)M: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 slots:[11022-16383] (5362 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 182.61.35.33:16386 to make it join the cluster.[OK] New node added correctly.182.61.35.33:16380&gt; cluster nodes9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579942113607 15 connectedaa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 master - 0 1579942110000 0 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 master - 0 1579942112000 19 connected 0-98 5461-5561 10923-11021b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 myself,master - 0 1579942110000 14 connected 5562-109226c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579942111000 15 connected 11022-163835923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master - 0 1579942109000 18 connected 99-5460d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 slave 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 0 1579942111000 18 connected93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579942112604 14 connected 我们可以从上面的信息中看到，新添加端口为16386的节点依然为一个master节点,这时候，我们需要把它变为16385的从节点，需要进行cluster replicate nodeId这个命令,但是这时候如果直接添加的话就会报错:12182.61.35.33:16380&gt; cluster replicate fc576f7421e4b453dc500d4cbe028716bb1efbf8(error) ERR To set a master the node must be empty and without assigned slots. 这是因为主节点和从节点要都为空，才能添加主从关系。所以，我们就麻烦一点，先把slot迁移走。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655root@6fba3a16d91d:/data# redis-cli --cluster reshard 182.61.35.33:16385 -a dockerredisWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.&gt;&gt;&gt; Performing Cluster Check (using node 182.61.35.33:16385)M: fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385 slots:[0-98],[5461-5561],[10923-11021] (299 slots) masterS: 93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383 slots: (0 slots) slave replicates b1790b6e64961f2da927cd78ad64cf02463727daM: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 slots:[99-5460] (5362 slots) master 1 additional replica(s)S: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots: (0 slots) slave replicates 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2M: aa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386 slots: (0 slots) masterM: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5562-10922] (5361 slots) master 1 additional replica(s)M: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 slots:[11022-16383] (5362 slots) master 1 additional replica(s)S: 9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381 slots: (0 slots) slave replicates 6c255660ba138e0cb7ac521ad945bafbf0901f1a[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 300What is the receiving node ID? b1790b6e64961f2da927cd78ad64cf02463727daPlease enter all the source node IDs. Type &apos;all&apos; to use all the nodes as source nodes for the hash slots. Type &apos;done&apos; once you entered all the source nodes IDs.Source node #1: fc576f7421e4b453dc500d4cbe028716bb1efbf8Source node #2: doneReady to move 300 slots. Source nodes: M: fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385 slots:[0-98],[5461-5561],[10923-11021] (299 slots) master Destination node: M: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5562-10922] (5361 slots) master 1 additional replica(s) Resharding plan: Moving slot 0 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 1 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 2 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 3 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 4 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 6 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 7 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 8 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 9 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 12 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 13 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 14 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 15 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 16 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 17 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 18 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 19 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 20 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 21 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 22 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 23 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 24 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 25 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 26 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 27 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 28 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 29 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 30 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 31 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 32 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 33 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 34 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 35 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 36 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 37 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 38 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 39 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 40 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 41 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 42 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 43 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 44 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 45 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 46 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 47 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 48 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 49 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 50 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 51 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 52 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 53 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 54 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 55 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 56 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 57 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 58 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 59 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 60 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 61 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 62 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 63 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 64 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 65 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 66 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 67 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 68 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 69 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 70 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 71 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 72 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 73 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 74 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 75 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 76 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 77 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 78 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 79 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 80 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 81 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 82 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 83 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 84 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 85 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 86 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 87 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 88 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 89 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 90 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 91 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 92 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 93 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 94 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 95 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 96 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 97 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 98 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5461 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5462 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5463 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5464 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5465 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5466 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5467 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5468 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5469 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5470 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5471 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5472 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5473 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5474 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5475 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5476 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5477 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5478 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5479 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5480 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5481 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5482 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5483 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5484 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5485 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5486 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5487 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5488 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5489 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5490 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5491 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5492 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5493 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5494 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5495 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5496 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5497 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5498 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5499 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5500 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5501 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5502 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5503 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5504 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5505 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5506 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5507 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5508 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5509 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5510 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5511 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5512 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5513 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5514 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5515 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5516 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5517 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5518 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5519 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5520 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5521 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5522 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5523 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5524 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5525 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5526 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5527 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5528 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5529 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5530 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5531 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5532 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5533 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5534 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5535 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5536 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5537 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5538 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5539 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5540 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5541 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5542 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5543 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5544 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5545 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5546 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5547 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5548 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5549 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5550 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5551 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5552 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5553 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5554 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5555 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5556 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5557 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5558 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5559 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5560 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 5561 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10923 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10924 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10925 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10926 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10927 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10928 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10929 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10930 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10931 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10932 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10933 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10934 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10935 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10936 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10937 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10938 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10939 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10940 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10941 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10942 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10943 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10944 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10945 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10946 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10947 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10948 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10949 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10950 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10951 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10952 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10953 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10954 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10955 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10956 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10957 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10958 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10959 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10960 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10961 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10962 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10963 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10964 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10965 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10966 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10967 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10968 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10969 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10970 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10971 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10972 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10973 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10974 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10975 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10976 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10977 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10978 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10979 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10980 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10981 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10982 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10983 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10984 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10985 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10986 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10987 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10988 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10989 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10990 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10991 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10992 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10993 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10994 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10995 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10996 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10997 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10998 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 10999 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11000 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11001 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11002 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11003 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11004 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11005 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11006 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11007 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11008 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11009 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11010 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11011 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11012 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11013 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11014 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11015 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11016 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11017 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11018 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11019 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11020 from fc576f7421e4b453dc500d4cbe028716bb1efbf8 Moving slot 11021 from fc576f7421e4b453dc500d4cbe028716bb1efbf8Do you want to proceed with the proposed reshard plan (yes/no)? yesMoving slot 0 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 1 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 2 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 3 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 4 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 6 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 7 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 8 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 9 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 12 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 13 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 14 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 15 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 16 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 17 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 18 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 19 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 20 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 21 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 22 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 23 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 24 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 25 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 26 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 27 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 28 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 29 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 30 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 31 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 32 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 33 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 34 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 35 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 36 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 37 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 38 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 39 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 40 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 41 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 42 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 43 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 44 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 45 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 46 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 47 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 48 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 49 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 50 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 51 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 52 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 53 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 54 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 55 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 56 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 57 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 58 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 59 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 60 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 61 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 62 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 63 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 64 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 65 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 66 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 67 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 68 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 69 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 70 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 71 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 72 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 73 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 74 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 75 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 76 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 77 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 78 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 79 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 80 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 81 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 82 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 83 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 84 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 85 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 86 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 87 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 88 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 89 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 90 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 91 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 92 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 93 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 94 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 95 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 96 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 97 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 98 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5461 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5462 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5463 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5464 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5465 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5466 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5467 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5468 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5469 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5470 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5471 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5472 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5473 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5474 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5475 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5476 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5477 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5478 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5479 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5480 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5481 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5482 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5483 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5484 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5485 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5486 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5487 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5488 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5489 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5490 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5491 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5492 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5493 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5494 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5495 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5496 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5497 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5498 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5499 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5500 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5501 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5502 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5503 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5504 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5505 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5506 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5507 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5508 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5509 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5510 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5511 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5512 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5513 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5514 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5515 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5516 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5517 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5518 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5519 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5520 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5521 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5522 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5523 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5524 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5525 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5526 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5527 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5528 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5529 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5530 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5531 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5532 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5533 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5534 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5535 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5536 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5537 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5538 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5539 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5540 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5541 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5542 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5543 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5544 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5545 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5546 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5547 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5548 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5549 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5550 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5551 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5552 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5553 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5554 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5555 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5556 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5557 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5558 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5559 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5560 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 5561 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10923 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10924 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10925 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10926 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10927 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10928 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10929 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10930 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10931 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10932 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10933 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10934 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10935 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10936 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10937 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10938 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10939 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10940 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10941 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10942 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10943 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10944 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10945 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10946 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10947 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10948 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10949 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10950 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10951 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10952 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10953 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10954 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10955 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10956 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10957 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10958 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10959 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10960 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10961 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10962 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10963 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10964 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10965 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10966 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10967 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10968 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10969 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10970 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10971 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10972 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10973 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10974 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10975 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10976 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10977 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10978 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10979 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10980 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10981 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10982 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10983 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10984 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10985 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10986 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10987 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10988 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10989 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10990 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10991 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10992 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10993 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10994 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10995 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10996 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10997 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10998 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 10999 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11000 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11001 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11002 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11003 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11004 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11005 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11006 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11007 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11008 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11009 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11010 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11011 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11012 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11013 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11014 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11015 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11016 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11017 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11018 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11019 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11020 from 182.61.35.33:16385 to 182.61.35.33:16380: Moving slot 11021 from 182.61.35.33:16385 to 182.61.35.33:16380:182.61.35.33:16385&gt; cluster nodes93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579943275000 20 connected5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master - 0 1579943276697 18 connected 99-5460d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 slave 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 0 1579943275000 18 connectedaa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 master - 0 1579943275694 0 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 myself,master - 0 1579943273000 19 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579943274000 20 connected 0-98 5461-110216c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579943271683 15 connected 11022-163839d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579943274000 15 connected 迁移完后，登录到从节点进行主从配置:1234567891011182.61.35.33:16386&gt; cluster replicate fc576f7421e4b453dc500d4cbe028716bb1efbf8OK182.61.35.33:16386&gt; cluster nodes6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579943419000 15 connected 11022-16383aa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 myself,slave fc576f7421e4b453dc500d4cbe028716bb1efbf8 0 1579943419000 0 connected93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579943421134 20 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579943420132 20 connected 0-98 5461-11021fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 master - 0 1579943423140 19 connected9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579943419129 15 connectedd1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 slave 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 0 1579943421000 18 connected5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master - 0 1579943422137 18 connected 99-5460 可以看到主从关系已经配置完成，我们再进哈希槽迁移,上面已经进行了两次哈希槽迁移，所以这里就省略掉，来看看最后的成功吧.123456789182.61.35.33:16386&gt; cluster nodes6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579943609713 15 connected 11022-16383aa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 myself,slave fc576f7421e4b453dc500d4cbe028716bb1efbf8 0 1579943606000 0 connected93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579943608708 20 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579943607706 20 connected 5662-11021fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 master - 0 1579943607000 21 connected 0-98 5461-56619d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579943610716 15 connectedd1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 slave 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 0 1579943610000 18 connected5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master - 0 1579943608000 18 connected 99-5460 删除节点我们下线一个主节点，看看Redis Cluster的状态。这里我试试让16382这个端口的节点宕机，可以看到之前的从节点变为了主节点: 123456789182.61.35.33:16379&gt; cluster nodes93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579944069787 20 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 master - 0 1579944068000 21 connected 0-98 5461-56615923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 master,fail - 1579944028558 1579944027556 18 disconnectedaa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 slave fc576f7421e4b453dc500d4cbe028716bb1efbf8 0 1579944070795 21 connected9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579944068000 15 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579944068000 20 connected 5662-11021d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 myself,master - 0 1579944070000 22 connected 99-54606c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579944071795 15 connected 11022-16383 我们再把16382这个节点启动起来，会发现，由于原来的从节点变为了主节点，所以它会变为其它节点的从节点(原主节点)。 123456789182.61.35.33:16379&gt; cluster nodes93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579944182000 20 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 master - 0 1579944186307 21 connected 0-98 5461-56615923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 slave d1b77c63e1e652b786c9e52e42f7594e9752903c 0 1579944185000 22 connectedaa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 slave fc576f7421e4b453dc500d4cbe028716bb1efbf8 0 1579944184000 21 connected9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579944187310 15 connectedb1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579944185306 20 connected 5662-11021d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 myself,master - 0 1579944183000 22 connected 99-54606c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579944184302 15 connected 11022-16383 现在我们来进行节点删除，这里我们删除点16379这个节点，首先需要把节点上的solt迁移到其它节点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258root@6fba3a16d91d:/data# redis-cli --cluster reshard 182.61.35.33:16385 -a dockerredisWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.&gt;&gt;&gt; Performing Cluster Check (using node 182.61.35.33:16385)M: fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385 slots:[0-98],[5461-5661] (300 slots) master 1 additional replica(s)S: 93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383 slots: (0 slots) slave replicates b1790b6e64961f2da927cd78ad64cf02463727daS: 5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382 slots: (0 slots) slave replicates d1b77c63e1e652b786c9e52e42f7594e9752903cM: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots:[99-5460] (5362 slots) master 1 additional replica(s)S: aa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386 slots: (0 slots) slave replicates fc576f7421e4b453dc500d4cbe028716bb1efbf8M: b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380 slots:[5662-11021] (5360 slots) master 1 additional replica(s)M: 6c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384 slots:[11022-16383] (5362 slots) master 1 additional replica(s)S: 9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381 slots: (0 slots) slave replicates 6c255660ba138e0cb7ac521ad945bafbf0901f1a[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 99-5460What is the receiving node ID? fc576f7421e4b453dc500d4cbe028716bb1efbf8Please enter all the source node IDs. Type &apos;all&apos; to use all the nodes as source nodes for the hash slots. Type &apos;done&apos; once you entered all the source nodes IDs.Source node #1: d1b77c63e1e652b786c9e52e42f7594e9752903cSource node #2: doneReady to move 99 slots. Source nodes: M: d1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379 slots:[99-5460] (5362 slots) master 1 additional replica(s) Destination node: M: fc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385 slots:[0-98],[5461-5661] (300 slots) master 1 additional replica(s) Resharding plan: Moving slot 99 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 100 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 101 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 102 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 103 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 104 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 105 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 106 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 107 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 108 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 109 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 110 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 111 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 112 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 113 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 114 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 115 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 116 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 117 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 118 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 119 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 120 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 121 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 122 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 123 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 124 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 125 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 126 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 127 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 128 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 129 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 130 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 131 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 132 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 133 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 134 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 135 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 136 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 137 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 138 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 139 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 140 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 141 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 142 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 143 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 144 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 145 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 146 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 147 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 148 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 149 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 150 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 151 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 152 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 153 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 154 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 155 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 156 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 157 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 158 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 159 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 160 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 161 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 162 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 163 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 164 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 165 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 166 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 167 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 168 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 169 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 170 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 171 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 172 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 173 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 174 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 175 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 176 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 177 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 178 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 179 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 180 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 181 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 182 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 183 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 184 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 185 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 186 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 187 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 188 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 189 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 190 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 191 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 192 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 193 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 194 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 195 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 196 from d1b77c63e1e652b786c9e52e42f7594e9752903c Moving slot 197 from d1b77c63e1e652b786c9e52e42f7594e9752903cDo you want to proceed with the proposed reshard plan (yes/no)? yesMoving slot 99 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 100 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 101 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 102 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 103 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 104 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 105 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 106 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 107 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 108 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 109 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 110 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 111 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 112 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 113 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 114 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 115 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 116 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 117 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 118 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 119 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 120 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 121 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 122 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 123 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 124 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 125 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 126 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 127 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 128 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 129 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 130 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 131 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 132 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 133 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 134 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 135 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 136 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 137 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 138 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 139 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 140 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 141 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 142 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 143 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 144 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 145 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 146 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 147 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 148 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 149 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 150 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 151 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 152 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 153 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 154 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 155 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 156 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 157 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 158 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 159 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 160 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 161 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 162 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 163 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 164 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 165 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 166 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 167 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 168 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 169 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 170 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 171 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 172 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 173 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 174 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 175 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 176 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 177 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 178 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 179 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 180 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 181 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 182 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 183 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 184 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 185 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 186 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 187 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 188 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 189 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 190 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 191 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 192 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 193 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 194 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 195 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 196 from 182.61.35.33:16379 to 182.61.35.33:16385: Moving slot 197 from 182.61.35.33:16379 to 182.61.35.33:16385:...........182.61.35.33:16385&gt; cluster nodes93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579945059435 24 connected5923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579945058000 24 connectedd1b77c63e1e652b786c9e52e42f7594e9752903c 182.61.35.33:16379@26379 master - 0 1579945058000 22 connectedaa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 slave fc576f7421e4b453dc500d4cbe028716bb1efbf8 0 1579945059000 23 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 myself,master - 0 1579945055000 23 connected 0-5459 5461-5661b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 master - 0 1579945058431 24 connected 5460 5662-110216c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579945056000 15 connected 11022-163839d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579945057430 15 connected 迁移完后就可以进行节点删除，redis-cli cluster del-node 要查询集群信息的IP:端口 要移除的node id 12345678910111213root@6fba3a16d91d:/data# redis-cli --cluster del-node 182.61.35.33:16385 d1b77c63e1e652b786c9e52e42f7594e9752903c -a dockerredisWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.&gt;&gt;&gt; Removing node d1b77c63e1e652b786c9e52e42f7594e9752903c from cluster 182.61.35.33:16385&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node.182.61.35.33:16380&gt; cluster nodes9d1935b3462566449d9f3c3c38b8d3ab9b97b134 182.61.35.33:16381@26381 slave 6c255660ba138e0cb7ac521ad945bafbf0901f1a 0 1579945617000 15 connectedaa8b86fd7a3396f2c1eb43b3a5d1e98c3a791cca 182.61.35.33:16386@26386 slave fc576f7421e4b453dc500d4cbe028716bb1efbf8 0 1579945618000 23 connectedfc576f7421e4b453dc500d4cbe028716bb1efbf8 182.61.35.33:16385@26385 master - 0 1579945617000 23 connected 0-5459 5461-5661b1790b6e64961f2da927cd78ad64cf02463727da 182.61.35.33:16380@26380 myself,master - 0 1579945615000 24 connected 5460 5662-110216c255660ba138e0cb7ac521ad945bafbf0901f1a 182.61.35.33:16384@26384 master - 0 1579945617633 15 connected 11022-163835923fef0a1e7a7b3a77290af1c13e1e57678f6e2 182.61.35.33:16382@26382 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579945618635 24 connected93fe47529c3f671d6d5b75c46ac965d43b904c34 182.61.35.33:16383@26383 slave b1790b6e64961f2da927cd78ad64cf02463727da 0 1579945616000 24 connected 可以看到16379的节点被移除掉。 redis-cli常用命令1234567891011121314151617181920212223242526272829303132333435363738redis-cli --cluster helpCluster Manager Commands: create host1:port1 ... hostN:portN #创建集群 --cluster-replicas &lt;arg&gt; #从节点个数 check host:port #检查集群 --cluster-search-multiple-owners #检查是否有槽同时被分配给了多个节点 info host:port #查看集群状态 fix host:port #修复集群 --cluster-search-multiple-owners #修复槽的重复分配问题 reshard host:port #指定集群的任意一节点进行迁移slot，重新分slots --cluster-from &lt;arg&gt; #需要从哪些源节点上迁移slot，可从多个源节点完成迁移，以逗号隔开，传递的是节点的node id，还可以直接传递--from all，这样源节点就是集群的所有节点，不传递该参数的话，则会在迁移过程中提示用户输入 --cluster-to &lt;arg&gt; #slot需要迁移的目的节点的node id，目的节点只能填写一个，不传递该参数的话，则会在迁移过程中提示用户输入 --cluster-slots &lt;arg&gt; #需要迁移的slot数量，不传递该参数的话，则会在迁移过程中提示用户输入。 --cluster-yes #指定迁移时的确认输入 --cluster-timeout &lt;arg&gt; #设置migrate命令的超时时间 --cluster-pipeline &lt;arg&gt; #定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10 --cluster-replace #是否直接replace到目标节点 rebalance host:port #指定集群的任意一节点进行平衡集群节点slot数量 --cluster-weight &lt;node1=w1...nodeN=wN&gt; #指定集群节点的权重 --cluster-use-empty-masters #设置可以让没有分配slot的主节点参与，默认不允许 --cluster-timeout &lt;arg&gt; #设置migrate命令的超时时间 --cluster-simulate #模拟rebalance操作，不会真正执行迁移操作 --cluster-pipeline &lt;arg&gt; #定义cluster getkeysinslot命令一次取出的key数量，默认值为10 --cluster-threshold &lt;arg&gt; #迁移的slot阈值超过threshold，执行rebalance操作 --cluster-replace #是否直接replace到目标节点 add-node new_host:new_port existing_host:existing_port #添加节点，把新节点加入到指定的集群，默认添加主节点 --cluster-slave #新节点作为从节点，默认随机一个主节点 --cluster-master-id &lt;arg&gt; #给新节点指定主节点 del-node host:port node_id #删除给定的一个节点，成功后关闭该节点服务 call host:port command arg arg .. arg #在集群的所有节点执行相关命令 set-timeout host:port milliseconds #设置cluster-node-timeout import host:port #将外部redis数据导入集群 --cluster-from &lt;arg&gt; #将指定实例的数据导入到集群 --cluster-copy #migrate时指定copy --cluster-replace #migrate时指定replace help For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster.","link":"/posts/2841798292.html"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Datax","slug":"Datax","link":"/tags/Datax/"},{"name":"数据同步","slug":"数据同步","link":"/tags/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"},{"name":"Tool","slug":"Tool","link":"/tags/Tool/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"Docker命令","slug":"Docker命令","link":"/tags/Docker%E5%91%BD%E4%BB%A4/"},{"name":"ECMAScript","slug":"ECMAScript","link":"/tags/ECMAScript/"},{"name":"Golang","slug":"Golang","link":"/tags/Golang/"},{"name":"Go基础","slug":"Go基础","link":"/tags/Go%E5%9F%BA%E7%A1%80/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Go","slug":"Go","link":"/tags/Go/"},{"name":".net core","slug":"net-core","link":"/tags/net-core/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":".net","slug":"net","link":"/tags/net/"},{"name":"微信公众号","slug":"微信公众号","link":"/tags/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"ack","slug":"ack","link":"/tags/ack/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/tags/Spring-Boot/"},{"name":"Swagger","slug":"Swagger","link":"/tags/Swagger/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"mysql explain","slug":"mysql-explain","link":"/tags/mysql-explain/"},{"name":"文件存储","slug":"文件存储","link":"/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"name":"autofac","slug":"autofac","link":"/tags/autofac/"},{"name":"swagger","slug":"swagger","link":"/tags/swagger/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"redash","slug":"redash","link":"/tags/redash/"},{"name":"微信","slug":"微信","link":"/tags/%E5%BE%AE%E4%BF%A1/"},{"name":"ABP","slug":"ABP","link":"/tags/ABP/"},{"name":"Gis","slug":"Gis","link":"/tags/Gis/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"}],"categories":[{"name":"Tool","slug":"Tool","link":"/categories/Tool/"},{"name":"JavaScript","slug":"JavaScript","link":"/categories/JavaScript/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Golang","slug":"Golang","link":"/categories/Golang/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"消息队列","slug":"消息队列","link":"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":".net core","slug":"net-core","link":"/categories/net-core/"},{"name":"Nginx","slug":"Nginx","link":"/categories/Nginx/"},{"name":".net","slug":"net","link":"/categories/net/"},{"name":"Vue","slug":"Vue","link":"/categories/Vue/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"文件存储","slug":"文件存储","link":"/categories/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"name":"shell","slug":"shell","link":"/categories/shell/"},{"name":"微信","slug":"微信","link":"/categories/%E5%BE%AE%E4%BF%A1/"},{"name":"Gis","slug":"Gis","link":"/categories/Gis/"},{"name":"Redis","slug":"Redis","link":"/categories/Redis/"}]}